#< ignore

```{r "setup"}

library(RTutor)
library(yaml)

# Adapt working directory
setwd("C:/Users/murat/Desktop/Master")
ps.name = "Lessons from Pollution Monitoring in China"; sol.file = paste0(ps.name,"_sol.Rmd")
# character vector of all packages you load in the problem set
libs = c("ggplot2", "haven", "dplyr", "tidyr", "modelsummary","kableExtra","fixest","readr","sf","rnaturalearth","gridExtra","broom","igraph")

create.ps(sol.file = paste0(ps.name,"_sol.Rmd"), ps.name=ps.name, user.name = NULL, libs=libs, stop.when.finished=FALSE, addons = "quiz")
# Show the problem set in the webbrowser
show.ps(ps.name,sample.solution=FALSE,auto.save.code = TRUE)

```
#>

# Lessons from Pollution Monitoring in China 

Author: Murat Sunmez

Hello readers and welcome to my master’s thesis on the effects of air pollution monitoring on local environmental enforcement in China. Do not be surprised by the structure of this thesis, as the entire document is generated from an *RMarkdown* file using the *R* programming language. Throughout the thesis, you will encounter code chunks used for data cleaning, model estimation, and visualization. These rely on a large panel dataset that is preloaded and processed in the background. This dynamic format ensures full transparency and complete reproducibility of all results. 
At its core, the thesis takes the form of an interactive problem set, which includes exercises and quizzes designed to help readers engage directly with the empirical methods and underlying data.

This thesis replicates and extends the empirical study **“Informed Enforcement: Lessons from Pollution Monitoring in China”** by Sebastian Axbard and Zichen Deng, published in 2024 in the *American Economic Journal: Applied Economics*. The original paper examines how the introduction of real-time air pollution monitors influenced local enforcement behavior and air quality outcomes in Chinese cities. The authors find that monitoring improves the targeting of inspections, increases enforcement activity, and leads to reductions in pollution levels, particularly in contexts where local officials have strong promotion incentives.
The original article is available [here](https://www.aeaweb.org/articles?id=10.1257/app.20210386).

The problem set is published here:

* `GitHub`: <https://github.com/MuratSunmez/RTutorLessonsFromPollutionMonitoring>
* `ShinyApps`: <https://muratsunmez.shinyapps.io/LessonsFromPollutionMonitoringChina/>

In recent decades, China has experienced rapid industrial development, which has led to serious environmental problems. Although the central government introduced increasingly strict pollution regulations, enforcement at the local level often remained weak. One key reason was the lack of reliable and independent data on local pollution, which made it difficult for central authorities to oversee and evaluate local officials effectively (Zheng & Shi, 2017; World Bank, 2017). Similar problems have been observed in many other countries, particularly where performance data is limited or easy to manipulate (Nakagaki et al., 2012; Sandefur & Glassman, 2015).

In 2015, China launched a national system of real-time air quality monitoring. The new monitors reported hourly pollution levels directly to the central government and made the data publicly available. This shift limited the ability of local governments to interfere with the data and improved the central government's ability to hold them accountable (Axbard & Deng, 2024). Earlier cases of data manipulation had revealed how local incentives could distort reporting, which led to a later reform where monitor operations were handed over to external companies.

The introduction of these monitors provided a more objective and continuous view of local environmental outcomes. It helped align local enforcement with national goals and created a new channel of accountability. Weak enforcement remains one of the biggest barriers to solving global environmental challenges. By improving transparency and limiting local discretion over data, China's monitoring reform offers important insights for improving regulatory effectiveness in other countries as well (Banerjee et al., 2008; Acemoglu et al., 2020).

## Exercise Content

1. Introduction

 1.1 Evolution of Pollution Monitoring and Accountability Mechanism in China
 
 1.2 Geographical Overview of Pollution and Enforcement Data

2. Data Overview

 2.1 Description of the Data Sources
 
 2.2 Introduction to the Data Set
 
3. Firm-Level Analysis of Monitoring Impacts

 3.1 Assignment and Location of Monitors

 3.2 Conceptual Framework of the DiD Approach
 
 3.3 Estimating the DiD Model Using Regression
 
 3.4 Cluster-Robust Standard Errors
 
 3.5 Fixed Effects
 
4. City-Level Analysis of Monitoring Impacts

 4.1 City Level Evidence
 
 4.2 Incorporating Fixed Effects and Control Variables
 
 4.3 Instrumental Variables Approach
 
5.  Mechanisms of Improved Governance and Enforcement

 5.1 Top-Down Accountability
 
 5.2 Changing Information Provision

6. Conclusion

7. References


## Overview

This thesis begins by outlining the institutional background of environmental policy in China, with a focus on how real-time air pollution monitoring was introduced to strengthen accountability and reduce local discretion. Particular attention is given to the role of political incentives and the reassignment of monitoring responsibilities to independent third parties.
Next, the main data sources are introduced, including firm-level enforcement records, satellite-based Aerosol Optical Depth (AOD) measurements, ground-level fine particulate matter (PM2.5) data, and spatial information on monitor placement. The structure and coverage of the panel dataset are explained in detail to prepare the ground for the empirical analysis.
The core analysis starts at the firm level with a difference-in-differences approach. The estimation strategy is developed step by step: first, a baseline specification is introduced; then, cluster-robust standard errors are applied to account for correlated errors within groups; finally, fixed effects are included to control for time-invariant firm characteristics and time trends.
Building on this, the analysis moves to the city level. Here, additional control variables are added to capture observed heterogeneity across cities. To address potential endogeneity in monitor placement, an instrumental variables strategy is implemented using centrally assigned monitor quotas.
Finally, the thesis investigates potential mechanisms. This includes an analysis of how political incentives affect local enforcement behavior and how the reform improved data credibility, as shown by the stronger correlation between satellite-based and ground-level pollution measurements after the monitors were removed from local control.

The structure of this project closely follows the original study and offers a guided application of core empirical tools in applied policy analysis.

## Instructions for Working with the Problem Set

Before diving into the exercises, here’s a quick orientation to help you make the most of this interactive format.

You’re free to explore the tasks in any order you like. However, I suggest progressing through them step by step, as later parts often build on the logic and concepts introduced earlier. Whether you're experienced in R or just getting started, there's clear guidance provided in every section to support you along the way. 

This problem set blends a variety of interactive components:

- **Code Exercises**  
  Tasks vary in format:
  - Some ask you to complete partial code with missing elements  
  - Others require you to write a solution from scratch  
  - In some cases, the code is already complete, so you can simply run it and interpret the results.
  
- **Interactive Features**  
  Below each code block, you’ll find useful buttons:
  - `edit`: Unlock the chunk if it’s currently read-only  
  - `check`: Submit your code to see if it’s correct  
  - `solution`: Display the correct answer if you get stuck  
  - `data`: Open the dataset viewer for exploring tables  
  - `run chunk`: Run the code without checking correctness  
  - `original code`: Revert to the default code if needed  

- **Quizzes:**  
  Short quizzes appear throughout the set to test your intuition or recap what you’ve learned. Don’t worry if you guess, they’re here to reinforce key points.

- **Info Blocks:**  
  You’ll also find expandable info sections offering explanations on concepts, functions, or datasets relevant to a given task.

- **Awards:**
  Completing full exercises might unlock small awards such as summaries, fun illustrations, or helpful tips that make learning more enjoyable.
  
You’re now ready to begin! This problem set is designed to be both educational and engaging. Have fun experimenting with code, discovering insights, and deepening your understanding along the way.

<br/>

## Exercise 1 -- Introduction

The first chapter is intended to give a first impression of the topic covered in this problem set and why it is important. 

First, we examine how China’s national air quality monitoring system emerged in response to growing environmental concerns. We look at how real-time monitoring reshaped accountability structures, strengthened central oversight, and led to institutional changes such as the transfer of control to independent third parties.

Then, we explore the spatial dimension of air pollution and enforcement. This includes the distribution of PM2.5 pollution across Chinese cities and the geographic rollout of monitoring stations. We analyze how monitor placement aligns with pollution hotspots and enables more effective regulatory targeting.

Together, these components provide a comprehensive foundation for understanding the policy context and spatial data structure behind China’s environmental monitoring reforms.


### Structure

1.1 Evolution of Pollution Monitoring and Accountability Mechanism in China
 
1.2 Geographical Overview of Pollution and Enforcement Data

## Exercise 1.1 -- Evolution of Pollution Monitoring and Accountability Mechanism in China



China’s rapid industrialization has driven economic growth, but it has also resulted in significant environmental degradation (Kostka & Mol, 2013). For decades, pollution levels escalated due to weak enforcement of environmental regulations and local governments prioritizing economic development over environmental protection. The inconsistency between national environmental policies and their local implementation has created major accountability challenges, with enforcement often hampered by unreliable data and regulatory loopholes (Haddad, 2015, pp. 11, 15).

In response to growing public concern and increased international scrutiny, the Chinese government introduced a series of far-reaching pollution control measures. Among them, the *Air Pollution Prevention and Control Action Plan* of 2013 marked a significant shift in national environmental policy by tightening emission standards and introducing stronger accountability mechanisms for local governments.


#< quiz "Pollution_Monitoring_China"
question: What do you think was a key feature of the 2013 Air Pollution Prevention and Control Action Plan?
sc:
- Reduction of coal consumption by 50%.
- Introduction of real-time air quality monitoring stations.*
- Implementation of a carbon tax.
- Delegation of monitoring to local governments only.
success: Correct! 
failure: Not quite. Try again.
#>

One of the most notable initiatives was the nationwide deployment of real-time air quality monitoring stations in 2015, designed to enhance transparency and accountability in regulatory enforcement. These stations provided critical data on PM2.5 concentrations, enabling regulators to take more targeted actions against non-compliant entities. To learn more about why PM2.5 is harmful and plays a key role in air quality monitoring, you can click on the info box: *Why PM2.5 Matters: Health, Economics, and Policy Relevance* in the next chapter.

The introduction of real-time air pollution monitoring in China strengthened environmental enforcement. Local governments became more active in inspections and penalties, especially in areas near monitoring stations, reflecting a shift toward more targeted and responsive regulation (Axbard & Deng, 2024).

The transfer of monitor operations from local authorities to independent third parties further improved data integrity by reducing manipulation and enhancing the credibility of pollution reporting.

Nevertheless, local officials still face conflicting priorities between economic growth and environmental protection, and enforcement capacity continues to vary across regions.

China’s experience shows how technology can enhance regulatory oversight. For developing countries facing similar challenges, integrating automated monitoring into governance systems offers a promising path toward more effective and accountable environmental enforcement.

The following graph illustrates key policy milestones in China’s efforts to address air pollution over time.



<center>
  <img src="Organizational_development.png" alt="Organizational development" width="1100"><br>
  <em>Source: Axbard and Deng (2024), Online Appendix, Figure D1.</em>
</center>


<br/>





The figure shows how the introduction and transfer of control over air quality monitors reshaped the flow of information between China's central and local governments. Although the central government continues to set regulations and local governments remain responsible for enforcement, the quality and availability of pollution data changed over time and across cities. These adjustments play a critical role in enhancing the central government’s ability to monitor air pollution levels and ensure accountability among local authorities.

This process can be divided into three distinct phases:

**Pre-Monitoring Phase (Before January 2015)**: Before 2015, there were no air quality monitors in place to systematically transmit pollution data to the central government. In this period, Cities A and B enforce regulations independently, relying solely on locally gathered information. This setup leaves the central government with limited insight into local pollution levels and enforcement activities, potentially creating gaps in oversight and accountability.

**Introduction of Monitors (January 2015)**: Starting in January 2015, the central government implements air quality monitors in various cities, including Cities A and B. Each city has a different number of monitors (represented as "M" in the figure), which results in variation in data coverage and information quality. Cities with more monitors gain a more comprehensive view of pollution sources. The monitors provide detailed, real-time pollution data, which is transmitted directly to the central government. However, the responsibility for enforcement still resides with the local governments, allowing them to interpret and act on the data as they see fit.

**Transfer of Control to Third Parties (November 2016)**: In November 2016, control over the air quality monitors is transferred from local governments to independent third-party entities.
These third parties are contracted directly by the central government, aiming to reduce opportunities for data manipulation by local governments. The independent management of monitors is intended to ensure higher data quality and objectivity, as third parties have less incentive to align with local governments’ potential motives to underreport pollution levels.

We will take a closer look at this development in *Chapter 5*.

**Summary**

In this chapter, we traced the evolution of China’s pollution monitoring and accountability mechanisms. We outlined how rising environmental concerns and weak local enforcement prompted the central government to introduce real-time air quality monitoring in 2015. These monitors were intended to enhance transparency and enable more targeted enforcement actions by local authorities. Overall, these reforms should strengthen top-down accountability and demonstrate the potential of technological interventions to close enforcement gaps in environmental governance.

The following chapter provides a spatial overview of air pollution and the distribution of monitoring stations across cities in China.

<br/>


## Exercise 1.2 -- Geographical Overview of Pollution and Enforcement Data

The spatial distribution of pollution and enforcement is key to understanding the effectiveness of environmental regulation. In 2015, China launched a nationwide monitoring program, installing **552** real-time air quality monitors across cities to strengthen oversight and improve regulatory compliance. This chapter outlines the geographic coverage of the monitoring system and examines how monitor placement correlates with patterns of local enforcement activity.


#< quiz "Air_Pollution_Statistics"
question: According to the World Health Organization (WHO) data from 2016, what percentage of the world's population lived in areas where air pollution exceeded WHO guidelines?
sc:
- More than 90%.*
- Around 50%.
- Less than 25%.
success: Excellent, your answer is correct!
failure: Try again.
#>

<br/>


Air pollution remains one of the most pressing environmental challenges globally. According to the World Health Organization (WHO, 2016), more than 90 percent of the world’s population lived in areas where air quality exceeded recommended guideline limits. This alarming statistic underscores the profound impact of pollution on public health and economic performance.

The problem is particularly acute in emerging economies such as China, where pollution levels have frequently surpassed those ever recorded in high-income countries. Rapid industrialization and economic growth have placed immense pressure on air quality, resulting in severe environmental degradation and negative health outcomes. Among the most notable consequences are increased rates of respiratory and cardiovascular diseases, as well as a reduction in average life expectancy.

#< info "Why PM2.5 Matters: Health, Economics, and Policy Relevance"

One of the most critical air pollutants in environmental regulation is PM2.5, which refers to particulate matter with a diameter of 2.5 micrometers or less. Due to its microscopic size, it can penetrate deep into the respiratory system and even enter the bloodstream (WHO, 2021, p. 77).

The health risks associated with long-term PM2.5 exposure are severe. It has been linked to:

- Respiratory and cardiovascular diseases,
- Strokes and lung cancer,
- An increased risk of premature death.

Concentrations of PM2.5 vary across space and time and tend to rise especially in winter months, when temperature inversions trap pollutants near the surface (Su et al., 2018).

Major sources of PM2.5 include:

- Vehicle emissions in urban areas,
- Industrial production,
- Biomass and coal burning.

In addition to its health impacts, PM2.5 also entails significant economic costs. It increases public healthcare expenditures and lowers labor productivity, particularly in densely populated and industrialized regions (Chang et al., 2016, pp. 2–4).

Because of its wide-ranging effects, PM2.5 serves as a central metric in air quality monitoring. It not only reflects pollution intensity but also provides insight into the effectiveness of environmental regulation. Reliable data on PM2.5 enable central governments to identify local noncompliance, intervene where necessary, and design more targeted air quality improvement strategies.

#>

Despite global efforts to address air pollution, significant challenges remain. Governments must continue to implement robust environmental regulations and ensure their effective enforcement. In China, for example, a national air quality monitoring program was introduced to enhance compliance and improve oversight. Monitoring stations were strategically deployed across the country to enable real-time tracking of pollution levels and strengthen accountability among local authorities.

As a starting point, this chapter examines the spatial distribution of air pollution in China, with a particular focus on regional disparities in PM2.5 concentrations. The following visualization identifies areas most affected by high pollution levels and provides valuable insights for policymakers and regulatory agencies. By revealing spatial pollution patterns, this analysis contributes to the design of targeted air quality interventions and supports broader strategies for sustainable urban planning and environmental governance.

**Task:** To illustrate pollution levels in China, just `check` the following chunk:

```{r "1.2.1", warning=FALSE}

#< task

# Load the city_pm data
city_pm <- read_rds("city_pm.rds")

# Remove rows with missing values in the coordinates (centroid_lon and centroid_lat)
city_pm_clean <- city_pm %>%
  filter(!is.na(centroid_lon) & !is.na(centroid_lat))

# Convert the cleaned city_pm data into an sf object
city_pm_sf <- st_as_sf(city_pm_clean, coords = c("centroid_lon", "centroid_lat"), crs = 4326)

# Load the China map for context
china_map <- ne_countries(scale = "medium", country = "China", returnclass = "sf")

# Create the map with PM2.5 values and the basemap
ggplot() +
  geom_sf(data = china_map, fill = "gray95", color = "black") +
  geom_sf(data = city_pm_sf, aes(color = pm25), size = 3) +
  scale_color_gradient(low = "yellow", high = "red", name = "PM2.5 2010") +
  labs(title = "Distribution of PM2.5 Values in China in 2010", 
       x = "Longitude", 
       y = "Latitude") +
  theme_bw()
#>

```

#< quiz "PM2.5_Distribution"
question: What does the PM2.5 distribution map primarily illustrate?
sc:
- PM2.5 levels are uniformly distributed across the country.
- There is no clear pattern in PM2.5 distribution.
- PM2.5 pollution is highest in coastal areas only.
- High PM2.5 concentrations are primarily located in the eastern and southern regions.*
success: Excellent, your answer is correct!
failure: Try again.
#>

In the visualization, PM2.5 concentrations are illustrated using color-coded dots distributed across China. The color scale ranges from yellow (indicating low pollution levels) to red (indicating high levels), with darker shades highlighting areas of particularly severe air pollution.

The spatial pattern reveals that certain regions, especially in eastern and southern China, recorded notably higher PM2.5 concentrations in 2010. These areas are characterized by dense populations, intensive industrial activity, and heavy traffic, all of which contribute significantly to air pollution.

In the next step, we examine the geographical distribution of air quality monitoring stations across China. These stations play a crucial role in measuring pollution levels by providing reliable data on PM2.5 concentrations and helping to identify pollution hotspots. Their spatial distribution enables policymakers and researchers to better understand regional disparities and to develop targeted strategies for improving air quality.


**Task:**  To illustrate the locations of air pollution monitors across China, just `check` the following chunk:

```{r "1.2.2"}

#< task

# Load the monitoring station data
monitor_city_long <- read_rds("monitor_city_long.rds")

# Create an sf object with the coordinates of the monitoring stations
monitor_sf <- st_as_sf(monitor_city_long, coords = c("monitor_lon", "monitor_lat"), crs = 4326)


# Load the China map for context
china_map <- ne_countries(scale = "medium", country = "China", returnclass = "sf")

# Create the map with the monitoring stations and the basemap
ggplot() +
  geom_sf(data = china_map, fill = "gray95", color = "black") +
  geom_sf(data = monitor_sf, color = "blue", size = 2, shape = 21, fill = "red") +
  labs(title = "Monitoring Stations in China", 
       x = "Longitude", 
       y = "Latitude") +
  theme_bw()
#>
```

#< quiz "Monitoring_Stations_China"
question: What conclusions can be drawn from the distribution of air quality monitoring stations in China?
sc:
- Monitoring stations are concentrated in regions with higher population density and pollution levels, suggesting targeted efforts in urban and industrial areas.*
- The distribution of monitoring stations is uniform across the country, ensuring equal coverage in all regions.
- Most monitoring stations are located in remote and rural areas to capture background pollution levels.
- There is no clear relationship between the placement of monitoring stations and pollution levels.
success: Well done!
failure: Not quite.
#>

The graphic above displays the geographic distribution of air quality monitoring stations across China. Each point represents a station that collects data on air pollutants, including PM2.5 concentrations, to provide insights into air quality across different regions. The stations are strategically placed to cover urban and rural areas, ensuring comprehensive monitoring and accurate data collection.

Comparing this map to the previously shown PM2.5 distribution graphic, we can observe that monitoring stations are densely concentrated in regions where higher pollution levels were detected, particularly in eastern and southern China. This suggests a targeted effort to monitor areas with significant pollution sources such as industrial zones, high population densities, and major transportation hubs.

The placement of air pollution monitors in China was determined by detailed instructions from the *Ministry of Environmental Protection* (MEP). These monitors were installed in the “built-up areas”, which correspond to the main urban centers of prefecture-level cities. The number of monitors allocated to each city depended on factors such as the city’s population size and the geographical area of the built-up region.
The exact geographical positioning of the monitors was determined using a simulation method that considered factors such as surrounding buildings, traffic patterns, and seasonal wind directions.

We examine this topic in more detail in *Chapter 3.1* at the firm level, focusing on how monitor proximity affects enforcement, and in *Chapter 4.1* at the city level, assessing how monitor placement shapes pollution measurement. This two-level perspective highlights the broader role of monitoring in environmental governance.


#< quiz "monitor_bias_check"
question: What do you think? Was the placement of pollution monitors in China influenced by previous local enforcement activity? 
sc:
- Yes, monitors were placed where previous enforcement was weak.
- Yes, monitors were concentrated in high-complaint areas.
- No, placement was based on past regulatory patterns.
- No, the placement was independent of prior enforcement activity.*
success: Correct!
failure: Not quite. 
#>



Axbard and Deng (2024) find no evidence that historical enforcement activity influenced the placement of air pollution monitors. In other words, monitor locations were not biased by previous regulatory actions. This enhances the credibility of the data, as it indicates that the recorded pollution levels reflect actual exposure rather than being driven by strategic placement aligned with enforcement patterns.

We will examine this topic more closely in *Chapter 4*, where we analyze the monitoring network at the city level and assess its implications for accurately measuring air pollution.

The data collected from these monitoring stations are crucial for analyzing pollution trends, assessing compliance with air quality standards, and developing mitigation strategies. The information provided in the paper offers further insights into the methodology used to analyze PM2.5 levels, highlighting factors such as emission sources, meteorological influences, and seasonal variations.

By examining both maps together, we can better understand the relationship between pollution levels and the availability of monitoring infrastructure, which plays a key role in informing policies aimed at improving air quality across China.


**Summary**

We have visualized the spatial distribution of PM2.5 pollution in 2010 alongside the locations of subsequently introduced air quality monitors. Crucially, the placement of these monitors was not influenced by prior enforcement activity, which reduces concerns about selection bias and enables a more accurate assessment of regulatory effects.

In the following exercise, we will take a closer look at the underlying datasets to further examine the recorded air pollution levels and explore their policy implications.

#< award "Mapping the Problem" >
Level unlocked! You’ve mapped out the pollution hotspots and monitor zones. Ready to deploy some policy power-ups?
#>

## Exercise 2 -- Data Overview


In this chapter, we provide a detailed overview of the structure and composition of the data used in our analysis. A thorough understanding of the underlying datasets is crucial for interpreting the results and assessing the credibility of our empirical approach.

We begin by outlining the main data sources, which include administrative records on environmental enforcement from local bureaus, satellite-based pollution measurements (AOD), real-time data from ground-level air quality monitors, and socioeconomic information on firms and cities. The integration of administrative and remote-sensing data allows for a comprehensive assessment of both pollution outcomes and enforcement behavior.

Next, we focus on the primary dataset used throughout the analysis, referred to as *firm_enf.dta*. This dataset links firm-level enforcement records with spatial, temporal, and environmental variables. Since related datasets at the city and monitor levels are largely derived from the same structure and share key variables, understanding *firm_enf.dta* provides a strong foundation for interpreting the empirical results and conducting subsequent analyses.


### Structure

2.1 Description of the Data Sources

2.2 Introduction to the Data Set


## Exercise 2.1 -- Description of the Data Sources


The dataset used in this analysis comprises several key sources, categorized into proprietary and public data. The proprietary data is available only in anonymized versions, with all identifying information removed. Each source contributes unique insights, enabling a comprehensive evaluation of air pollution trends and enforcement activities.

The Proprietary Data consists of:

**Enforcement Records:**
The enforcement data, obtained from the *Institute of Public & Environmental Affairs* (IPE), monitors regulatory compliance in China and contains anonymized information on actions taken against firms violating air quality regulations, such as fines, warnings, production suspensions, and equipment upgrades. As documented in Axbard and Deng (2024), the dataset includes **55,184** enforcement records collected between 2010 and 2017 across **177** prefecture-level cities, providing a detailed basis for classifying violation types (air, water, waste, procedural) and assessing the intensity of regulatory actions.


**Firm Characteristics:**
In addition to enforcement data, the dataset includes firm-level information from the *Annual Survey of Industrial Firms* (ASIF), conducted by the *National Bureau of Statistics*. It contains detailed data on firms’ economic performance, including revenue, production, employment, energy use, and environmental spending. This allows us to examine how firm characteristics relate to regulatory compliance and how enforcement impacts firm outcomes. As with the enforcement data, all identifiers have been removed to ensure confidentiality.


**City Boundaries:**
Geospatial data from the *Environmental Systems Research Institute* (ESRI) provides detailed boundaries for Chinese prefecture-level cities. As we have seen in *Chapter 1.2*, these boundaries form the spatial framework of our analysis. They allow us to link enforcement and pollution data to specific regions, enabling the identification of spatial patterns and pollution hotspots. This supports geospatial analysis and helps visualize enforcement activity in relation to pollution sources and population centers.


<br/>




The Public Data consists of:

**Pollution Recordings:**
The dataset includes daily air pollution measurements obtained from China’s *Ministry of Environmental Protection* (MEP). This data provides detailed records of PM2.5 concentrations across a nationwide network of monitoring stations.
The pollution recordings are crucial for assessing temporal trends in air quality and evaluating the effectiveness of enforcement measures in reducing pollutant levels. The data is available at high temporal and spatial resolutions, allowing for in-depth analyses of pollution fluctuations and the impact of policy interventions. The historical data, covering multiple years, is particularly valuable for conducting before-and-after comparisons of pollution levels following regulatory changes. This forms the basis for the difference-in-differences (DiD) analyses presented in the following chapters.

**Weather Data:**
Weather conditions significantly influence air pollution levels. To control for these effects, our dataset includes meteorological variables from the *China Meteorological Administration*, such as temperature, humidity, wind, and precipitation. While we do not directly analyze weather patterns in our work, incorporating this data allows us to account for natural variation in air quality and better isolate the effects of regulatory interventions.


**Aerosol Optical Depth (AOD) Data:**
In addition to ground-based pollution measurements, the dataset includes satellite-derived pollution indicators from *NASA*, specifically Aerosol Optical Depth (AOD) data. AOD measures the density of particulate matter in the atmosphere by detecting the scattering and absorption of sunlight by airborne particles.
This data provides an additional perspective on pollution levels, complementing ground-based measurements by offering broader spatial coverage and identifying long-range transport of pollutants. AOD data is particularly useful for validating PM2.5 measurements and assessing pollution trends in regions where ground-based monitoring is limited. 
    

<br/>


**Summary**

In this chapter, we reviewed the main data sources used in the analysis and understood their structure and purpose. We distinguished between proprietary and public data and explored key variables. This forms the foundation for our upcoming empirical work.

In the next step, we will introduce one specific dataset in more detail to lay the groundwork for our analysis.



## Exercise 2.2 -- Introduction to the Data Set

In this section, we introduce the core dataset used in our analysis, providing an overview of its structure, key variables, and sources. A solid understanding of the data’s composition is essential for accurately interpreting the results and evaluating the effects of air pollution monitoring and enforcement in China.

We begin our analysis by focusing on the dataset *firm_enf.dta*, as it also serves as the starting point for the original authors. This dataset forms a central foundation of the study and provides a representative overview of the overall data structure. Many of the other datasets used throughout the analysis are closely linked to or derived from *firm_enf.dta*, and they share a similar structure. In particular, they contain the same key variables that are crucial for the broader empirical investigation.

For this reason, it is sufficient at this stage to focus on *firm_enf.dta*, as it provides the necessary foundation for both data exploration and the development of the empirical models presented in later chapters.

The dataset is provided in *.dta* format and the accompanying code is written for *Stata version 17*. Since base *R* does not natively support *.dta files*, we use the `read_dta()` function from the `haven` package to import the data. A brief explanation of how this function works is provided in the info box below for those unfamiliar with it.

#< info "Loading data with read_dta()."

To import *Stata* *.dta* files into *R*, we use the `read_dta()` function from the `haven` package.

```{r eval=FALSE}
# Load the package
library(haven)

# Read the Stata file
read_dta("data.dta")
```

To make the data easier to work with later, it’s common to store it in a variable, here named `dat`:
```{r eval=FALSE}
# Store the data in a variable
dat <- read_dta("data.dta")
```
After completing these steps, the *Stata* file is successfully loaded and available in R for further processing or analysis.

Additional information on `read_dta()` is available in the following [read_dta() documentation](https://haven.tidyverse.org/reference/read_dta.html).

#>


**Task:**  Load the `haven` package by calling the `library()` function.

```{r "2.2.1"}

#< task
#use library() to load the haven package
#>
library("haven")

```
After successfully loading the library `haven`, let us use the function `read_dta()` to read the data set.


**Task:** Use `read_dta()` to read the file *firm_enf.dta* and save it in a variable called `data`. 

**Note**: The file *firm_enf.dta* is quite large, so loading it may take some time. If you are working within a Shiny app, you may experience technical issues such as freezing or repeated disconnections. The exact cause is unclear, but it is likely related to the file size and how Shiny handles memory.
Therefore, it is recommended to skip the entire *Chapter 3* when using the Shiny version of this problem set, as all exercises in this chapter rely on *firm_enf*.

```{r "2.2.2}

#< task
#use read_dta() to load "firm_enf.dta" and save it in data. 
#>
data <- read_dta("firm_enf.dta")

```
#< award "Patience" >
That took a moment... but great things take time! Your dataset is ready, and so are you.
#>


As you may have noticed, the *.dta* file takes a considerable amount of time to load. This is a common issue when working with large *Stata* files in *R*, as loading via the `haven` package can be relatively slow due to internal data conversions and metadata handling.

To avoid repeated delays and improve performance, I have converted all original datasets from the paper into the *.rds* format. As a format native to *R*, *.rds* allows for significantly faster and more efficient processing of large datasets, resulting in a smoother workflow throughout the analysis.

#< info "Loading data with read_rds()."

The `read_rds()` function, part of the `readr` package in the tidyverse, is used to read a single *R* object stored in a *.rds* file. Similar to `readRDS()` from base *R*, it loads one object at a time, but integrates more seamlessly with the tidyverse workflow.

One key advantage is its compatibility with piping `(%>%)` and tidyverse-style data processing, making it a preferred choice in many modern *R* workflows.

This makes it particularly useful when you want to load data into a specific object without affecting your current environment.

```{r eval=FALSE}
# Load the contents of an .rds file using readr
library(readr)
read_rds("data.rds")

# Assign the loaded object to a variable
df <- read_rds("data.rds")
``` 

For more detailed information about the `read_rds()` function, please refer to the following [read_rds() documentation](https://readr.tidyverse.org/reference/read_rds.html).
#>  

Now, let us load the same dataset in its *.rds* format, which offers faster loading and better performance in *R*. We use the `read_rds()` function from *R's* `readr` package.

**Task:** First, load the `readr` package. Then, use `read_rds()` to read the file *firm_enf_clean.rds* and save it in a variable called `dat`. Note: For future work, I have cleaned the dataset by removing unnecessary variables that are not relevant for our analysis. This ensures a streamlined dataset and cleaner output later.

```{r "2.2.3}


#< task
#Load the readr package and use read_rds() to load "firm_enf_clean.rds" into the variable dat
#>
library(readr)
dat = read_rds("firm_enf_clean.rds")

```
**Note:** The *firm_enf* dataset, originally close to 2 GB in *DTA* format, shrank to less than 50 MB after being converted to an *RDS* file. 

With the dataset successfully loaded into `dat`, we can now take a closer look at its structure and the parameters it contains.

There are multiple ways to gain an overview of a dataset. One common approach is using the `head()` function in *R*, which displays the first six rows of `dat`. Alternatively, we can examine a random subset of six rows using the `sample_n(data, rows)` function. These methods help us familiarize ourselves with the variables and better understand how the dataset is organized.

**Task:** Apply the `head()` function to `dat` and set the number of rows (`n`) to 3, in order to display only the first three observations of the dataset.

```{r "2.2.4"}

#< task
#use head() to show the first three rows of dat
#>
head(dat, 3)

```
These are the first three rows of our dataset, highlighting some of the key variables it contains. Each row represents a firm-level observation for a specific year and quarter. The dataset links firms to the nearest air quality monitor, combining information on their location, industry, revenues, and employment with regulatory enforcement actions and pollution exposure. This structure allows us to analyze how proximity to monitoring stations affects local environmental enforcement.

**Monitoring Stations and Locations**

The columns `lat` and `lon` contain the geographic coordinates of each monitoring station, allowing for spatial mapping and detailed geographic analysis. These coordinates help visualize the distribution of monitoring stations across different regions, enabling us to examine whether enforcement actions are concentrated in specific areas.

The `city_id` column serves as a unique identifier for each city, linking pollution measurements and enforcement records to specific locations. This allows us to analyze how enforcement intensity varies across different cities and whether certain urban areas experience more regulatory actions than others.

Additionally, the `min_dist` column records the minimum distance between a firm and the nearest monitoring station. This variable is particularly important for assessing the impact of pollution monitoring on enforcement decisions. By analyzing how firms located closer to monitoring stations experience regulatory actions compared to those further away, we can evaluate whether proximity to monitors increases the likelihood of enforcement measures being taken.

**Pollution Enforcement Indicators**

The dataset includes several key columns that track enforcement actions taken against firms in response to pollution violations. These indicators provide valuable insights into the intensity of regulatory enforcement and help assess how strictly pollution regulations are applied across different locations and time periods.

For instance, the column `any_air_fine` indicates whether a firm was issued a fine for air pollution violations, serving as a direct measure of monetary penalties imposed by regulators. Similarly, `any_air_warning` records whether a firm received an official warning for non-compliance with air quality regulations. These variables help us evaluate the extent to which firms are held accountable for their environmental impact.

By analyzing these enforcement indicators, we can explore patterns in regulatory actions, identify factors influencing enforcement intensity, and examine whether stricter monitoring leads to increased penalties or warnings for firms violating pollution standards.


**Environmental Data**

The dataset also includes key environmental variables that influence pollution levels and dispersion. The `tem_mean` column records the average temperature, which affects air pollution dynamics, such as smog formation and pollutant accumulation.

Wind-related variables, `angle` and `upwd`, provide wind direction and upward airflow, crucial for understanding pollution dispersion. Strong winds can reduce local pollution, while stagnant conditions may lead to accumulation. These factors help analyze how natural conditions interact with pollution levels and enforcement actions.

**Time and Policy Impact**

The `year` and `quarter` columns specify the time of each observation, allowing us to track trends in pollution and enforcement across different periods. These variables help identify seasonal patterns and observe longer-term developments in regulatory activity.

A key variable in our analysis is `post1`, which indicates whether an observation was recorded after the introduction of pollution monitors. This variable is essential for evaluating the effects of the monitoring policy on enforcement intensity and pollution levels. To assess these effects, we focus on data from 2015, the year in which the monitoring system was introduced. By comparing enforcement actions and pollution outcomes before and after this point in time, we can examine how monitoring influenced regulatory responses and environmental conditions.

This temporal variation will also play a central role in our difference-in-differences (DiD) approach later on, where we rely on before-and-after comparisons to estimate causal effects of the policy intervention.

So, let's see what happens when we filter the data set for the year 2015 to examine differences in the `post1` column. This will allow us to analyze variations in enforcement and pollution levels before and after the introduction of monitoring.

#< info "Filter data with filter()."

The `filter()` function from the `dplyr` package is used to select rows from a data frame based on logical conditions. It is particularly helpful when working with large datasets and focusing on specific observations that meet certain criteria. For instance, it can be used to filter data by `year`, `region`, or any other variable of interest.

Before using `filter()`, make sure to load the `dplyr` package:

```{r eval=FALSE}
#Load package 
library(dplyr)
``` 


```{r eval=FALSE}
#Usage of filter
filter(data, condition1, condition2, ...)
``` 
- data: The data frame you want to filter.
- condition: One or more logical expressions that determine which rows to keep.

Only rows where the condition evaluates to `TRUE` will be returned.

Useful filter functions (and operators), that can be applied when constructing logical expressions to filter the data are:

- `==`, `>`, `>=`, `<`, etc
- `&`, `|`, `!`, `xor()`
- `ìs.na()`
- `between()`, `near()`

The original dataset remains unchanged unless you assign the result to a new object.

`filter()` is especially powerful when used as part of a `dplyr` pipeline (`%>%`), allowing for clean and readable data transformations. 

For more detailed information about the `filter()` function, please refer to the following [filter() documentation](https://dplyr.tidyverse.org/reference/filter.html).
#>  

#< info "Use the pipe operator %>% for cleaner code."

The pipe operator `%>%` is a core feature of the `dplyr` and `tidyverse` packages. It is used to write clear, readable code by linking together a sequence of data manipulation steps.

Instead of wrapping functions inside each other, `%>%` passes the result from one function directly into the next. This makes it easier to understand the flow of operations when transforming data.

```{r eval=FALSE}
# General structure
data %>%
  step1() %>%
  step2() %>%
  step3()
```
What is the purpose of `%>%` in data manipulation?

- Keeps code clean and readable
- Avoids complex nesting
- Mirrors the logical order of data processing steps

It is most commonly used with functions like `filter()`, `mutate()`, `select()`, `arrange()`, and `summarise()`.

Additional information is available in the following [pipe operator %>% documentation](https://uc-r.github.io/pipe).
#>


**Task:** Start by loading the `dplyr` package.

```{r "2.2.5"}

#< task
#load the library "dplyr"
#>
library(dplyr)

```

After loading the `dplyr` package, we now want to filter the dataset for the year 2015 and display the first three rows.

**Task:** Fill in the blanks to filter the dataset for the year 2015 and display the first three rows again.

```{r "2.2.6"}
#< fill_in
# Filter the dataset to include only observations from the year 2015 

dat_2015 = dat %>%
  filter(year == ___)

head(dat_2015, 3)
#>

dat_2015 = dat %>%
  filter(year == 2015)

head(dat_2015, 3)

```


#< quiz "Observing_Post1_Changes"
question: What does the `post1` column show after filtering the dataset for the year 2015?
sc:
- The post1 values are still a mix of 0 and 1, reflecting both pre- and post-monitoring data.
- All post1 values are now 1, meaning all observations fall within the post-monitoring period.*
- The number of post1 = 0 values has increased, indicating more pre-monitoring observations.
- The post1 variable is missing from the filtered dataset, suggesting a processing error.
success: Correct!
failure: Not quite.
#>


The 2010 and 2015 data differ significantly due to the introduction of pollution monitors in 2015. This shift is captured by the `post1` variable, which takes the value 1 for observations from 2015, indicating the post-monitoring period, and 0 for 2010, representing the pre-monitoring phase.
With the implementation of monitors in 2015, we can examine how enforcement behavior changed in response. In particular, we focus on variables such `as any_air_fine`, `any_air_warning`, and other indicators of enforcement activity. This distinction enables us to assess whether the presence of monitoring led to stronger regulatory responses and increased enforcement intensity.

We will explore this in greater detail in *Chapter 3.1*, where we analyze differences in enforcement patterns before and after the introduction of the monitoring system.

**Summary**

In this sub-exercise, we took a closer look at the first dataset, which serves as a foundational building block for the analyses that follow. By exploring its structure and identifying key variables related to pollution enforcement and monitoring, we established the basis for evaluating policy impacts in the upcoming chapters.

Moving forward, we will examine whether the introduction of pollution monitors increased enforcement intensity and improved air quality, laying the groundwork for a difference-in-differences analysis.


## Exercise 3 -- Firm-Level Analysis of Monitoring Impacts


In this chapter, we want to examine how the introduction of pollution monitors affected environmental enforcement at the firm level, using a difference-in-differences (DiD) framework. Our main goal is to identify whether firms located closer to monitors experienced a higher likelihood of enforcement actions compared to those farther away.

First, we describe how monitors were assigned and placed across cities based on population size and urban area. This centrally coordinated process supports the assumption that monitor placement was exogenous to local enforcement behavior.

Next, we outline the conceptual framework of the DiD approach, which allows us to compare changes in enforcement between treated and control firms over time.

We then estimate the DiD model using regression analysis. To ensure valid inference, we apply cluster-robust standard errors that account for within-group correlation, and include fixed effects to control for unobserved heterogeneity across firms and over time.

This stepwise approach enables us to assess the causal impact of pollution monitoring on enforcement intensity with increasing precision.


### Structure

3.1 Assignment and Location of Monitors

3.2 Conceptual Framework of the DiD Approach

3.3 Estimating the DiD Model Using Regression

3.4 Cluster-Robust Standard Errors

3.5 Fixed Effects



## Exercise 3.1 -- Assignment and Location of Monitors

In this section, we want to analyze how pollution monitors were assigned to cities and where they were placed. We examine the criteria used by the *Ministry of Environmental Protection* (MEP) to determine the number and location of monitors, as well as whether their placement was influenced by pre-existing enforcement patterns or pollution trends. By comparing enforcement activities and pollution levels before and after the introduction of monitors, we aim to assess whether monitor placement was neutral or strategically targeted to high-pollution areas.


**Task:** Just click on `check` to read the data set and save it in `dat`.

```{r "3.1.1"}

#< task

dat = read_rds("firm_enf_clean.rds")
#>

```
In the previous chapter, we explored the structure of the *firm_enf_clean* dataset. Now, we want to analyze how the placement of pollution monitors relates to enforcement actions by examining whether proximity to a monitor influences the likelihood of regulatory enforcement.

To do this, we will create a scatterplot showing how the probability of air pollution-related enforcement actions changes depending on a firm's distance from the nearest air quality monitor. By comparing data from the pre-monitoring period (2010–2014) with the post-monitoring period (2015–2017), we aim to identify whether the introduction of monitors led to observable shifts in enforcement patterns. This analysis will help determine if regulatory actions intensified near monitoring stations and whether monitor placement played a role in shaping enforcement trends.

To begin, we will filter the dataset to include only firms located within 50 km of a monitor and those that started operations in 2010 or earlier.

**Task:** Replace the ___ placeholders with the correct parameter to filter the dataset and save it to `dat_filtered`. 

```{r "3.1.2"}
#< fill_in the
dat_filtered <- dat %>%
  filter(min_dist < __, starty <= __) 
#>
dat_filtered <- dat %>%
  filter(min_dist < 50, starty <= 2010)

```
Companies that are more than 50 kilometers away or were founded after 2010 are now excluded from the data set.

We now want to add a new column, `post`, using the `mutate()` function from the `dplyr` package. This column will indicate whether the observation belongs to the post-policy period (2015–2017) if `post1` is equal to 1, or to the pre-policy period (2010–2014) if `post1` is equal to 0.

#< info "Add or modify columns with mutate()."

The `mutate()` function from the `dplyr` package is used to create new variables or transform existing ones in a data frame. It’s ideal for tasks like recoding values, generating indicators, or applying mathematical operations to columns.

```{r eval=FALSE}
# Usage of mutate
mutate(data, new_column = transformation)
``` 
- data: Your data frame.
- new_column: The name of the column to add or modify.
- transformation: An expression or condition used to define the new values.

Typical use cases include:

- Creating labels from numeric codes: mutate(data, status = ifelse(code == 1, "Active", "Inactive"))

- Applying calculations: mutate(data, log_income = log(income))

`mutate()` is especially powerful within a `%>%` pipeline, keeping code readable and efficient.

For more detailed information about the `mutate()` function, please refer to the following [mutate() documentation](https://dplyr.tidyverse.org/reference/mutate.html).
#>  

**Task:** Add a new column, `post`, to classify observations as "post-policy (2015-2017)" if `post1` is 1, otherwise as "pre-policy (2010-2014)"if `post1` is 0.
Use the `mutate()` function from the `dplyr` package to create the new column. This function allows you to modify or add variables to a dataset efficiently.

```{r "3.1.3"}
#< fill_in 

dat_filtered <- dat_filtered %>%
mutate(post = ifelse(post1 == __, "Post-policy (2015-2017)", "Pre-policy (2010-2014)"))
head(dat_filtered, 3)
 
#>
dat_filtered <- dat_filtered %>%
mutate(post = ifelse(post1 == 1, "Post-policy (2015-2017)", "Pre-policy (2010-2014)"))
head(dat_filtered, 3)

```

After successfully adding the `post` column, we can now use it to analyze differences between the pre-policy (2010-2014) and post-policy (2015-2017) periods. This classification appears as a new column on the far right of the table.

To analyze the relationship between enforcement probability and distance to the nearest monitor, we will use the `ggplot()` function from the `ggplot2` package, which is part of the tidyverse collection. The `ggplot2` package is widely used for data visualization in *R*, allowing us to create clear and customizable graphics.


#< info "How to create and customize grouped scatter plots with ggplot2."

`ggplot2` provides a flexible framework for visualizing grouped data, such as comparing outcomes across categories or over time. A common use case is plotting group-specific means or proportions using scatter plots, with options to manually adjust aesthetics like color, shape, axis scales, and layout.

This approach is especially useful when visualizing trends across subgroups (e.g. treatment vs. control) or when working with binned summary data.

Before you begin, make sure to load the `ggplot2` package, which provides powerful tools for creating customizable plots.

**Step 1: Basic plot setup with grouping**

You start by passing your dataset to `ggplot()` and specifying the aesthetic mappings:

```{r eval=FALSE}
ggplot(data, aes(x = x_var, y = y_var, color = group_var, shape = group_var)) +
geom_point(size = point_size)
``` 
- x_var is typically a variable like time, distance, or categorical bins.
- y_var contains the outcome to plot, usually an average or proportion.
- group_var indicates groups you want to visually separate (e.g., by time period, region, or treatment status).

 **Step 2: Customizing appearance with manual scales**

You can manually define the colors and shapes used for each group:

```{r eval=FALSE}
+ scale_color_manual(values = c("color1", "color2", ...))
+ scale_shape_manual(values = c(shape1, shape2, ...))
``` 

**Step 3: Formatting axes and labels**

To improve the readability of your axes, you can define custom labels and breaks:

```{r eval=FALSE}
+ scale_x_continuous(breaks = ..., labels = ...)
+ scale_y_continuous(limits = ..., breaks = ...)

```

Use `scale_x_discrete()` when your x-variable represents grouped categories or bins (e.g., time periods or distance bands).

Add descriptive labels and a plot title with:

```{r eval=FALSE}
+ labs(x = "X-axis Label", y = "Y-axis Label", title = "Plot Title", color = "Legend Label", shape = "Legend Label")

``` 

**Step 4: Styling with themes**

Use a minimal base theme and customize layout, font sizes, and legend placement for a clean and professional look.


For now, we are going to use the following layers and their arguments: 
```{r eval=FALSE}
+ theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    legend.text = element_text(size = ...),
    axis.title = element_text(size = ...),
    axis.text = element_text(size = ...),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = ..., size = ...)
  )
``` 
This type of plot is ideal when:
- Showing differences in trends across groups (e.g., before/after a policy change)
- Visualizing summarized or binned data
- Comparing values across categories like regions, treatments, or time periods
- Creating clear, publication-ready plots with group-level information

If you need more information on `ggplot2` and its functions, you can follow this link [ggplot2() documentation](https://ggplot2.tidyverse.org/reference/).
#>  

In this case, we will generate a binned scatterplot, which summarizes the probability of enforcement actions related to air pollution based on a firm's proximity to the nearest monitor. This visualization will help us determine whether enforcement intensity changed after the introduction of monitors and whether firms closer to monitors faced stricter regulatory actions.

**Task:** First, load the `ggplot2` package using the `library()` function.

```{r "3.1.4"}

#< task
# Load the ggplot2 package
#>
library(ggplot2)

```

After loading the `ggplot2` library, we will now visualize how the likelihood of air pollution-related enforcement changes with a firm's distance to the nearest monitor, distinguishing between the pre- and post-monitoring periods using the `post` variable.

**Task** Fill in the blanks to create a binned scatterplot showing how the probability of air pollution-related enforcement `(any_air)` varies with distance to the nearest monitor `(min_dist)`, before and after monitors were introduced `(post)`.

```{r "3.1.5", warning=FALSE}
#< fill_in
# Create a binned scatterplot of enforcement vs. monitor distance
ggplot(dat_filtered, aes(x = _________, y = ____________, color = post, shape = post)) +
  stat_summary_bin(fun = mean, bins = 20, geom = "point") +
  labs(x = "Distance to the Closest Monitor (km)", 
       y = "Any Air Pollution Related Enforcement") +
  scale_color_manual(values = c("#DC143C", "black")) +
  scale_shape_manual(values = c(16, 18)) +
  theme_minimal() +
  theme(legend.position = "top", 
        legend.title = element_blank(),
        plot.background = element_rect(fill = "white", color = NA))

#>
ggplot(dat_filtered, aes(x = min_dist, y = any_air, color = post, shape = post)) +
  stat_summary_bin(fun = mean, bins = 20, geom = "point") +
  labs(x = "Distance to the Closest Monitor (km)", 
       y = "Any Air Pollution Related Enforcement") +
  scale_color_manual(values = c("#DC143C", "black")) +
  scale_shape_manual(values = c(16, 18)) +
  theme_minimal() +
  theme(legend.position = "top", 
        legend.title = element_blank(),
        plot.background = element_rect(fill = "white", color = NA))

```

#< quiz "Enforcement_Gradient"
question: What does the enforcement gradient graph primarily show about the relationship between distance to monitors and enforcement likelihood?
sc:
- Enforcement was consistently high regardless of distance or time period.
- Distance to monitors had no effect on enforcement likelihood.
- Enforcement increased significantly near monitors after their introduction (2015–2017).*
- The introduction of monitors reduced enforcement activities overall.
success: Excellent, your answer is correct!
failure: Try again. 
#>

The graph shows that the introduction of air pollution monitoring significantly increased the enforcement of environmental regulations. During the pre-policy period (2010–2014), the likelihood of enforcement was independent of distance and averaged **0.21%**. After the monitors were introduced (post-policy 2015–2017), enforcement likelihood increased substantially, especially near the monitors, and decreased with greater distances. This demonstrates that the monitors motivated local authorities to enforce regulations more effectively and target polluters located closer to the monitors.
Additionally, the consistent enforcement activity before 2015 shows that the probability of enforcement was not affected by the distance to the planned monitor locations. This suggests that the placement of monitors was not determined strategically or based on existing enforcement patterns. Instead, the monitors were likely allocated based on objective factors such as city size, population, or urban characteristics, rather than local enforcement levels.


Next, we analyze trends in environmental enforcement by comparing firms based on their distance to air pollution monitors. The treatment group consists of firms located within 10 kilometers of a pollution monitor. These firms are expected to face increased regulatory scrutiny, as their emissions are more directly observable due to closer monitoring. In contrast, the control group includes firms located more than 10 kilometers away from any monitor. These firms serve as a baseline for comparison, under the assumption that they are less exposed to direct monitoring and, consequently, to immediate enforcement pressure.

This analysis aims to examine whether both groups followed similar enforcement trends prior to 2015, thereby allowing us to assess whether any subsequent changes were driven by the installation of the monitors rather than by pre-existing differences.

In the following chapter, where we conduct the difference-in-differences (DiD) analysis, we will revisit this treatment-control framework in greater detail to formally assess the causal impact of the pollution monitors.


**Task:** For now, just `check` the following chunk to visualize enforcement trends over time for firms located near and far from pollution monitors.

```{r "3.1.6"}
#< task_notest
bin_means <- dat_filtered %>%
  mutate(min_dist_10 = case_when(
    min_dist < 10 ~ "0-10 km",
    min_dist < 50 ~ "10-50 km",
    TRUE ~ NA_character_
  )) %>%
  group_by(year, min_dist_10) %>%
  summarise(mean_any_air = mean(any_air, na.rm = TRUE), .groups = 'drop')

ggplot(bin_means, aes(x = year, y = mean_any_air, color = min_dist_10, shape = min_dist_10)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "#DC143C")) +
  scale_shape_manual(values = c(18, 16)) +
  labs(x = "Year", y = "Any air-pollution-related enforcement") +
  scale_x_continuous(breaks = 2010:2017, labels = as.character(2010:2017)) +
  scale_y_continuous(limits = c(0.001, 0.013), breaks = seq(0.001, 0.013, by = 0.002)) +
  theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    legend.text = element_text(size = 12),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "gray", size = 0.2)
  )

#>
```

#< quiz "Enforcement_Trends"
question: What does the graph reveal about enforcement trends?
sc:
- Higher enforcement far from monitors.
- No change after 2015.
- Random enforcement patterns.
- Similar trends before 2015, sharper increase near monitors after 2015.*
success: Excellent, your answer is correct!
failure: Try again.
#>



This graph on enforcement trends builds upon the findings of the first graph by demonstrating that enforcement patterns for firms within 0–10 km of a monitor (treatment group) and those within 10–50 km (control group) were similar before 2015. This confirms that monitor placement was not influenced by pre-existing enforcement trends. However, after 2015, enforcement activity increased significantly for firms located closer to monitors, whereas firms further away experienced only a modest increase. This underscores the impact of monitor introduction in driving more targeted and localized enforcement, reinforcing their role in enhancing regulatory compliance and effectively guiding local government efforts.


In the final step, we want to analyze pollution levels and trends based on Aerosol Optical Depth (AOD) in areas where monitors are installed and compare them to surrounding regions. Due to limitations in the AOD data, pollution is measured within three zones: the monitor pixel (approximately 11 km × 11 km), the city center (10–50 km from a monitor), and the broader surrounding area (>50 km). These zones correspond to the treatment and control groups used in the previous enforcement analysis.

**Task:** Just `check` the following chunk to visualize annual AOD levels from 2010 to 2017 across three zones to compare pollution trends before and after monitor installation.

```{r "3.1.7"}
#< task_notest
pm <- read_rds("pm.rds") %>%
  select(-pm25) %>%
  rename(pm3 = pm_indirect, pm2 = pm_direct) %>%
  pivot_longer(starts_with("pm"), names_to = "group", values_to = "pm")

pix <- read_rds("pix.rds")
combined_data <- bind_rows(pm, pix) %>%
  mutate(group = replace_na(group, "1"),
         time = interaction(year, month),
         group = recode(group, "1" = 1, "pm2" = 2, "pm3" = 3),
         group = factor(group, levels = 1:3, labels = c("Monitor (<= 10km)", "City Center (10-50km)", "Surrounding Area (> 50km)")),
         year_bin = as.factor(year))

binned_data <- combined_data %>%
  group_by(year_bin, group) %>%
  summarise(mean_pm = mean(pm, na.rm = TRUE), .groups = "drop")

ggplot(binned_data, aes(x = year_bin, y = mean_pm, color = group, shape = group)) +
  geom_point(size = 3) +
  labs(y = "Aerosol Optical Depth", x = "Year", title = "Aerosol Optical Depth by Location", color = "Group", shape = "Group") +
  scale_color_manual(values = c("blue", "red", "green")) +
  scale_shape_manual(values = c(16, 17, 18)) +
  theme_minimal() + 
  theme(legend.position = "bottom") + 
  scale_y_continuous(limits = c(0.24, 0.48), breaks = seq(0.24, 0.48, by = 0.04)) +
  scale_x_discrete(labels = as.character(2010:2017))

#>
```

The last graph illustrates trends in Aerosol Optical Depth (AOD) from 2010 to 2017 across three areas: locations near monitors (<10 km), the city center (10–50 km), and the surrounding area (>50 km). The results show that AOD levels were consistently higher in areas near monitors compared to the city center and surrounding regions, suggesting that monitors were placed in high-pollution areas. However, despite these differences in absolute levels, all three areas followed a similar downward trend over time. This indicates that pollution reductions were not limited to monitored areas but were part of a broader regional decline. The findings support the idea that monitor placement was based on initial pollution levels rather than local pollution trends.

**Summary**

In this section, we examined how pollution monitors were assigned and whether their placement influenced enforcement. We found that enforcement increased significantly after 2015, especially near monitors, while pre-monitoring patterns were similar across distances. This suggests that monitor placement was not driven by prior enforcement activity. AOD data show that monitors were placed in more polluted areas, but all regions followed similar downward pollution trends, supporting the credibility and neutrality of monitor assignment.

Building on these findings, the next chapter introduces the conceptual framework of the difference-in-differences (DiD) approach. This framework allows us to formally assess the causal impact of pollution monitoring by comparing changes in outcomes over time between firms with different exposure to monitoring.

## Exercise 3.2 -- Conceptual Framework of the DiD Approach

Building on our previous analyses, we now turn to a difference-in-differences (DiD) approach to estimate the impact of pollution monitoring on regulatory enforcement at the firm level. So far, we have examined this relationship through the enforcement gradient, long-term trends, and the influence of aerosol optical depth (AOD) on regulatory actions. These approaches have provided valuable insights into how enforcement intensity evolves in response to pollution monitoring.

To further quantify this effect, we now apply a simplified DiD specification, comparing firms located within 10 km of a monitor to those at a greater distance. This allows us to isolate the causal impact of monitoring presence on enforcement activity by leveraging variation across both time and distance.

The difference-in-differences (DiD) estimator is a widely used method for identifying causal effects by comparing changes in outcomes over time between a treatment and a control group, under the assumption that both would have followed similar trends in the absence of treatment (Bertrand, Duflo, & Mullainathan, 2004, p. 249).

#< quiz "treatment_control_group"

question: Which firms are defined as the treatment group in the difference-in-differences (DiD) framework used in this study? This was briefly discussed in the previous chapter.
sc:
- Firms located within 10 km of a monitor*
- Firms located beyond 10 km of a monitor
- Firms in the same industry as those near a monitor
- Firms with higher-than-average pollution levels

success: Great, your answer is correct!
failure: Try again.
#>

In our analysis, firms are categorized into two distinct groups based on their proximity to pollution monitors: the treatment group, which consists of firms located within 10 km of a monitor, and the control group, which includes firms beyond this distance. 

The underlying assumption of this method is that, in the absence of treatment, both groups would have followed similar trends in enforcement probability over time. By comparing changes in enforcement between the two groups before and after the introduction of monitoring, we can estimate the causal impact of regulatory oversight.

Firms within 10 km of a monitor are expected to experience greater scrutiny from environmental regulators, leading to a higher likelihood of enforcement actions. In contrast, firms beyond this threshold serve as a comparison group, reflecting what would have happened to the treatment group in the absence of monitoring. This approach effectively controls for broader economic and policy trends that may influence enforcement across all firms, ensuring that our estimated effect is driven specifically by the presence of pollution monitors.

#< quiz "pollution_monitoring_treatment_effect"
question: What does a higher likelihood of enforcement actions for firms near pollution monitors indicate about the treatment effect?
sc:
- The treatment effect is negative.
- There is no treatment effect.
- The treatment effect is positive.*
- The treatment effect is spurious.
success: Great, your answer is correct!
failure: Try again.
#>



Having established the treatment and control groups, we now proceed to manually compute the difference-in-differences (DiD) estimator. This step allows us to directly observe how enforcement probabilities change over time for firms near pollution monitors compared to those further away. By performing the calculation step by step, we can gain a deeper understanding of how DiD isolates the causal effect of environmental monitoring on enforcement actions.


The formula to compute the difference-in-differences (DiD) estimator is given by:


$$
DiD = (\bar y _{exp,tr} - \bar y _{exp,co}) - (\bar y _{pre,tr} - \bar y _{pre,co})
$$
following the standard notation in Wooldridge (2010, p. 130). The terms represent the average outcomes for the following groups:

* the treatment group during the experimental period $\bar y _{exp,tr}$(firms within 10 km of a monitor after its introduction);
* the control group during the experimental period $\bar y _{exp,co}$(firms beyond 10 km of a monitor after its introduction);
* the treatment group during the pre-experimental period $\bar y _{pre,tr}$(firms within 10 km of a monitor before its introduction);
* the control group during the pre-experimental period $\bar y _{pre,co}$(firms beyond 10 km of a monitor before its introduction).

<br/>

In this section, we will manually apply the difference-in-differences (DiD) formula to our dataset. The goal is to compute the average enforcement probability for both the treatment and control groups, before and after the introduction of pollution monitoring. This results in four mean values, which will then be used to calculate the treatment effect.

To keep things clear, we’ll break the process into two steps. First, we calculate the group averages as defined by the DiD setup. Then, we use these averages to compute the final DiD estimator, which captures the effect of pollution monitoring on enforcement actions.

Before we proceed, we’ll load the dataset. From this point on, we’ll be working with a pre-filtered version that I’ve already cleaned and transformed based on the adjustments introduced in the previous chapter. Variables such as the minimum distance to the nearest monitor, the monitor start year, and other key filters have already been applied. This allows us to skip repetitive preprocessing steps and focus directly on the analysis.


**Task:** Just `check` the chunk that saves the filtered data set into `dat`.

```{r "3.2.1"}

#< task
#read firm_enf.rds with read_rds() and save it in dat
#>
dat <- read_rds("filtered_firm_enf.rds")

```

We now prepare the dataset for the difference-in-differences (DiD) analysis by generating the key interaction variable `min_dist_10_post1`. This variable combines two binary indicators: `min_dist_10`, which identifies firms located within 10 kilometers of a pollution monitor, and `post1`, which marks observations from the period after monitoring was introduced. The interaction equals one only for firms in the treatment group during the post-monitoring period. It serves as the core variable in our DiD specification, capturing the impact of pollution monitoring on enforcement actions for nearby firms.

**Task:** Fill in the blanks (_) to create the interaction term `min_dist_10_post1`, which identifies firms that are both within 10 km of a pollution monitor and observed in the post-monitoring period.

```{r "3.2.2"}

#< fill_in
# Create interaction term for DiD: firms within 10 km and in post-monitoring period
dat <- dat %>%
  mutate(min_dist_10_post1 = ______*______)

#>

dat <- dat %>%
  mutate(min_dist_10_post1 = min_dist_10 * post1)
```


After creating the interaction term `min_dist_10_post1`, we now want to compute the average enforcement probability for each group using the `group_by()` and `summarise()` functions. Specifically, we group the data by treatment status `(min_dist_10)` and time period `(post1)`, then apply `summarise()` to calculate the mean enforcement outcome `(any_air)` for each group. By grouping the data by treatment status `(min_dist_10)` and time period `(post1)`, we can compare how enforcement actions changed before and after the introduction of pollution monitoring for both the treatment and control groups.


#< info "Grouping and summarizing data with group_by() and summarise()."

The combination of `group_by()` and `summarise()` from the `dplyr` package is a powerful tool for computing summary statistics within groups of a dataset. While `group_by()` defines how the data should be grouped, `summarise()` performs the actual summary operation on each group.

Together, they allow for clear and efficient group-level analysis, commonly used when comparing averages across categories such as time periods, treatment groups, or regions.

```{r eval=FALSE}
# Basic structure
data %>%
  group_by(group_var1, group_var2, ...) %>%
  summarise(new_column = summary_function(target_var), .groups = "drop")
``` 

Explanation of the main components:

- `group_by()`: Specifies one or more variables to group the data by. Each unique combination defines a group.
- `summarise()`: Computes summary statistics (e.g., `mean()`, `sum()`, `n()`) for each group.
- new_column = ...: The name for the output column containing the summary result.
- .groups: Optional argument controlling the structure of the result (e.g., whether grouping is kept or dropped).

For more details about the `group_by()` and `summarise()` functions, you can check out this helpful overview: [Documentation group_by() and summarize()](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/summarise).
#>  

**Task:** Fill in the blanks (_) to compute the average enforcement probability for each group using `group_by()` and `summarise()`.

```{r "3.2.3"}
#< fill_in
# Compute average enforcement probability for each group
mean_values <- dat %>%
  _________(min_dist_10, post1) %>%
  _____________(mean_outcome = mean(any_air, na.rm = TRUE), .groups = "drop")

#>

# Compute average enforcement probability for each group
mean_values <- dat %>%
  group_by(min_dist_10, post1) %>%
  summarise(mean_outcome = mean(any_air, na.rm = TRUE), .groups = "drop")
```

After calculating the average enforcement probabilities for each group, we now extract these values following the DiD formula notation. This step is important as it systematically assigns the mean outcomes to the four key components of the difference-in-differences calculation from the formula above.
Remember, 0 and 1 distinguish between groups: `min_dist_10` = 1 represents firms within 10 km of a monitor (treatment), while `min_dist_10` = 0 represents firms beyond 10 km (control). Similarly, `post1` = 1 indicates the post-monitoring period, and `post1` = 0 refers to the pre-monitoring period.

**Task:** Fill in the blanks (_) by inserting 0 or 1 to correctly extract the mean enforcement probabilities for each group.


```{r "3.2.4"}
#< fill_in
# Extract values based on the DiD formula notation
y_exp_tr  <- mean_values$mean_outcome[mean_values$min_dist_10 == _ & mean_values$post1 == _]  # Treated after monitoring
y_exp_co  <- mean_values$mean_outcome[mean_values$min_dist_10 == _ & mean_values$post1 == _]  # Control after monitoring
y_pre_tr  <- mean_values$mean_outcome[mean_values$min_dist_10 == 1 & mean_values$post1 == 0]  # Treated before monitoring
y_pre_co  <- mean_values$mean_outcome[mean_values$min_dist_10 == 0 & mean_values$post1 == 0]  # Control before monitoring
#>


# Extract values based on the DiD formula notation
y_exp_tr  <- mean_values$mean_outcome[mean_values$min_dist_10 == 1 & mean_values$post1 == 1]  # Treated after monitoring
y_exp_co  <- mean_values$mean_outcome[mean_values$min_dist_10 == 0 & mean_values$post1 == 1]  # Control after monitoring
y_pre_tr  <- mean_values$mean_outcome[mean_values$min_dist_10 == 1 & mean_values$post1 == 0]  # Treated before monitoring
y_pre_co  <- mean_values$mean_outcome[mean_values$min_dist_10 == 0 & mean_values$post1 == 0]  # Control before monitoring

```


After correctly extracting the mean enforcement probabilities for each group, we now have the key values needed for the difference-in-differences (DiD) calculation. These values represent the average enforcement probability for the treatment and control groups before and after the introduction of pollution monitoring.
Before moving on to the final step of computing the DiD estimator, let's quickly check the extracted values to ensure they are correctly assigned and consistent with our expectations.

**Task:**  Click on `check` to show the values of the formula`s components:

```{r "3.2.5"}

#< task_notest
library(kableExtra)

data.frame(
  "PreExp.Treat" = round(y_pre_tr, 5), 
  "Exp.Treat" = round(y_exp_tr, 5), 
  "PreExp.Cont" = round(y_pre_co, 5), 
  "Exp.Cont" = round(y_exp_co, 5)
) %>%
  kbl() %>%
  kable_classic() %>%
  kable_styling(font_size = 15)
#>

```

#< quiz "did_enforcement_increase"
question: Based on the extracted mean enforcement probabilities, what can we conclude about the enforcement trend?
sc:
- The control group experienced a stronger increase in enforcement probability compared to the treatment group.
- The treatment group experienced a stronger increase in enforcement probability compared to the control group.*
- Both groups saw the same increase in enforcement probability.
- Enforcement probability remained unchanged over time.

success: Great, your answer is correct! 
failure: Try again. 
#>

The extracted values show that enforcement probability increased for both groups after monitoring, but more strongly for the treatment group (from **0.00217** to **0.01032**) compared to the control group (from **0.00207** to **0.00774**). This suggests a positive treatment effect, indicating stronger enforcement near monitors. 

Now that we have extracted the mean enforcement probabilities for each group, we will compute the difference-in-differences (DiD) estimator.

**Task:**  Fill in the blanks (_) to correctly compute the difference-in-differences (DiD) estimator using the extracted mean enforcement probabilities.


```{r "3.2.6"}
#< fill_in
# Compute the difference-in-differences estimator
DiD_manual <- (y_exp_tr - _______) - (_______ - y_pre_co)
round(DiD_manual, 5)
#>

DiD_manual <- (y_exp_tr - y_exp_co) - (y_pre_tr - y_pre_co)
round(DiD_manual, 5)

```

#< quiz "did_interpretation"
question: The manually computed difference-in-differences (DiD) estimator is **0.00248**. How should we interpret this result?
sc:
- After the introduction of pollution monitoring, the average enforcement probability for firms within 10 km of a monitor increased by 0.00248 more compared to firms further away.*
- The overall enforcement probability increased by 0.00248 for all firms after monitoring was introduced.
- After the introduction of pollution monitoring, the average enforcement probability for both treated and control firms increased by 0.00248 more compared to a world without monitoring.
- The overall enforcement probability decreased by 0.00248 for all firms after monitoring was introduced.

success: Great, your answer is correct! 
failure: Try again. 
#>

The DiD estimator of **0.00248** indicates that, after the introduction of pollution monitoring, firms within 10 km of a monitor experienced a **0.00248** higher increase in enforcement probability compared to firms further away. This suggests that pollution monitoring led to stronger regulatory enforcement in closer proximity to monitors.
This result confirms that enforcement actions were not just increasing overall, but were targeted more intensely at firms near monitors. 


**Summary**

In this section, we explored how the difference-in-differences (DiD) approach helps to control for unobservable confounding factors, making it a powerful tool for causal inference. We manually computed the DiD estimator and interpreted its meaning, showing how pollution monitoring influenced enforcement actions for firms near and far from monitors.

While the manual calculation provides valuable intuition, the more common approach is to estimate the DiD effect using linear regression. This method allows us to compute standard errors, which help assess the statistical significance of the estimated effect. Additionally, regression-based estimation enables the inclusion of fixed effects and control variables, helping to account for potential biases.

In the next section, we will prepare the data and estimate the DiD effect using linear regression, allowing for a more precise and robust analysis.


## Exercise 3.3 -- Estimating the DiD Model Using Regression 


After introducing the difference-in-differences (DiD) approach and calculating the treatment effect manually, we now turn to its implementation using a regression framework. While the manual calculation provides an intuitive understanding of the method, regression-based estimation offers greater flexibility and precision, making it the standard in applied empirical research (Angrist & Pischke, 2008, pp. 18, 26).

Applying linear regression allows us to estimate the treatment effect while also generating key statistical measures such as standard errors, p-values, and confidence intervals. These are essential for assessing the robustness and significance of the results. 

Another key advantage of the regression approach is the ability to control for additional variables that may influence the outcome. While the manual DiD calculation only considers time and treatment group status, actual enforcement decisions may be affected by factors such as economic conditions, weather patterns, or political dynamics. By including fixed effects for firms, industries, or provinces, the regression model allows us to better isolate the causal effect of pollution monitoring.

In this section, we implement the difference-in-differences (DiD) estimation using a simple regression framework and compare the estimated treatment effect to the manually calculated DiD. More advanced elements such as **clustered standard errors**, **fixed effects**, and **control variables** will be introduced in subsequent sections.

Let us now reload the dataset and begin with the regression analysis.

**Task:** Read the file *filtered_firm_enf.rds* with help of `readRDS()` and save it in `dat`.

```{r "3.3.1"}

#< task
# read "filtered_firm_enf.rds" with help of readRDS()
#>
dat = readRDS("filtered_firm_enf.rds")

```

To ensure comparability with the manual DiD estimator from the previous chapter, we begin by replicating the same setup in our regression model. This includes creating an interaction variable that identifies firms located within 10 kilometers of a pollution monitor during the post-monitoring period. The variable captures the combined effect of treatment status and timing, and serves as the key component for estimating the causal impact of monitoring on enforcement actions.


**Task:** Press `check` to create the DiD interaction term by multiplying `min_dist_10` with `post1`.

```{r "3.3.2"}
#< task
# Create DiD interaction term: treatment group (≤10 km) × post-monitoring period
#>

dat <- dat %>%
  mutate(min_dist_10_post1 = min_dist_10 * post1)

```

To begin, we estimate a basic DiD model, identical to our manual calculation, without any additional fixed effects or control variables. This ensures a direct comparison between the two methods.

The regression equation we use is:

$$
y_{it}=β_0+β_1⋅post_t+β_2⋅mindist10_i+β_3⋅post_t×mindist10_i+ε_{it}
$$

where $y_{it}$ is a binary indicator equal to 1 if an enforcement action was taken against firm $i$ at time $t$. The variable $post_t$ equals 1 in the post-monitoring period, and $mindist10_i$ equals 1 if firm $i$ is located within 10 km of a pollution monitor. The interaction term $post_t \times mindist10_i$ captures the difference-in-differences (DiD) effect, and $\varepsilon_{it}$ is the error term.

#< quiz "did_regression"
question: What does the coefficient $β₃$ in the difference-in-differencess (DiD) regression represent?
sc:
- The overall difference in enforcement probability before and after monitoring was introduced.
- The baseline enforcement probability for firms farther than 10 km from a monitor.
- The general trend of enforcement actions over time, regardless of monitoring.
- The causal effect of pollution monitoring on the probability of enforcement actions for firms within 10 km of a monitor.*

success: Great, your answer is correct!
failure: Try again.
#>  

While the dependent variable is on the left side of the regression formula, the independent variables are placed on the right side. In our case, we aim to match the results from the manually computed difference-in-differences (DiD) estimator.

#< quiz "independent_variables"
question: Which variables are the independent ones in our difference-in-differences (DiD) regression?
sc:
- Two dummy variables, one indicating the treatment status and another one the period.
- Two dummy variables, one indicating the treatment status and another one the period, further one interaction term between those two dummies.*
- One dummy variable indicating the treatment status and the pollution indicator.
- Two dummy variables, one indicating the treatment status and another one the period, further one interaction term between those two dummies and the pollution indicator.

success: Correct! 
failure: Try again. 
#>  

Now that we have identified the independent variables in our difference-in-differences (DiD) regression, we can proceed to estimating the model. The next step is to run the regression using these variables and compare the results with our manually computed DiD estimator.


**Task:** Use the `lm()` function to estimate the difference-in-differences (DiD) regression model. The regression should include the time dummy `(post1)`, the treatment dummy `(min_dist_10)`, and their interaction term `(post1:min_dist_10)` to capture the DiD effect.

If you don’t know how to specify interactions in `lm()`, have a look at the information box below.

#< info "Running a linear regression with interaction terms using lm()."

The `lm()` function in base *R* is used to estimate linear regression models. It allows for flexible specification of relationships between variables, including interaction terms between two or more predictors.


```{r eval=FALSE}

# Basic structure of lm() with an interaction term
model <- lm(outcome_var ~ var1 + var2 + var1:var2, data = your_data)
```

- outcome_var: the dependent variable (what you want to explain)
- var1 and var2: independent variables (e.g., treatment, time)
- var1:var2: the interaction term between two variables
- data = your_data: the data frame containing the variables

Alternatively, you can use the shorthand `*` to include both main effects and the interaction in one step:

```{r eval=FALSE}
model <- lm(outcome_var ~ var1 * var2, data = your_data)
``` 

This automatically expands to:

```{r eval=FALSE}
outcome_var ~ var1 + var2 + var1:var2

``` 

Such models are commonly used when assessing effect heterogeneity or estimating treatment effects, particularly in designs like difference-in-differences.
For more details about the `lm()` function, you can check out this helpful overview: [Documentation lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm).

#>

```{r "3.3.3"}

#< fill_in
# Run the difference-in-differences regression using lm()
reg_lm <- lm(any_air ~ post1 + min_dist_10 + ________________, data = dat)
#>


reg_lm <- lm(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat)

```

After saving the regression results in `reg_lm`, we can examine them using the function `modelsummary()` from the `modelsummary` package. This function provides a structured output of our regression results, making it easier to interpret the estimated coefficients.

If you are interested in exploring this package further, check out the info box below.

#< info "Displaying regression results with modelsummary()."

The `modelsummary()` function from the `modelsummary` package creates clear and customizable tables of regression results. It supports labeling, formatting, significance levels, and various output styles—making it ideal for reports or publications.


```{r eval=FALSE}
modelsummary(list("Model Title" = model_object))
``` 

To keep the output clear and tailored to our needs, we focus on a few key options: 

- `coef_rename`: Rename variable labels
- `stars`: Add significance stars
- `gof_omit`: Hide model stats like AIC, BIC, etc.
- `fmt`: Set decimal places
- `output`: Define output format (e.g., `kableExtra`)

 For more information, see the full documentation here: [Documentation modelsummary()](https://modelsummary.com/).

#>

Recall that our manual calculation of the difference-in-differences (DiD) estimator yielded a value of **0.00248**. Now, compare it to the coefficient of the interaction term `(post1:min_dist_10)` in the regression output to verify whether both approaches yield consistent results.


**Task:** Just `check` the chunk to show the regression results nicely.

```{r "3.3.4"}

#< task_notest
# Load the modelsummary package and display the regression results in a well-formatted table.

library(modelsummary)

modelsummary(list("Air Pollution Enforcement" = reg_lm), 
             coef_rename = c("post1" = "Post-Monitoring", 
                             "min_dist_10" = "Within 10km of Monitor", 
                             "post1:min_dist_10" = "Post-Monitoring × Within 10km"), 
             stars = c('*' = .1, '**' = 0.05 ,'***' = .01), 
             gof_omit = "AIC|BIC|Std.Errors|Log.Lik|R2|FE|F|RMSE", 
             title = "Air Pollution Monitoring and Enforcement - Linear Regression",
             fmt = 5,   # Set 5 decimal places
             output = "kableExtra") %>%  
  kableExtra::kable_classic(full_width = TRUE) %>%
  kableExtra::kable_styling(font_size = 15)
#>


```

In the table, you can see the intercept and three additional coefficients. These include the post-monitoring coefficient, the within 10km of Monitor coefficient, and the interaction term between these two variables. 

#< quiz "did_value_match"
question: Does the estimated coefficient of the interaction term match the manually computed difference-in-differences (DiD) estimator?
sc:
- Yes.*
- No.

success: Correct! 
failure: Try again. 
#>  

The interaction term’s coefficient **(0.00248)**, which captures the difference-in-differences (DiD) effect, is estimated the same as the result obtained from the manual DiD calculation in the previous exercise. It is statistically significant at the 1% level (p < 0.01), indicating a meaningful impact of pollution monitoring on enforcement actions.
Now, we would like to determine which coefficient specifically measures the effect of pollution monitoring on enforcement probability for firms near pollution monitors.


#< quiz "did_intercept"
question: What does the intercept **(0.00207)** represent in the regression?
sc:
- The baseline enforcement probability for firms farther than 10 km from a monitor before the monitoring program started.*
- The overall probability of enforcement actions for all firms.
- The probability of enforcement actions after the monitoring program started.
- The increase in enforcement probability for firms within 10 km of a monitor.

success: Correct! 
failure: Try again. 
#>  

The regression results provide strong evidence that pollution monitoring increases environmental enforcement. The intercept **(0.00207)** reflects the low baseline probability of enforcement for firms more than 10 km from a monitor before monitoring began. The post-monitoring coefficient **(0.00567)** indicates a general increase in enforcement across all firms, suggesting stricter regulation over time. The coefficient for `Within 10 km of Monitor` **(0.00010)** indicates that, prior to the introduction of monitoring, there were no significant differences in enforcement between firms located near and far from a monitor. This supports the parallel trends assumption, which is further examined and discussed in detail using the full model specification in *Chapter 3.5*. Most importantly, the interaction term **(0.00248)** captures the causal effect, meaning firms near monitors were significantly more likely to face enforcement after monitoring began. These results support the hypothesis that greater transparency through monitoring enhances regulatory enforcement.

**Summary**

This section estimated a basic DiD regression to examine the impact of pollution monitoring on enforcement activity. The key interaction term yielded a statistically significant coefficient of **0.00248**, indicating that firms near monitors were more likely to face enforcement after monitoring was introduced. This matches the manually computed DiD estimate and provides initial causal evidence. However, the model has not yet accounted for potential correlations in the error terms or unobserved heterogeneity, which will be addressed in the following steps by using clustered standard errors and including fixed effects.

## Exercise 3.4 -- Cluster-Robust Standard Errors

In the previous section, we estimated a basic difference-in-differences model to examine the effect of pollution monitoring on enforcement. While the results showed a significant increase in enforcement near monitors, the model did not account for correlated errors within cities. Since enforcement patterns may be influenced by shared city-level factors, standard errors must be adjusted accordingly. We now introduce cluster-robust standard errors at the city level to obtain valid inference. This step ensures that statistical significance is not overstated due to intra-group correlation and reflects best practices in applied econometric analysis.

To better understand the theoretical foundation of cluster-robust standard errors, we now derive their variance estimator in more detail.

We consider the classical linear regression model

$$
y = Xβ + ε,
$$

where *y* is the vector of observed outcomes, *X* is the matrix of regressors, *β* is the parameter vector, and *ε* is the vector of error terms.

Following White (1980, pp. 820–822), the asymptotic variance of the Ordinary Least Squares (OLS) estimator $\hat{\beta}$, conditional on X, is given by:

$$
\text{Var}(\hat{\beta}) = (X^T X)^{-1} B (X^T X)^{-1},
$$

with

$$
B = X^T Var(ε \mid X) X.
$$

In classical Ordinary Least Squares theory, it is typically assumed that the error terms are **homoskedastic** and **independent**. This implies that $\text{Var}(\varepsilon \mid X) = \sigma^2 I$ (Greene, 2012, p. 62), where $\sigma^2$ is constant across observations and $I$ is the identity matrix. However, this assumption is often violated in the context of grouped or panel data. For example, firms within the same city may be subject to correlated shocks or unobserved regional factors. In such cases, clustering standard errors at the group level leads to more reliable inference (Cameron & Miller, 2015, pp. 318–321).

Assume the data is divided into C clusters. Then, the model for each cluster c = 1, ..., C is

$$
y_c = X_c β + ε_c,
$$

where *yc*, *Xc*, and *εc* refer to the outcome, regressors, and error term within cluster c.

The **cluster-robust variance estimator** is then given by (Cameron & Miller, 2015, p. 324):

$$
Var_{cluster}(\hat{\beta}) = (X^T X)^{-1} \left( \sum_{c=1}^{C} X_c^T \hat{u}_c \hat{u}_c^T X_c \right) (X^T X)^{-1},
$$

where *ûc* denotes the residuals for cluster c.

This estimator allows for unrestricted correlation of residuals within clusters while assuming independence across clusters. It yields consistent standard errors as the number of clusters C grows large. Clustered standard errors are particularly important when the treatment is assigned at the cluster level or when policy implementation is organized by groups such as cities, regions, or institutions (Cameron & Miller, 2015).

To connect the theoretical foundation of standard errors with empirical practice, we now turn to a hands-on analysis using again the *filtered_firm_enf.rds* dataset. In the following tasks, we revisit the regression model from *Exercise 3.2* and examine whether the key assumptions behind standard errors, such as homoskedasticity and independence, hold in this context.


**Task:** Press `check` to read the file *filtered_firm_enf.rds* with help of `readRDS()` and save it in `dat`.

```{r "3.4.1"}

#< task
#read "filtered_firm_enf.rds" with help of readRDS()

dat = readRDS("filtered_firm_enf.rds")
#>
```

The assumptions of independent error terms and homoskedasticity are fundamental in linear regression because they ensure that the estimated standard errors correctly reflect the variability of the coefficient estimates. When these conditions are met, standard errors provide valid and unbiased measures of statistical uncertainty.

If independence is violated, as is often the case in clustered or time-dependent data, residuals can become correlated. This typically leads to underestimated standard errors and increases the risk of drawing incorrect conclusions. Similarly, when the variance of the error terms differs across observations, known as heteroskedasticity, standard errors may not adequately capture the true variability in the data.
Ignoring these violations can result in invalid inference, such as mistakenly identifying insignificant effects as statistically significant (Angrist & Pischke, 2008, pp. 221ff.).

Remember, we are still considering the difference-in-differences (DiD) regression model:

$$
y_{it}=β_0+β_1⋅post_t+β_2⋅mindist10_i+β_3⋅post_t×mindist10_i+ε_{it}
$$

**Task:** Press `check` to estimate the simple regression again and store it in `reg_lm`:

```{r "3.4.2"}

#< task
reg_lm <- lm(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat)
#>

```


To assess the validity of the model assumptions, we analyze the residuals from the baseline regression. This helps identify potential issues such as heteroskedasticity or intra-cluster correlation. If the residual variation differs systematically across cities or time, standard errors may be underestimated. This provides a strong justification for using cluster-robust standard errors in the analysis.

**Task:** Replace the blanks to correctly compute residuals from the `reg_lm` model in the given code snippet.

```{r "3.4.3 "}

#< fill_in
# Extract residuals from the linear regression model (reg_lm)
dat$resid <- _________________
#>

dat$resid <- residuals(reg_lm)
```


To examine heteroskedasticity across clusters, we focus on the five cities with the highest number of observations. This selection ensures that the analysis is based on data-rich clusters, which improves the reliability and interpretability of the residual variation across cities.

**Task:** Replace the blanks to select the five cities `(city_id)` with the highest number of observations. 

```{r "3.4.4 "}

#< fill_in
# Select top 5 cities by number of observations
top_cities <- dat %>%
  count(city_id) %>%
  top_n(__, _) %>%
  pull(city_id)

#>
top_cities <- dat %>%
  count(city_id) %>%
  top_n(5, n) %>%
  pull(city_id)

```

Next, we calculate the standard deviation of residuals for each city and year. This step allows us to examine how the variability of residuals evolves over time within cities, providing valuable insight into the presence of heteroskedasticity.

**Task:** Replace the blank to calculate the standard deviation of the residuals. This step summarizes how much the unexplained variation (residuals) fluctuates within each city and year.

```{r "3.4.5"}

#< fill_in
# Calculate residual standard deviation by city and year
resid_by_city <- dat %>%
  filter(city_id %in% top_cities) %>%
  group_by(city_id, year) %>%
  summarise(sd_resid = sd(______, na.rm = TRUE)) %>%
  ungroup()
#>

resid_by_city <- dat %>%
  filter(city_id %in% top_cities) %>%
  group_by(city_id, year) %>%
  summarise(sd_resid = sd(resid, na.rm = TRUE)) %>%
  ungroup()
```

To explore potential differences in residual variability across cities, we present the distribution of residuals by city using a boxplot. This allows us to assess whether the spread of residuals varies systematically across locations.

**Task:** Just `check` the next chunk to create a boxplot showing the distribution of residual spread (standard deviation) across the five selected cities.

```{r "3.4.6"}

#< task_notest
# Visualize the distribution of residual variability across cities using a boxplot
ggplot(resid_by_city, aes(x = factor(city_id), y = sd_resid)) +
  geom_boxplot(fill = "skyblue", alpha = 0.7) +
  labs(
    title = "Distribution of Residual Spread by City",
    x = "City ID",
    y = "Standard Deviation of Residuals"
  ) +
  theme_minimal()
#>

```

#< quiz "Heteroskedasticity.boxplot2"
question: Why does this plot support the use of cluster-robust standard errors?
sc:
- Because the average residuals are always zero.
- Because clustering ensures better model fit.
- Because residual variation appears unequal across cities, violating homoskedasticity.*
- Because the regression coefficients change when clustering is applied.
success: Correct! 
failure: Try again. Focus on the variance between cities.
#>

The boxplot demonstrates clear variation in the spread of residuals across the five selected cities. While some cities exhibit a relatively narrow distribution of residual standard deviations, others display a considerably wider spread, often with visible outliers. This pattern suggests a violation of the classical homoskedasticity assumption, which requires constant variance of the error terms. The results indicate signs of heteroskedasticity, as the variability of the residuals differs across clusters.

Moreover, since environmental enforcement is shaped by local policy decisions, administrative structures, and potentially unobserved political factors, it is plausible that error terms are correlated within cities. Ignoring this intra-cluster correlation can result in underestimated standard errors in standard OLS models, thereby inflating the statistical significance of estimated effects and leading to incorrect inferences.

To address these issues, we now refine our estimation approach by applying cluster-robust standard errors at the city level. This correction accounts for correlation of residuals within cities, while still assuming independence between cities.

Since environmental regulation and its enforcement are usually carried out at the local level, firms within the same city are often subject to similar, unobserved influences. These may include changes in local environmental policy, variations in administrative capacity, or fluctuations in enforcement effort. Such shared factors can lead to a violation of the assumption that error terms are independent, which is a key requirement for conventional OLS estimation. If this intra-city correlation is ignored, standard errors may be underestimated, which in turn can exaggerate the precision of the results and lead to incorrect conclusions.

Clustering standard errors at the city level corrects for this problem by allowing for arbitrary correlation of residuals within clusters, while maintaining the assumption of independence across clusters. This adjustment improves the validity of statistical inference by yielding more accurate standard errors and more reliable hypothesis tests.

To implement this in practice, we use the `feols()` function from the `fixest` package in *R*, which allows for efficient estimation of regression models with cluster-robust standard errors.

#< info "Estimating regressions with cluster-robust standard errors using feols()."

The `feols()` function from the `fixest` package is a fast and flexible alternative to `lm()`, designed for linear regression models with fixed effects and robust inference options. It is particularly useful when working with panel or grouped data.

One key advantage of `feols()` is its ability to cluster standard errors, which allows the model to account for intra-group correlation. For example, firms located in the same city may be influenced by similar unobserved factors, leading to correlated error terms.

```{r eval=FALSE}
# Load the fixest package
library(fixest)
# Estimate DiD model with clustered standard errors
reg_feol <- feols(outcome ~ var1 + var2 + var1:var2, data = your_data, cluster = ~ group_id)
``` 

- `cluster` = ~ group_id: adjusts standard errors for clustering (e.g., by city, firm, or region)
- The formula can include interaction terms, as in `lm()`
- Fixed effects can also be added using the `|` syntax, which we will cover in the following chapter.

For more details on the feols function, see the [Rdocumentation](https://www.rdocumentation.org/packages/fixest/versions/0.8.4/topics/feols).

#>


**Task:** To begin, load the `fixest` package, which provides access to the `feols()` function for estimating regression models with clustered standard errors.

```{r "3.4.7"}

#< task
#>
library(fixest)

```

After loading the fixest package, we now want to apply city-level clustering to our difference-in-differences model. This step ensures that standard errors account for potential within-city correlation, making our inference more robust and reliable.

**Task:** Fill in the blanks to apply city-level clustering to the same model as above. This ensures that standard errors are robust to within-city correlations, improving the reliability of our inference.

```{r "3.4.8"}


#< fill_in

reg_feol <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat, cluster = ___________)
#>

reg_feol <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat, cluster = ~ city_id)
```

Now, we will display the results using the function `summary()`.


**Task:** Press `check` to display the regression results stored in `reg_feol` using the `summary()` function.

```{r "3.4.9"}
#< task
# Show the regression results
summary(reg_feol)
#>
```


#< quiz "clustered_se_feols"
question: What is the main difference between the regression output of `lm()` and `feols()` when clustering on `city_id`?
sc:
- feols() can display different types of R² values depending on model specification, while lm() only reports a single R².
- feols() reports cluster-robust standard errors that account for within-city correlations, whereas lm() uses conventional standard errors.*
- feols() does not support clustering of standard errors.
- feols() automatically reduces coefficient magnitudes when clustering is applied.
success: Correct! 
failure: Not quite. 
#>

The output structure received when using `feols()` for the regression differs slightly from the one we already discussed. Compared to the output of `lm()`, we now see clustered standard errors reported in the second column of the Coefficients section, reflecting the adjustment for within-city correlations.

Additionally, we now observe an Adjusted R² (Adj. R²) and Root Mean Squared Error (RMSE). Unlike `lm()`, `feols()` does not display a traditional F-statistic but still provides robust standard errors and model fit indicators. Since we did not include fixed effects in this regression, we only see a single (adjusted) R² value.

By clustering the standard errors on `city_id`, we ensure that the statistical inference remains valid even when residuals within cities are correlated. We will compare these results with the previous `lm()` regression to examine how accounting for clustering affects our estimates.

Before comparing the results of the standard OLS and the cluster-robust OLS using `modelsummary`, we first re-estimate the OLS model using the `feols` function. In the previous step, we used the base *R* function `lm`, which does not support clustering. To maintain a consistent model specification and allow for direct comparison, we now run the same regression again using `feols` without applying clustering.

**Task:** Press `check` to generate a comparison table that displays the results of the same regression model estimated once with default standard errors and once with standard errors clustered at the city level.

```{r "3.4.10",results='asis'}
#< task
# Perform OLS without clustering
reg_feol_ols <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat)

# Perform OLS with clustered standard errors
reg_feol <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, 
                  data = dat, 
                  cluster = ~ city_id)

# Display the models in a formatted HTML table
modelsummary(
  list("OLS" = reg_feol_ols, "Clustered OLS" = reg_feol),
  coef_rename = c(
    "post1" = "Post-Monitoring", 
    "min_dist_10" = "Within 10km of Monitor", 
    "post1:min_dist_10" = "Post-Monitoring × Within 10km"
  ), 
  stars = c('*' = .1, '**' = 0.05, '***' = .01), 
  gof_omit = "AIC|BIC|Std.Errors|Log.Lik|R2|FE|F|RMSE", 
  title = "Air Pollution Monitoring and Enforcement - Linear Regression",
  fmt = 5,   # Set 5 decimal places
  output = "kableExtra"
) %>%  
  kableExtra::kable_classic(full_width = TRUE) %>%
  kableExtra::kable_styling(font_size = 15)
#>
```

#< quiz "Impact of Clustering on Standard Errors"
question: What is the main effect of clustering standard errors at the city level?
choices:
- It changes the estimated coefficients.
- It reduces the number of observations.
- It increases the standard errors, making inference more robust.*
- It improves the model fit by increasing R².
explanation: Correct! Clustering does not affect coefficient estimates or R² but increases standard errors to account for within-group correlation, leading to more reliable inference.
success: Correct! 
failure: Incorrect. Try again! 
#>


The estimated coefficients remain identical across both models, as clustering affects only the standard errors, not the coefficient values.

However, the standard errors in the clustered model are larger, reflecting adjustments for within-city correlations. For example, the standard error for `post1:min_dist_10` increases from **0.00026** (under classical OLS assumptions) to **0.00068** (Clustered), indicating that OLS likely underestimated the true variability. Despite these changes, R² and Adjusted R² remain the same, as clustering does not impact model fit but rather the precision of the estimates.

This comparison highlights the importance of clustering when observations within cities are correlated. Clustering provides more robust inference by correcting for underestimated standard errors in the OLS model, ensuring that statistical significance is not overstated.

**Summary**

In this chapter, we explored the limitations of conventional OLS standard errors in the context of clustered data. Our analysis revealed heteroskedasticity across cities and suggested that firms within the same city may be subject to shared unobserved factors, potentially inducing correlated errors. To address these concerns, we applied cluster-robust standard errors at the city level. This adjustment accounts for within-cluster correlation while preserving cross-cluster independence. Comparing OLS with the clustered model showed that clustering increases standard errors without affecting coefficient estimates, thereby enhancing the reliability of statistical inference.

In the next chapter, we begin by testing the parallel trends assumption and then fully extend our model by incorporating fixed effects. This sets the foundation for a credible difference-in-differences estimation of the treatment effect.


## Exercise 3.5 -- Fixed Effects

While the baseline difference-in-differences (DiD) model provides a solid framework for estimating the causal effect of policy interventions, it does not account for unobserved heterogeneity that may bias the results. In our initial specification, we compared firms located near pollution monitors with those farther away, before and after the introduction of monitoring. However, firms may differ in characteristics that are constant over time but still systematically influence enforcement outcomes. Moreover, broader economic or regulatory trends may simultaneously affect all firms, regardless of their proximity to monitors.

Before turning to fixed Effects, it is essential to first consider the core identifying assumption of the DiD framework. This assumption states that, in the absence of the treatment, the outcomes of the treated and control groups would have followed parallel trends over time.

**Parallel Trends in the DiD Approach**

The DiD approach relies on the parallel trends assumption, which states that, in the absence of treatment, the difference between treated and control groups would have remained constant over time. This ensures that any post-treatment divergence can be attributed to the intervention itself rather than to pre-existing trends (Mailman School of Public Health, Columbia University, 2024).

To validate this assumption, we analyze pre-treatment trends from 2010 to 2014 by comparing firms located within 10 kilometers of a monitor (treatment group) to those farther away (control group). If both groups show similar patterns in enforcement before 2015, this lends support to the parallel trends assumption. In that case, any divergence in enforcement outcomes after 2015 can be attributed to the introduction of pollution monitoring rather than to pre-existing differences in local regulatory behavior.
In our case, the parallel trends assumption implies that the enforcement gap between firms near and far from a monitor would have remained stable if China had not introduced the monitoring policy in 2015. 

#< quiz "parallel_trends_assumption"
question: What does a common trend between treatment and control groups before 2015 indicate?
sc:
- That the difference-in-differences (DiD) method is likely to provide causally valid estimates.*
- That the policy had an impact before 2015.
- That pollution levels in treatment and control cities are always identical.
- That both groups experienced a sudden drop in pollution levels after 2015.
success: Correct! 
failure: Not quite! 
#>

By visually inspecting the pre-2015 trends, we assess whether the assumption holds. If both groups display a common trend before the intervention, the DiD approach is more likely to yield causally valid estimates of the policy’s impact on air quality. Conversely, if the trends diverge significantly before 2015, this could indicate pre-existing differences between treatment and control cities, potentially biasing our results.

Let’s now check this assumption by visualizing the pre-policy enforcement trends for both groups.

I have already completed the data preparation, so we can now directly load the processed dataset. As in the previous section, we constructed an interaction term (`min_dist_10_post1`) to capture the treatment effect. This variable combines the treatment group indicator (`min_dist_10`) with the post-treatment period (`post1`), allowing us to estimate the differential effect after the intervention.

To visualize trends in enforcement, we calculated the average enforcement probability (`any_air`) for each year and treatment group. This was done by grouping the data by `year` and `min_dist_10`, computing the mean of the outcome variable while excluding missing values. The resulting summary dataset is now ready for visualization and further analysis.

**Task:** Press `check` to load the dataset *dat.mean.rds* using `readRDS()` and assign it to the variable named `dat.mean`. 

```{r "3.5.1"}

#< task
dat.mean <- readRDS("dat.mean.rds")
#>


```

To assess whether the parallel trends assumption holds, we need to compare the average enforcement probability before and after the policy intervention for both the treatment group (firms within 10 km of a monitor) and the control group (firms farther away).

**Task:** Replace the blanks ___ to define the pre- (year < 2015) and post-policy (year > 2014) periods for the treatment and control groups, enabling the calculation of average enforcement probabilities before and after the intervention for the difference-in-differences analysis.

```{r "3.5.2"}

#< fill_in
# Calculate average enforcement probability before and after 2015 for treated and control groups
y_pre_tr = filter(dat.mean, year < ____, min_dist_10 == 1)$mean_outcome %>% mean() %>% round(4)
y_exp_tr = filter(dat.mean, year > ____, min_dist_10 == 1)$mean_outcome %>% mean() %>% round(4)
y_pre_co = filter(dat.mean, year < ____, min_dist_10 == 0)$mean_outcome %>% mean() %>% round(4)
y_exp_co = filter(dat.mean, year > ____, min_dist_10 == 0)$mean_outcome %>% mean() %>% round(4)

#>
y_pre_tr = filter(dat.mean, year < 2015, min_dist_10 == 1)$mean_outcome %>% mean() %>% round(4)
y_exp_tr = filter(dat.mean, year > 2014, min_dist_10 == 1)$mean_outcome %>% mean() %>% round(4)
y_pre_co = filter(dat.mean, year < 2015, min_dist_10 == 0)$mean_outcome %>% mean() %>% round(4)
y_exp_co = filter(dat.mean, year > 2014, min_dist_10 == 0)$mean_outcome %>% mean() %>% round(4)


```

Now, we want to visualize the enforcement probability trends for both the treatment and control groups over time. This plot will help us examine whether the parallel trends assumption holds by comparing the pre-policy trends (before 2015) and assessing any divergence after the policy intervention. 

**Task:** Just run the chunk to visualize the enforcement trends and assess the validity of the parallel trends assumption. 

```{r "3.5.3", fig.width=11}

#< task_notest
# Optimized Y-Offsets for clearer label positioning
y_offsets <- c(0.001, 0.0007, 0.0012, 0.0005)  # Increased values for better placement

# Plot with improved annotations
ggplot(dat.mean, aes(x = year, y = mean_outcome, color = as.factor(min_dist_10), group = min_dist_10)) +
  geom_line(linewidth = 1.2) + 
  geom_point(size = 3) +
  geom_vline(xintercept = 2015, linetype = "dashed", color = "black", linewidth = 1) +  # Mark policy change
  xlab("Year") +
  ylab("Enforcement Probability") + 
  scale_color_discrete(labels = c("Far from Monitor", "Within 10km of Monitor"), name = "Firm Location") +
  ggtitle("Enforcement Probability Trends (2010-2017)") + 
  theme_minimal(base_size = 14) +
  # Ensure 2017 is explicitly included in the X-axis
  scale_x_continuous(breaks = seq(2010, 2017, by = 1), limits = c(2010, 2017)) +
  scale_y_continuous(limits = c(0.00, max(dat.mean$mean_outcome) + 0.002), 
                     breaks = seq(0.00, max(dat.mean$mean_outcome) + 0.002, by = 0.002), 
                     labels = scales::number_format(accuracy = 0.0001)) +  
  # Improved label annotations with proper spacing
  annotate("label", x = 2014, y = y_pre_tr + 0.001, label = y_pre_tr, fill = "white", size = 4, fontface = "bold", hjust = -0.5, vjust = 0) +
  annotate("label", x = 2017, y = y_exp_tr + 0.001, label = y_exp_tr, fill = "white", size = 4, fontface = "bold", hjust = 1.2) +
  annotate("label", x = 2014, y = y_pre_co + 0.001, label = y_pre_co, fill = "white", size = 4, fontface = "bold", hjust = 0.5, vjust = 0) +
  annotate("label", x = 2017, y = y_exp_co + 0.001, label = y_exp_co, fill = "white", size = 4, fontface = "bold", hjust = 1.3)
#>

```

The graph illustrates the enforcement probability trends from 2010 to 2017 for firms located within 10 km of a pollution monitor (treatment group) and firms farther than 10 km (control group). The dashed vertical line at 2015 marks the introduction of air pollution monitors, allowing us to observe the impact of this policy intervention.

#< quiz "Parallel Trends and Policy Impact"
question: Before the policy intervention in 2015, what does the parallel trends assumption imply about the enforcement probability trends for the treatment and control groups?
sc:
- The treatment group had a significantly higher enforcement probability than the control group before 2015.
- The control group had a significantly higher enforcement probability than the treatment group before 2015.
- Both groups followed a similar enforcement probability trend before 2015.*
- The enforcement probability fluctuated randomly between the two groups before 2015.
success: Great, your answer is correct! 
failure: Not quite. 
#>

The graph shows that enforcement probabilities for treatment and control groups followed parallel trends prior to 2015, supporting the key assumption of the DiD framework. After the introduction of pollution monitors, enforcement increased more sharply for firms within 10 km of a monitor. By 2017, the treatment group shows a notably higher enforcement probability, indicating that proximity to monitoring led to intensified regulatory action. This visual pattern aligns with a causal interpretation of the policy effect.

The current DiD model does not account for unobserved firm or time-specific factors that may bias the results. To address this, we next incorporate fixed effects for firms, time, and group-specific trends by industry and province over time. This extension improves identification by controlling for constant firm characteristics and common shocks, allowing for a more precise estimate of the policy’s causal effect.

## Fixed Effects

Fixed effects (FE) models are widely employed in econometric analyses to control for unobserved heterogeneity that may bias estimates of causal relationships. They are particularly useful in panel data settings, where multiple observations of the same entities (e.g., individuals, firms, or regions) are available over time. By accounting for time-invariant characteristics that could otherwise confound the relationship between explanatory and dependent variables, fixed effects models enhance the precision and credibility of empirical findings.

#< quiz "fixed_effects_quiz"
question: What do you think is the primary purpose of fixed effects in econometric models?
sc:
- To eliminate bias caused by unobserved, time-invariant factors.*
- To estimate the effects of time-invariant characteristics.
- To reduce the correlation between independent variables.
- To make the model more complex without any real benefit.
success: Correct! 
failure: Incorrect. Try again! 
#>

The primary purpose of fixed effects is to eliminate bias caused by unobserved, time-invariant factors. Many observational studies suffer from **omitted variable bias**, where certain characteristics that influence the dependent variable are not included in the regression but remain correlated with the explanatory variables. Such omissions can lead to misleading conclusions about causal relationships. Fixed effects estimation overcomes this issue by eliminating unobserved, time-invariant heterogeneity and allowing individual-specific components to be arbitrarily correlated with the regressors, thereby ensuring consistent estimation (Wooldridge, 2010, p. 266).

To improve identification, we now incorporate fixed effects into the regression model. These account for unobserved factors that remain constant within firms, industries, or provinces over time and allow us to focus on variation that reflects the impact of pollution monitoring.

We include year fixed effects to capture national trends that affect all firms equally. Firm fixed effects control for characteristics specific to each firm that do not change over time. In addition, we add industry-by-year and province-by-year fixed effects to account for sectoral and regional developments that may influence enforcement.

This structure helps isolate the treatment effect by controlling for both observed and unobserved factors that could otherwise bias the results.

After incorporating fixed effects for `year`, `firm`, and `industry-by-time`, our regression formula is structured as follows:


$$
y_{ijpt} = \beta_1 \cdot mindist10km_{it} + \delta_i + \theta_{jt} + \eta_{pt} + \varepsilon_{ijpt}
$$

Here, $y_{ijpt}$ denotes the probability of enforcement for firm $i$ in industry $j$, province $p$, and quarter $t$. The variable $mindist10km_{it}$ is a treatment indicator equal to 1 if the firm is located within 10 km of a pollution monitor in period $t$. $\delta_i$, $\theta_{jt}$, and $\eta_{pt}$ represent firm, industry-time, and province-time fixed effects, respectively, while $\varepsilon_{ijpt}$ is the error term. The coefficient $\beta_1$ captures the causal effect of proximity to monitoring on enforcement.

#< quiz "interpretation_beta1_advanced" 
question: The estimated coefficient $β_1$ represents the effect of a firm being within 10 km of a pollution monitor on enforcement probability. How should this be interpreted?
sc:
- The enforcement probability increases over time for all firms, regardless of distance to a monitor.
- Firms within 10 km of a monitor experience a higher enforcement probability compared to those farther away, holding fixed effects constant.*
- β1 captures the overall enforcement probability level in the post-monitoring period.
- β1 reflects time-invariant firm characteristics that influence enforcement probability.
success: Correct! 
failure: Not quite. 
#> 

As in the previous chapter, we will use the `feols()` function from the `fixest` package to estimate our regression models. This function is specifically designed for panel data and allows us to flexibly incorporate fixed effects while also supporting clustered standard errors. From this point onward, all regression analyses will be conducted using `feols`, as it offers both efficiency and robustness for the structure of our data.

**Task:** Load the dataset *filtered_firm_enf.rds* and store it as `dat` for use in the upcoming regression analysis.

```{r "3.5.4"}

#< task

#>

dat = readRDS("filtered_firm_enf.rds")

```


Next, we will again transform the dataset by creating a new variable, `min_dist_10_post1`, which will be the product of `min_dist_10` and `post1`. We will also generate a grouping variable, `ind_time`, based on the combination of `industry`, `prov_id`, and `time`, allowing us to uniquely identify each group.



**Task:** Fill in the blanks to correctly group the dataset. Complete the `group_by()` function with the appropriate variables before creating `ind_time`. The correct grouping variables are `industry`, `prov_id`, and `time`.

```{r "3.5.5"}
#< fill_in
# Create a unique group ID by industry, province, and time for fixed effects in DiD analysis
dat <- dat %>%
  mutate(
    min_dist_10_post1 = if_else(is.na(min_dist_10) | is.na(post1), NA_real_, min_dist_10 * post1)
  ) %>%
  group_by(_______, _______, _____) %>%
  mutate(ind_time = cur_group_id()) %>%
  ungroup()

#>
dat <- dat %>%
  mutate(
    min_dist_10_post1 = if_else(is.na(min_dist_10) | is.na(post1), NA_real_, min_dist_10 * post1)
  ) %>%
  group_by(industry, prov_id, time) %>%
  mutate(ind_time = cur_group_id()) %>%
  ungroup()

```

By completing this task, we have successfully generated a unique group identifier, `ind_time`, which enables more robust analysis at the group level and facilitates the inclusion of fixed effects in subsequent regression models.

**Task:** Press `check` to display a random sample of rows from the dataset.

```{r "3.5.6"}
#< task
# Display a random sample of rows to verify the new variable
dat %>%
  select(id,industry, prov_id, time, ind_time) %>%
  sample_n(10)  # Randomly select 10 rows
#>
```


The `id` column uniquely identifies each observation, while the variables `industry`, `prov_id`, and `time` define groups based on `sector`, `province`, and `time` period. The newly created variable `ind_time` serves as a unique numeric identifier for each of these groups. Observations that belong to the same group share the same `ind_time` value, making it suitable for group-level analyses such as fixed-effects modeling. The displayed table confirms that `ind_time` has been generated correctly.

Now, we will estimate the regression model using `feols()`, including fixed effects for `time`, `id`, `industry` interacted with `time`, and `prov_id` interacted with `time`, while clustering standard errors at the `city level`.

#< info "Using fixed effects in feols()."

The `feols` function from the `fixest` package allows you to include fixed effects directly in the model formula using the `|` symbol. Fixed effects help control for unobserved heterogeneity across units, time periods, or groups.

```{r eval=FALSE}
feols(outcome ~ independent_vars | fixed_effects, data = your_data)
``` 

- The part before the `|` contains the usual regression formula (e.g., treatment, controls, interactions).
- The part after the `|` specifies one or more fixed effects.
- Fixed effects can be added as single variables or as interactions (e.g., group^time).

#>


**Task:** Fill in the blanks to complete the `feols` regression by adding the correct fixed effects.

```{r "3.5.7"}
#< fill_in
model_any_air <- feols(any_air ~ min_dist_10_post1 | ______ + _____ + industry^time + prov_id^time, 
                       cluster = ~city_id, data = dat)


#>
model_any_air <- feols(any_air ~ min_dist_10_post1 | time + id + industry^time + prov_id^time, 
                       cluster = ~city_id, data = dat)


```


With this validation in place, we can now proceed with the next step. We will estimate the difference-in-differences (DiD) coefficient as reported by the authors. This will allow us to quantify the policy’s impact on enforcement probability and directly compare our results to their findings.

Before displaying the regression results, we calculate the mean of the dependent variable `any_air`.  This step is useful because it provides a baseline against which we can interpret the estimated treatment effect. By comparing the size of the coefficient to the average enforcement probability, we gain a clearer understanding of the policy’s relative impact.

**Task:**  Fill in the blanks to calculate the mean of the dependent variable `any_air` using the `$-operator`. 

```{r "3.5.8"}
#< fill_in
mean_any_air <- round(mean(____________, na.rm = TRUE), 4)


#>
mean_any_air <- round(mean(dat$any_air, na.rm = TRUE), 4)

```

Now that we have calculated the mean of the outcome variable `(any_air)`, we can present the regression results. For this purpose, I created a comparison table that presents the results from the standard OLS model and the OLS model with clustered standard errors at the city level, both of which were analyzed in previous chapters, alongside the newly estimated fixed effects model. This structured comparison enables us to examine how different modeling choices influence the estimated treatment effect and to evaluate the robustness of our empirical findings.

**Task:** Simply press `check` to display the regression results.


```{r "3.5.9"}
#< task
# Run models
reg_feol_ols <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat)
reg_feol <- feols(any_air ~ post1 + min_dist_10 + post1:min_dist_10, data = dat, cluster = ~ city_id)
reg_feol_FE <- feols(any_air ~ min_dist_10_post1 | time + id + industry^time + prov_id^time, 
                     cluster = ~city_id, data = dat)

# Mean of dependent variable
mean_fe <- round(mean(dat$any_air, na.rm = TRUE), 5)

# Regression table
modelsummary(
  list(
    "OLS" = reg_feol_ols,
    "Clustered OLS" = reg_feol,
    "Fixed Effects OLS" = reg_feol_FE
  ),
  coef_rename = c(
    "post1" = "Post-Monitoring",
    "min_dist_10" = "Within 10km of Monitor",
    "post1:min_dist_10" = "Post-Monitoring × Within 10km",
    "min_dist_10_post1" = "Post-Monitoring × Within 10km"
  ),
  stars = c('*' = .1, '**' = 0.05, '***' = .01),
  gof_omit = ".*",
  fmt = "%.5f",
  output = "kableExtra"
) %>%
  kable_classic(full_width = TRUE) %>%
  kable_styling(font_size = 15) %>%
  add_footnote(label = paste("Mean of any_air:", mean_fe), notation = "none")

#>

```


At this stage, our results are fully transparent and closely aligned with the findings reported by the authors. In the earlier specification without fixed effects, we estimated a treatment effect of **0.00248**. After incorporating fixed effects, the estimate increases to **0.0033**. Moreover, the average enforcement probability of **0.0046** matches the value reported in the original study, confirming both the consistency and robustness of our estimation approach. 
**Note**: For reference, the mean of the dependent variable `(any_air)` has been added as a footnote to the regression table.

#< quiz "monitor_enforcement_increase"
question: By what percentage does the presence of a monitor increase the probability of an air-pollution-related enforcement action?
sc:
- 72%*
- 46%
- 33%
- 10%

success: Great, your answer is correct!
failure: Try again.
#>

The **72%** increase in enforcement probability is derived by comparing the additional enforcement likelihood for firms within 10 km of a monitor to the average quarterly enforcement probability. Specifically, the estimated effect shows that firms near a monitor experience a **0.33** percentage point higher probability of enforcement. Given that the average enforcement probability in a quarter is **0.46%**, we calculate the relative increase as (**0.0033** / **0.0046**) × 100 = **72%**. This means that the presence of a monitor significantly raises the likelihood of enforcement actions, making firms within 10 km much more likely to be subject to air-pollution-related regulatory measures compared to firms farther away.

The enforcement trends between treated and control firms were already largely parallel in the pre-treatment period, even without accounting for fixed effects. For this reason, we do not examine parallel trends at the firm level in greater detail. Instead, we will analyze them more systematically in *Chapter 4.2*, focusing on the city level and including both fixed effects and relevant control variables. For now, we turn to the interpretation of the remaining results, focusing on different types of enforcement actions such as `shutdowns`, `upgrading`, `fines`, and `warnings`.

**Task:** Press `check` to estimate the effect of being within 10km after the monitoring policy (Distance < 10km × Post) on different types of enforcement actions. The output will display regression results for the four most common enforcement categories: `suspension`, `upgrading`, `fines`, and `warnings`.


```{r "3.5.10"}
#< task

# Define outcome variables
outcomes <- c("any_air_shutdown", "any_air_renovate", "any_air_fine", "any_air_warning")

# Run regression models for each outcome
models <- setNames(lapply(outcomes, function(var) {
  feols(as.formula(paste(var, "~ min_dist_10_post1 | time + id + industry^time + prov_id^time")), 
        cluster = ~city_id, data = dat)
}), outcomes)

# Generate the formatted table using modelsummary and kableExtra
modelsummary(
  models,
  coef_rename = c(
    "min_dist_10_post1" = "Post-Monitoring × Within 10km"
  ), 
  stars = c('*' = .1, '**' = 0.05, '***' = .01), 
  gof_omit = "AIC|BIC|Std.Errors|Log.Lik|R2|FE|F|RMSE", 
  title = "Air Pollution Monitoring and Enforcement - Fixed Effects Regression",
  fmt = "%.4f",   # Ensures NO scientific notation (4 decimal places)
  output = "kableExtra"
) %>%  
  kableExtra::kable_classic(full_width = TRUE) %>%
  kableExtra::kable_styling(font_size = 15)
#>

```


The results suggest that proximity to pollution monitors leads to a significant increase in the likelihood of stricter enforcement actions. Firms located within 10 km of a monitor are **0.14** percentage points more likely to face shutdown orders, equipment renovations, and fines. 


#< quiz "Understanding_Warning_Results"
question: What do the results for the `Warning` enforcement action indicate?
sc:
- Proximity to a monitor significantly increases the likelihood of receiving a warning.
- Warnings are the most common enforcement action influenced by monitors.
- Warnings are completely eliminated for firms near monitors.
- Proximity to a monitor has no significant effect on the likelihood of receiving a warning.*
success: Correct! 
failure: Not quite. 
#>

The results show that firms located near pollution monitors are more likely to face strict enforcement actions, such as shutdowns, equipment upgrades, and fines, while the likelihood of receiving a warning remains unchanged. This suggests that environmental regulators are prioritizing stronger penalties over simple warnings when responding to pollution data. Since warnings are not affected, it indicates that monitoring pushes authorities to take more decisive actions, ensuring that firms comply with environmental regulations through meaningful consequences rather than just cautionary notices.

#< award "Causal Thinker" >
Sharp eye! You used difference in differences and fixed effects to crack the case on how monitoring affects firms. The data could not hide the truth from you.
#>

**Summary**

In this section, we extended the baseline DiD model by incorporating fixed effects to control for unobserved firm- and region-specific characteristics. Before doing so, we confirmed that the parallel trends assumption holds, as treated and control groups followed similar enforcement patterns prior to the policy. The inclusion of fixed effects improved the credibility of our estimates and showed that firms near monitors are more likely to face stricter enforcement actions such as shutdowns, renovations, and fines, while the likelihood of warnings remains unchanged.

In this next step, we move our analysis to the city level. This perspective helps us understand the broader impact of pollution monitoring on enforcement intensity and environmental outcomes.

## Exercise 4 -- City-Level Analysis of Monitoring Impacts

This chapter shifts the focus from localized firm-level effects to a broader city-level analysis in order to assess the aggregate impact of China’s air pollution monitoring program. While previous chapters explored enforcement responses near individual monitors, the city perspective allows us to examine whether monitoring triggers broader changes in environmental governance and pollution outcomes across entire urban areas. This is crucial for capturing potential within-city spillover effects and understanding whether improvements in compliance extend beyond directly observed locations.

We take advantage of exogenous variation in monitoring intensity across cities, as the number of monitors was assigned centrally based on standardized criteria such as population size and urban area. This variation enables us to identify how differences in monitoring coverage affect overall enforcement activity and air quality, even in parts of the city that are not directly monitored.

We proceed by applying an alternative difference-in-differences approach at the city level, expanding the scope beyond the firm-level analysis. This refined framework enables us to estimate average treatment effects across cities and assess whether areas with more intensive monitoring exhibit stronger regulatory enforcement and greater improvements in air quality. To enhance identification, we incorporate city and time fixed effects as well as a set of control variables that account for unobserved heterogeneity. Finally, we implement an instrumental variables strategy using the centrally assigned number of monitors to address potential endogeneity and strengthen the causal interpretation of our findings.

Together, these steps aim to reveal whether broader environmental monitoring translates into systematic improvements in policy enforcement and pollution reduction at the city level.



### Structure

4.1 City-Level Evidence

4.2 Incorporating Fixed Effects and Control Variables

4.3 Instrumental Variables Approach



## Exercise 4.1 -- City-Level Evidence

In this section, we analyze the impact of pollution monitoring at the city level to understand its broader effects on environmental enforcement and air quality. While previous analyses focused on firm-level enforcement changes in proximity to monitors, this section takes a step back to examine the aggregate consequences of monitoring across entire cities.

By leveraging variation in the number of monitors installed in different cities, we aim to capture the extent to which broader coverage affects overall enforcement intensity, pollution levels, and potential spillover effects. Specifically, we investigate whether cities with more monitors experience stronger enforcement efforts and greater pollution reductions, beyond just localized changes near individual monitors.

We start this chapter by creating an HTML table that presents the Monitor Assignment Criteria, based on the technical regulations set by the central government for the selection of ambient air quality monitoring stations. 

**Task:** Just press `check` to create and display the table in a well-formatted HTML output.

```{r "4.1.1"}
#< task

# Create the data for the table
data <- data.frame(
  Group = c(1, 2, 3, 4),
  `Population (10,000)` = c("< 25", "25 – 50", "50 – 100", "100 – 200"),
  `Built-Up Area (sq. km)` = c("< 20", "20 – 50", "50 – 100", "100 – 200"),
  `Minimum Monitors` = c(1, 2, 4, 6),
  `Cities` = c(26, 86, 57, 8)
)

# Create and display the table in HTML format with centered columns
data %>%
  kbl(col.names = c("Group", "Population (10,000)", "Built-Up Area (sq. km)", "Minimum Monitors", "Cities")) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(1:5, extra_css = "text-align: center;")  # Center-align all columns
#>
```

This table is based on the official technical regulations for the selection of ambient air quality monitoring stations issued by the *Ministry of Environmental Protection* (2013) and is reconstructed from Axbard and Deng (2024, Online Appendix, Table C2). It is a self-produced representation adapted from the original source and serves as a key reference for the city-level analysis presented in this chapter.

#< quiz "Monitor Assignment Criteria"
question: Based on this Table, what determines the minimum number of monitors assigned to a city?
sc:
- The pollution levels in the city determine the number of monitors assigned.
- The economic activity and industrial output of a city are the primary factors for monitor assignment.
- Monitors are assigned randomly across cities.
- The population and the size of the built-up area of a city determine the minimum number of monitors assigned.*
success: Great, your answer is correct!
failure: Not quite. 
#>

This table illustrates the criteria used to assign monitors to cities based on population size and the extent of built-up areas. Cities with fewer than 250,000 people and a built-up area smaller than 20 km² receive one monitor, while those with populations between 250,000 and 500,000 and built-up areas of 20–50 km² are assigned two monitors. Cities with 500,000 to 1 million residents and built-up areas of 50–100 km² receive four monitors, whereas the largest cities, with populations between 1 and 2 million and built-up areas of 100–200 km², are allocated six monitors.

To provide a clearer overview of how cities are distributed across the monitor assignment groups defined in the table above, I created a bubble chart to visualize the data. In this chart, the number of cities per group is shown on the y-axis, while bubble size indicates the minimum number of required monitors and bubble color reflects the built-up area. This visual summary supports a more intuitive understanding of how the criteria translate into group-level monitoring requirements.

**Task:** Press `check` to generate a bubble chart of city groups by monitor count and built-up area.

```{r "4.1.2"}
#< task_notest
# Create dataset for plotting

data <- data.frame(
  Group = factor(1:4),
  Cities = c(26, 86, 57, 8),
  Monitors = c(1, 2, 4, 6),
  BuiltUpArea = c(18, 35, 75, 150)
)

ggplot(data, aes(x = Group, y = Cities, size = Monitors, fill = BuiltUpArea)) +
  geom_point(shape = 21, color = "black") +
  scale_fill_gradient(low = "lightyellow", high = "darkgreen") +
  scale_size_continuous(range = c(4, 10)) +
  labs(
       x = "Group", y = "Number of Cities",
       fill = "Built-Up Area (km²)",
       size = "Min. Monitors") +
  theme_minimal()

#>
```

The chart shows that most cities fall into Group 2, with moderate population size and built-up area, and are assigned two monitors. Group 3 also contains a significant number of cities with higher monitoring requirements. In contrast, Groups 1 and 4 represent the smallest and largest cities, respectively, and contain fewer cases, highlighting the skewed distribution of urban centers across assignment groups.


Building on the monitor assignment criteria, we now turn our attention to the relationship between the number of monitors assigned to a city and the extent of high-pollution activity covered by these monitors.

We now want to provide a detailed illustration of this relationship, showing how an increase in the number of monitors is associated with greater coverage of high-pollution activities. This is measured as the share of a city's high-polluting firms' revenue located within 10 kilometers of a monitor, adjusted for firm size using data from the *Environmental Survey and Reporting* (ESR) database.

**Task:** Press `check` to load the datasets *city_info.rds* and *share.rds* into the variables `city_info` and `share_data`, and display the first three rows of each dataset using `head()`.

```{r 4.1.3"}
#< task
#read city_info.rds and share.rds with read_rds() and save it

city_info <- read_rds("city_info.rds")
head(city_info, 3)

share_data <- read_rds("share.rds")
head(share_data, 3)
#>

```

The *city_info.rds* dataset provides demographic, geographic, and economic information at the city level. Key variables include the city identifier `(city_id)`, the number of installed monitors `(number)`, population size `(pop)`, built-up area `(area)`, gross domestic product `(GDP)`, and distance to the coast `(disttocoast)`. It also contains geographic coordinates `(env_lon, env_lat, centroid_lon, centroid_lat)` as well as predicted monitor assignments `(pred_number1, pred_number2)`, offering insights into how city characteristics may influence monitoring intensity.

The *share.rds* dataset complements this information by focusing on the spatial coverage of high-pollution activities. It includes the variable `city_id` along with measures such as `share_rev_10` and `share_emp_10`, which capture the share of revenue or employment from high-polluting firms located within 10 km of a pollution monitor. These values range from 0 to 1 and serve as proxies for how effectively monitors are positioned to cover major sources of emissions.

Both datasets are linked via the common identifier `city_id`, enabling the integration of structural city characteristics from *city_info* with spatial indicators of pollution exposure derived from the *share* dataset. This integrated data structure allows us to analyze how the intensity and spatial placement of air quality monitoring relate to the coverage of pollution-intensive activities.

That is precisely what we will do next.

The following code explores the relationship between the number of pollution monitors in a city and the spatial coverage of high-polluting activities. We begin by using `inner_join()` to merge the *city_info* and *share_data* datasets by the common identifier `city_id`, and then group the merged data by `number`, which indicates the number of monitors per city. For each group, we compute the average share of high-polluting firms located within 10 km of a monitor `(mean_y)`, along with a 95% confidence interval `(lower_ci, upper_ci)` based on the group’s standard deviation and size. This analysis helps assess whether more intensive monitoring is associated with better coverage of pollution sources.

#< info "Merging datasets using inner_join()."

The `inner_join()` function from the `dplyr` package is used to merge two datasets based on a common key. It returns only the rows with matching values in both datasets.

```{r eval=FALSE}
inner_join(x, y, by = "key_variable")
``` 

- x, y: the datasets to be joined
- by: the name of the common variable used for matching
 
An inner join is useful when you want to combine information from two sources but only keep observations that are present in both. This ensures that the resulting dataset includes only complete cases based on the join key.

Other common join types include `left_join()` (keep all from the left), `right_join()` (keep all from the right), and `full_join()` (keep all from both).

If you want to learn more about `inner_join()` and other join functions, you can click the following link: [Documentation inner_join()](https://dplyr.tidyverse.org/reference/mutate-joins.html).

#>


**Task:** Fill in the blanks to merge the `city_info` and `share_data` datasets using `inner_join()`, group the data by `number` (number of monitors), and calculate the mean and confidence intervals for `share_rev_10`.

```{r "4.1.4"}

#< fill_in
merged_data <- city_info %>%
  ________(_________, by = "city_id") %>%
  ________(number) %>%  # Group by the number of monitors
  summarise(
    mean_y = mean(share_rev_10, na.rm = TRUE),  # Calculate mean value
    lower_ci = mean_y - 1.96 * sd(share_rev_10, na.rm = TRUE) / sqrt(n()),  # Lower limit CI
    upper_ci = mean_y + 1.96 * sd(share_rev_10, na.rm = TRUE) / sqrt(n())   # Upper limit CI
  ) %>%
  ungroup()
#>

merged_data <- city_info %>%
  inner_join(share_data, by = "city_id") %>%
  group_by(number) %>%  # Group by the number of monitors
  summarise(
    mean_y = mean(share_rev_10, na.rm = TRUE),  #  Calculate mean value
    lower_ci = mean_y - 1.96 * sd(share_rev_10, na.rm = TRUE) / sqrt(n()),  # Lower limit CI
    upper_ci = mean_y + 1.96 * sd(share_rev_10, na.rm = TRUE) / sqrt(n())   # Upper limit CI
  ) %>%
  ungroup()

```
As a result, the `merged_data` object now contains a summarized dataset showing the mean coverage of high-pollution activities `(mean_y)` for each group of cities categorized by the number of monitors. Additionally, the 95% confidence intervals `(lower_ci and upper_ci)` provide an estimate of the uncertainty around the mean for each group. This allows us to analyze how monitoring intensity correlates with pollution coverage.

Next, we aim to visualize the summarized data by creating a plot that displays the relationship between the number of monitors `(number)` and the mean coverage of high-pollution activities `(mean_y)`. The plot will include points for the mean values and error bars representing the 95% confidence intervals `(lower_ci and upper_ci)`. 


**Task:** Just press `check` to create a plot to visualize the relationship between the number of monitors `(number)` and the mean coverage of high-pollution activities `(mean_y)`.

```{r "4.1.5"}
#< task_notest
#  Ensure error bars for monitor group 1 (bottom) and 6 (top) are visible
merged_data <- merged_data %>%
  mutate(
    lower_ci = ifelse(number == 1, 0, lower_ci),
    upper_ci = ifelse(number == 6, 1, upper_ci)
  )

# Create the plot
ggplot(merged_data, aes(x = number, y = mean_y)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2, linewidth = 0.5) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(breaks = 1:6) +
  labs(
    y = "Percent of High-Pollution Activity <10km from Monitor",
    x = "Number of monitors"
  ) +
  theme_minimal()

#>
```
So, what do we see here? 

The graph shows the relationship between the number of monitors installed in a city and the percentage of high-pollution activity covered within a 10 km radius. As the number of monitors increases, the share of high-pollution activity covered rises steadily, from around **20%** with 1 monitor to over **80%** with 6 monitors. This clear upward trend indicates that more monitors lead to better coverage of pollution sources.

#< quiz "Understanding_Confidence_Intervals"
question: What do the vertical lines in the plot represent?
sc:
- The confidence intervals around the estimated percentage of high-pollution activity.*
- The number of monitors in each city.
- The range of pollution levels in different areas.
- The distribution of monitors across different locations.
success: Correct! 
failure: Not quite. 
#>

The vertical error bars in the graph represent 95% confidence intervals, showing the range in which the true mean coverage of high-pollution activity likely falls. The intervals are narrower for cities with 2–4 monitors, as seen in the HTML table above, where most cities are concentrated (86 cities with 2 monitors and 57 with 4 monitors). Larger sample sizes in these groups reduce statistical uncertainty, leading to more precise estimates. In contrast, cities with 1 monitor (26 cities) or 6 monitors (8 cities) have fewer observations, resulting in wider confidence intervals and greater variability.

To causally identify the impact of an additional monitor on air pollution, we rely on the assumption that cities with different numbers of monitors would have followed similar pollution trajectories in the absence of the intervention. A critical first step is to assess the plausibility of this assumption. To that end, we begin by examining trends in Aerosol Optical Depth (AOD) for different groups of cities, categorized according to the minimum number of monitors assigned by the central government.

As a second descriptive analysis in this chapter, we will plot the de-meaned city-level AOD trends for our four monitor groups. This allows us to explore the evolution of air pollution over time and to visually inspect whether systematic differences in trends exist across these groups. While this does not constitute a formal test, it provides useful insight into the temporal dynamics of pollution across cities with varying monitoring intensity and helps lay the groundwork for our identification strategy.

**Task:** Press `check` to load the dataset *city_pm_clean.rds* using `read_rds()` and store it in the variable `city_pm`. Then display the first few rows with `head()` to preview the structure of the data.

```{r "4.1.6"}
#< task
# Load the city-level pollution dataset and preview the first rows
#>
city_pm <- read_rds("city_pm_clean.rds")
head(city_pm, 3)

```

**Note**: I have cleaned the dataset again by removing unnecessary environmental, centroid, and coastal distance variables to ensure a cleaner output and better presentation.

The *city_pm_clean.rds* file contains variables such as `pm25` (particulate matter values), `city_id` (city identifiers), `number_iv` (a grouping variable indicating the number of monitors), and `year` (year of observation).


We aim to calculate de-meaned values to remove time-invariant, city-specific effects from the `pm25` variable. Specifically, for each city `(city_id)`, we subtract the city-level mean of `pm25` from each observation. This process isolates the within-city variation in `pm25` over time by removing persistent differences across cities caused by factors such as geography, infrastructure, or industry that remain constant over time. These de-meaned values highlight how `pm25` deviates from the city-specific average, making it easier to analyze trends within cities.

**Task:** Calculate city-level de-meaned `pm25` values. Use `group_by(city_id)` to compute the average `pm25` within each city and subtract it from each observation. Store the residuals as a new variable called `pm_res` in the `city_pm` dataset. Then ungroup the data to return to the full dataset structure.

```{r "4.1.7"}

#< fill_in
city_pm <- city_pm %>%
  group_by(city_id) %>%
  mutate(______ = ___ - mean(pm25, na.rm = TRUE)) %>%  # Subtract city-level means
  ungroup()


#>
city_pm <- city_pm %>%
  group_by(city_id) %>%
  mutate(pm_res = pm25 - mean(pm25, na.rm = TRUE)) %>%  # Subtract city-level means
  ungroup()

```

Next, we want to group the data by `number_iv` (categories or groups, e.g., 1, 2, 4, 6) and year to calculate the average residuals `(pm_res)` for each combination of `group` and `year`. This involves summarizing the data by computing the mean of `pm_res` within each `group` and `year`.


By de-meaning the data, we focus only on the time-varying component of pollution within each city. This helps us analyze trends or impacts (such as the effect of additional monitors) without bias from static city-specific characteristics, ensuring that any observed changes are not driven by these unchanging differences but instead reflect meaningful variation over time.


**Task:** Group the dataset by `number_iv` and `year`, and then calculate the average of the de-meaned pollution values `(pm_res)` for each group-year combination. Store the result in a new dataset called `collapsed_data` and display the first few rows using `head()`.

```{r "4.1.8"}

#< fill_in
collapsed_data <- city_pm %>%
  _______(number_iv, _____) %>%
  _________(pm_res = mean(pm_res, na.rm = TRUE), .groups = 'drop')  # Use .groups to avoid grouping warning
head(collapsed_data )
#>
collapsed_data <- city_pm %>%
  group_by(number_iv, year) %>%
  summarize(pm_res = mean(pm_res, na.rm = TRUE), .groups = 'drop')  # Use .groups to avoid grouping warning
head(collapsed_data )

```


Finally, we can plot the trends in Aerosol Optical Depth (AOD) over time to examine how it develops across different groups. The plot will include key temporal markers to highlight important time periods and provide a clear comparison between the groups.

**Task:** Just `check` the chunk to visualize AOD trends over time by monitor group and highlight the policy introduction with a vertical line.

```{r "4.1.9"}

#< task_notest
ggplot(collapsed_data, aes(x = year, y = pm_res, color = factor(number_iv))) +
  geom_line(size = 1) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 2014.5, linetype = "dashed", color = "#9B1B30") +  # Cranberry vertical line
  scale_x_continuous(breaks = 2010:2017, labels = as.character(2010:2017)) +
  scale_y_continuous(limits = c(-0.1, 0.1)) +  # Set y-axis limits to match original
  labs(y = "Aerosol Optical Depth", x = "Year") +
  theme_minimal() +
  theme(legend.position = "none") +
  annotate("text", x = 2016.8, y = -0.008, label = "One") +
  annotate("text", x = 2016.8, y = -0.05, label = "Two") +
  annotate("text", x = 2016.8, y = -0.085, label = "Four") +
  annotate("text", x = 2016.8, y = -0.097, label = "Six")

#>
```

#< quiz "Assessing the Validity of Common Trends"
question: What evidence supports the assumption of common trends before 2015?
sc:  
- The AOD trends in all groups diverge before 2015.
- All four groups share a common AOD trend before 2015.* 
- Cities with one monitor experience a sharp decline in AOD before 2015.  
- The AOD levels remain constant in cities with six monitors after 2015.  
success: Correct! 
failure: Not quite. 
#>

Before 2015, all groups share a common AOD trend, supporting the validity of the common trends assumption. After 2015, AOD levels begin to diverge, with cities assigned a larger number of monitors experiencing a more substantial reduction in AOD. In contrast, cities with only one monitor show a relatively flat trend, indicating no major change in pollution, which makes them a suitable control group for causal analysis. These findings suggest that the installation of additional monitors is associated with a sharper decline in AOD levels.

**Summary**

In this section, we analyzed the relationship between the number of pollution monitors in a city and both the coverage of high-pollution activities and trends in air quality. We showed that cities with more monitors have significantly higher coverage of polluting firms, confirming the effectiveness of the monitor allocation criteria. Furthermore, by examining de-meaned AOD trends, we found that all city groups followed similar pollution trajectories before 2015, supporting the common trends assumption. After 2015, AOD levels declined more strongly in cities with more monitors, suggesting that broader monitoring contributed to improved air quality.

In the next chapter, we build on these findings by incorporating fixed effects and control variables to account for unobserved heterogeneity across cities and over time. We also formally assess the parallel trends assumption and apply a difference-in-differences framework to estimate the causal impact of monitoring intensity on air pollution.



## Exercise 4.2 -- Incorporating Fixed Effects and Control Variables

Having previously applied a DiD approach at the firm level, we now turn to a city-level DiD framework to estimate the broader effects of monitoring intensity on air pollution. Building on the descriptive analysis in the previous section, we develop a more rigorous empirical framework that allows us to compare pollution trends across cities with different levels of monitoring before and after the policy intervention. To address potential confounding factors and account for unobserved heterogeneity, we incorporate fixed effects and a set of relevant control variables into our regression models.

By incorporating fixed effects and a set of carefully selected control variables into the difference-in-differences framework, we are able to control for both unobserved time-invariant characteristics specific to each city and observable time-varying factors that may influence pollution levels. Fixed effects capture city-specific attributes such as geography, baseline industrial structure, or institutional capacity that remain constant over time. Control variables, on the other hand, account for dynamic influences like economic growth, weather conditions, or political developments. This combined approach improves the internal validity of our estimates and provides a more credible basis for drawing causal conclusions about the effect of monitoring intensity on environmental outcomes.

We begin this analysis by gradually extending the model to include fixed effects, allowing us to control for unobserved heterogeneity across cities and over time. This step also enables us to explore whether the parallel trends assumption holds in a more rigorous specification before introducing additional control variables and the full identification strategy.

First and foremost, let us read the data set *city_pm.rds*.

**Task:** Just `check` the chunk below to load the data set.

```{r "4.2.1"}

#< task
dat.pt <- readRDS("city_pm.rds")
#>

```


## Fixed Effects and Parallel Trends

City fixed effects play a central role in evaluating environmental policies at the municipal level. They control for time-invariant characteristics that differ across cities, such as baseline industrial structure, administrative capacity, or local regulatory culture, which could otherwise bias the estimated effect of policy interventions. In line with Axbard and Deng (2024), who include city fixed effects in their city-level difference-in-differences framework, this approach helps absorb persistent cross-sectional differences across municipalities. This, in turn, allows for a cleaner identification of the causal impact of changes in monitoring intensity.
In our setting, city fixed effects are combined with time and policy target by time fixed effects to account for broader trends and centrally defined environmental objectives. This layered structure improves the credibility of the difference-in-differences estimation by ruling out confounding from unobserved location-specific factors that remain constant over time.

As emphasized in *Chapter 3*, the validity of our identification strategy hinges on the parallel trends assumption, which underlies the difference-in-differences (DiD) framework. This assumption requires that, in the absence of treatment, the treated and control groups would have followed similar trajectories over time. To evaluate this assumption, we begin by estimating a baseline model that includes only city and year fixed effects, enabling a visual inspection of pre-treatment dynamics.

To guide this analysis, we follow a structured approach inspired by Sebastian Kranz’s method for assessing parallel trends, as outlined in his [*Economics and R Blog*](https://skranz.github.io/r/2021/10/20/ParallelTrendsPlot.html#plot). Although his procedure is typically applied to models that combine fixed effects with control variables (FE + CV), we begin by implementing it using a simpler fixed-effects-only specification. While Kranz provides an R package called `ParallelTrendsPlot` that automates the visualization, we replicate the core steps manually to better understand the underlying logic.

The approach comprises the following steps:

1. Define treatment and post-treatment indicators.
2. Estimate a fixed effects model without control variables.
3. Compute adjusted outcome values using predictions and residuals.
4. Aggregate data by treatment status and time.
5. Plot group trends and check for parallelism before intervention.


First, we define the key variables for the DiD analysis. The treatment variable indicates cities with above-median monitor counts, and the `post` variable marks the period from 2015 onward. The interaction term `exp_treat` equals one only for treated cities in the post-treatment period and captures the causal effect of the intervention.


**Task:**  Fill in the blanks to define the DiD interaction term `exp_treat` by multiplying the `post` and `treatment` indicators.

```{r "4.2.2"}

#< fill_in

# Define Treatment, Post, and Interaction Variables for DiD
dat.pt <- dat.pt %>%
  mutate(
    treatment = ifelse(number > median(number, na.rm = TRUE), 1, 0),  
    post = ifelse(year >= 2015, 1, 0),  
    exp_treat = ________ * _________  
  )


#>

dat.pt <- dat.pt %>%
  mutate(
    treatment = ifelse(number > median(number, na.rm = TRUE), 1, 0),  
    post = ifelse(year >= 2015, 1, 0),  
    exp_treat = post * treatment  
  )

  
```

The treatment variable is defined based on whether the number of a given observation is above the median value of number across all observations. If `number` is greater than the `median`, the observation is assigned to the treatment group (1), and if it is equal to or below the median, it is assigned to the control group (0). This data-driven approach ensures a balanced comparison by splitting the sample into two groups based on a central tendency measure.

We now estimate a fixed effects model to control for unobserved heterogeneity across cities and over time. By including `city`, `year`, and `month` fixed effects, we account for persistent city-specific characteristics as well as broader temporal trends. This allows us to isolate the effect of the policy intervention on PM2.5 pollution levels, ensuring that observed changes are not confounded by structural differences between cities or common shocks across time.

The inclusion of city fixed effects controls for persistent differences between cities, while year and month fixed effects account for broader annual trends and seasonal variations in air pollution. 

**Task:** Fill in the blanks to correctly specify the fixed effects structure in the model. Ensure that the model includes `city`, `year`, and `month` fixed effects while clustering at the city level for robust standard errors.

```{r "4.2.3"}

#< fill_in
# Estimate the fixed effects Model 

reg.fe <- feols(pm25 ~ exp_treat | _____ + ____ + _____,  
                cluster = ~city_id, data = dat.pt)
  
#>


reg.fe <- feols(pm25 ~ exp_treat | city_id + year + month,  
                cluster = ~city_id, data = dat.pt)
  
```

After estimating the fixed effects model, we now want to compute predicted values using the model and add residuals to retain meaningful variation in pollution levels. This step ensures that the adjusted PM2.5 values `(pm_fe)` reflect deviations from the fixed effects model, while accounting for `city`, `year`, and `month` effects.

By doing this, we obtain a version of PM2.5 that is free from fixed effects influences, allowing us to analyze trends without interference from time-invariant city characteristics or broader seasonal patterns. 

**Task:** Fill in the blanks to correctly compute predicted values using the fixed effects model. Use the `predict()` function to generate fitted values while adding residuals to retain within-group variation. 


```{r "4.2.4"}

#< fill_in

# Compute predicted values from fixed effects model and recover original variation by adding residuals
dat.pt$pm_fe <- predict(reg.fe, newdata = dat.pt) + resid(_____)
  
#>

dat.pt$pm_fe <- predict(reg.fe, newdata = dat.pt) + resid(reg.fe)
  
```

Now, we prepare the data for visualization by computing average PM2.5 levels for treatment and control groups over time. We summarize the data, calculate pre- and post-policy means for annotations, and generate a parallel trends plot to assess trend similarities before the intervention.

**Task:** Just `check` the chunk to plot parallel trends in FE-adjusted PM2.5 for treatment and control groups over time, including pre/post means and a 2015 policy marker.

```{r "4.2.5"}

#< task_notest
# Prepare Data for Plotting
dat.pt.mean.fe <- dat.pt %>%
  select(pm_fe, treatment, year) %>%
  group_by(treatment, year) %>%
  summarise(measurement = mean(pm_fe, na.rm = TRUE), .groups = "drop")

# Compute Mean Values for Annotations
y.pre.tr.fe <- dat.pt.mean.fe %>% filter(year < 2015, treatment == 1) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
y.exp.tr.fe <- dat.pt.mean.fe %>% filter(year > 2014, treatment == 1) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
y.pre.co.fe <- dat.pt.mean.fe %>% filter(year < 2015, treatment == 0) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
y.exp.co.fe <- dat.pt.mean.fe %>% filter(year > 2014, treatment == 0) %>% summarise(mean(measurement)) %>% pull() %>% round(2)

# Generate the Parallel Trends Plot (fixed effects Only)
plot_fe <- ggplot(dat.pt.mean.fe, aes(x = as.factor(year), y = measurement, 
                                      color = as.factor(treatment), group = as.factor(treatment))) + 
  geom_line(linewidth = 1.2) +
  geom_point(size = 3) +
  geom_vline(xintercept = as.factor(2015), linetype = "dashed", color = "black") +
  xlab("Year") +
  ylab("Average Air Pollution (PM2.5, FE Adjusted)") + 
  scale_color_discrete(labels = c("Control", "Treatment"), name = "Group") +
  ggtitle("Parallel Trends: Fixed Effects Model") +
  theme_bw() +
  annotate("label", x = "2014", y = y.pre.tr.fe - 0.02, label = y.pre.tr.fe, fill = "black", color = "white") +
  annotate("label", x = "2017", y = y.exp.tr.fe - 0.015, label = y.exp.tr.fe, fill = "black", color = "white") +
  annotate("label", x = "2014", y = y.pre.co.fe + 0.025, label = y.pre.co.fe, fill = "grey30", color = "white") +
  annotate("label", x = "2017", y = y.exp.co.fe + 0.02, label = y.exp.co.fe, fill = "grey30", color = "white")

# Print the plot
print(plot_fe)

#>

```

The graph visualizes the average PM2.5 levels over time for the treatment and control groups, with a dashed vertical line at 2015 marking the policy intervention.

#< quiz "parallel_trends_validation"
question: What does the pre-2015 trend indicate about the validity of the DiD approach?
sc:
- The treatment and control groups followed different trends, violating the parallel trends assumption.
- The treatment and control groups exhibited relatively stable trends before 2015, supporting the parallel trends assumption.*
- The treatment group had significantly higher pollution levels before 2015, making the analysis invalid.
- The parallel trends assumption is not necessary for a DiD analysis.

success: Well done!
failure: Try again! 
#>

Before the policy intervention in 2015, the treatment and control groups followed relatively stable and parallel trends in adjusted PM2.5 levels. Although the treatment group exhibited consistently higher pollution levels, the gap between the two groups remained fairly constant, supporting the parallel trends assumption that underlies a valid difference-in-differences (DiD) analysis.

After 2015, the average PM2.5 levels in the treatment group declined substantially from **0.53** in the pre-policy period (2010–2014) to **0.39** in the post-policy period (2015–2017). In contrast, the control group experienced a more moderate reduction, from **0.31** to **0.24** over the same periods. This divergence suggests that the monitoring policy had a stronger effect on treated cities, likely through enhanced enforcement or increased accountability.

Importantly, the values are adjusted for `city`, `year`, and `month` fixed effects. This ensures that the differences observed between the two groups are not driven by persistent city-level characteristics, time-specific shocks, or seasonal patterns. Taken together, the results provide strong evidence that the monitoring intervention contributed to a causal reduction in urban air pollution, underlining the regulatory effectiveness of real-time monitoring.


As previously mentioned, we now aim to incorporate additional control variables to address potential shortcomings in the parallel trends assumption. By doing so, we seek to reduce unexplained variance and ensure that our model accounts for key factors that may influence the outcome variable. This will allow us to assess whether the observed trends remain consistent when controlling for these additional variables.


## Control Variables and Parallel Trends

Now, we want to perform the same analysis, but this time incorporating both fixed effects (FE) and control variables (CV). By including `city`, `year`, and `month` fixed effects, along with additional control variables (`area`, `population`, `temperature`, `age`, `incentives`, and `predictions`), we aim to account for potential confounders while isolating the true policy effect. This approach ensures that our results are not driven by time-invariant city characteristics or broader time trends.

So let extend our model by incorporating additional control variables to improve the robustness of our parallel trends assumption check. These controls help account for potential confounding factors that may influence air pollution levels:

* *Area*: Captures geographic differences that might affect pollution levels.
* *Population (pop)*: Controls for differences in city size and density, which could impact pollution dynamics.
* *tem_meand*: Represents the mean temperature, accounting for weather-related fluctuations in air pollution.
* *age_year*: Controls for structural and demographic characteristics that evolve over time.
* *incentive2*: Captures policy or financial incentives that could influence pollution control measures.
* *pred*: represents precipitation (rainfall), controlling for its pollution-clearing effect.


We now aim to construct the following regression equation:

$$
y_{crt} = \beta_1 \cdot m_{ct} + \delta_c + \gamma_{rt} + \lambda \cdot X_{ct} + \varepsilon_{crt}
$$


In this equation, $y_{crt}$ is the outcome variable, defined either as monthly aerosol optical depth (AOD) or the log number of firms receiving air pollution enforcement. The variable $m_{ct}$ captures monitoring intensity, either actual or predicted. Fixed effects $\delta_c$ and $\gamma_{rt}$ control for time-invariant city characteristics and target-group-specific time trends, respectively. The term $X_{ct}$ includes city-level time-varying controls, such as weather, mayoral age, and baseline city features interacted with the post-treatment period. The coefficient $\beta_1$ measures the effect of monitoring intensity on the outcome, and $\varepsilon_{crt}$ is the error term.

By implementing this regression, we can isolate the causal impact of monitoring intensity on pollution and regulatory enforcement, accounting for both city- and firm-level fixed effects as well as regional and temporal variations. 

We want to check whether the parallel trends assumption still holds after including control variables and fixed effects. To do so, we once again follow the method proposed by Sebastian Kranz, which is specifically designed to handle this more complex setting.

Let's begin the analysis by estimating the difference-in-differences (DiD) regression with both control variables (CV) and fixed effects (FE).


#< info "Including control variables and fixed effects in a DiD regression with feols."

To estimate a difference-in-differences (DiD) model with control variables and fixed effects, we again use the `feols` function from the `fixest` package. Control variables (CV) account for observed characteristics, while fixed effects (FE) control for unobserved heterogeneity across units and time.

```{r eval=FALSE}
feols(outcome ~ treatment_vars + control_vars | fixed_effects, cluster = ~cluster_var, data = your_data)
```

- treatment_vars: includes treatment, post, and their interaction
- control_vars: observed covariates added on the right-hand side
- fixed_effects: added after the `|` symbol to account for unit and time variation
- cluster: specifies the level at which standard errors are clustered (e.g., city_id)

Using `feols` with CV, FE, and cluster helps improve causal identification by adjusting for both observed and unobserved confounding factors.

#>

**Task:** Complete the DiD regression by including all relevant control variables (CV) and fixed effects (FE). Add `year`, `city_id`, and `month` as fixed effects, and control for time-varying factors like `area`, `pop`, `tem_meand`, `age_year`, `incentive2`, and `pred` to improve causal inference.


```{r "4.2.6"}

#< fill_in

# Estimate the fixed effects Model with control variables
  
reg.fe.cv <- feols(pm25 ~ post + treatment + exp_treat + ______ + _____ + ______ + age_year + incentive2 + pred | year + city_id + month, 
                   cluster = ~city_id, data = dat.pt)
  
#>


reg.fe.cv <- feols(pm25 ~ post + treatment + exp_treat + area + pop + tem_meand + age_year + incentive2 + pred | year + city_id + month, 
                   cluster = ~city_id, data = dat.pt)
  
  
```


Then, we hold all control variables (CV) constant, except for the treatment indicator and the post-treatment period. This ensures that any variation in the predicted outcomes is solely attributable to the policy intervention, effectively isolating its impact. To simplify the analysis, we will set all continuous control variables to zero.

**Task:** Fill in the blanks to create a new dataset where all control variables are held constant.

```{r "4.2.7"}

#< fill_in

# Create a New Dataset with Constant Control Variables
dat.0 <- dat.pt %>%
  mutate(area = _, pop = _, tem_meand = _, age_year = _, incentive2 = _, pred=_)
  
  
#>


dat.0 <- dat.pt %>%
  mutate(area = 0, pop = 0, tem_meand = 0, age_year = 0, incentive2 = 0, pred = 0)  
  
```

Next, we compute predicted values using the fixed effects + control variables (FE + CV) model while adding residuals to retain variation. This ensures that the adjusted PM2.5 values reflect only the policy impact, removing confounding influences. Then, we aggregate the data to prepare for visualizing trends across treatment and control groups over time.

**Task:** Just `check` the following chunk to compute predicted values using the fixed effects model with control variables, add residuals, and prepare the data for visualization by aggregating mean values by `treatment` group and `year`.

```{r "4.2.8"}

#< task
#Compute Predicted Values Using fixed effects + CV and Add Residuals
#Prepare Data for Plotting

dat.pt$pm_cv <- predict(reg.fe.cv, newdata = dat.0) + resid(reg.fe.cv)

dat.pt.mean.fe.cv <- dat.pt %>%
select(pm_cv, treatment, year) %>%
group_by(treatment, year) %>%
summarise(measurement = mean(pm_cv, na.rm = TRUE), .groups = "drop")
  
#>

```

As before, we now compute the average values for annotations and generate the parallel trends plot. 
This step helps us visually assess whether the treatment and control groups follow similar trends before the intervention, now incorporating control variables to refine the analysis.

I have positioned the parallel trends model with FE from above alongside the FE + CV model to provide a clearer comparison and better evaluate how incorporating control variables influences the parallel trends.

**Task:** Just `check` the following chunk to display the results.

```{r "4.2.9", fig.width=14, fig.height=5}

#< task_notest
#  Compute Mean Values for Annotations
  y.pre.tr.cv <- dat.pt.mean.fe.cv %>% filter(year < 2015, treatment == 1) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
  y.exp.tr.cv <- dat.pt.mean.fe.cv %>% filter(year > 2014, treatment == 1) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
  y.pre.co.cv <- dat.pt.mean.fe.cv %>% filter(year < 2015, treatment == 0) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
  y.exp.co.cv <- dat.pt.mean.fe.cv %>% filter(year > 2014, treatment == 0) %>% summarise(mean(measurement)) %>% pull() %>% round(2)
# Parallel Trends Plot for Model 1 (fixed effects Only)
 lot_fe <- ggplot(dat.pt.mean.fe, aes(x = as.factor(year), y = measurement, 
                                      color = as.factor(treatment), group = as.factor(treatment))) + 
  geom_line(linewidth = 1.2) +
  geom_point(size = 3) +
  geom_vline(xintercept = as.factor(2015), linetype = "dashed", color = "black") +
  xlab("Year") +
  ylab("Average Air Pollution (PM2.5, FE Adjusted)") + 
  scale_color_discrete(labels = c("Control", "Treatment"), name = "Group") +
  ggtitle("Parallel Trends: Fixed Effects Model") +
  theme_bw() +
  annotate("label", x = "2014", y = y.pre.tr.fe - 0.02, label = y.pre.tr.fe, fill = "black", color = "white") +
  annotate("label", x = "2017", y = y.exp.tr.fe - 0.015, label = y.exp.tr.fe, fill = "black", color = "white") +
  annotate("label", x = "2014", y = y.pre.co.fe + 0.025, label = y.pre.co.fe, fill = "grey30", color = "white") +
  annotate("label", x = "2017", y = y.exp.co.fe + 0.02, label = y.exp.co.fe, fill = "grey30", color = "white")

# Generate the Parallel Trends Plot for fixed effects + control variables
plot_fe_cv <- ggplot(dat.pt.mean.fe.cv, aes(x = as.factor(year), y = measurement, 
                                            color = as.factor(treatment), group = as.factor(treatment))) + 
  geom_line(linewidth = 1.2) +
  geom_point(size = 3) +
  geom_vline(xintercept = as.factor(2015), linetype = "dashed", color = "black") +
  xlab("Year") +
  ylab("Average Air Pollution (PM2.5, FE + CV Adjusted)") + 
  scale_color_discrete(labels = c("Control", "Treatment"), name = "Group") +
  ggtitle("Parallel Trends: Fixed Effects + Control Variables") +
  theme_bw() +
  annotate("label", x = "2014", y = y.pre.tr.cv - 0.02, label = y.pre.tr.cv, fill = "black", color = "white") +
  annotate("label", x = "2017", y = y.exp.tr.cv - 0.015, label = y.exp.tr.cv, fill = "black", color = "white") +
  annotate("label", x = "2014", y = y.pre.co.cv + 0.025, label = y.pre.co.cv, fill = "grey30", color = "white") +
  annotate("label", x = "2017", y = y.exp.co.cv + 0.02, label = y.exp.co.cv, fill = "grey30", color = "white")

# Display Both Plots Side by Side for Direct Comparison
grid.arrange(plot_fe, plot_fe_cv, ncol = 2)
  

#>
```

The two plots compare average air pollution levels (PM2.5) for treatment and control groups before and after the policy intervention in 2015. The left plot represents the fixed effects (FE) model, while the right plot includes both fixed effects and control variables (FE + CV).

#< quiz "Comparing FE and FE with Controls"
question: What is the main takeaway from comparing the fixed effects (FE) model to the fixed effects plus control variables (FE + CV) model in the parallel trends plots?
sc:
- The overall patterns are similar, but the inclusion of control variables slightly shifts the level of pollution in both groups.*
- The control group shows a completely different trend in the FE + CV model.
- The treatment effect disappears when control variables are added.
- The FE + CV model shows a stronger violation of the parallel trends assumption.
success: Correct!
failure: Not quite. 
#>


Both plots indicate that the parallel trends assumption appears to be satisfied before the policy was introduced in 2015. In both the fixed effects model and the fixed effects model with control variables, the treatment and control groups show similar patterns in the pre-treatment period. While the inclusion of control variables slightly shifts the overall level of air pollution, it does not change the direction or shape of the trends.

This comparison supports the validity of the difference-in-differences design. However, it remains unclear which model more accurately reflects the true parallel trends, as each relies on different assumptions. Therefore, it is important to consider both specifications when evaluating the robustness of the results.

The fact that the results are similar across the two model types indicates that the estimated treatment effect is robust and not driven by omitted variable bias. The control variables do not substantially change the interpretation but rather strengthen the credibility of the findings.

After verifying the parallel trends assumption, we now examine the difference-in-differences (DiD) model with fixed effects and control variables, as specified by the authors. This will allow us to assess how including additional covariates impacts the estimated treatment effect.

To begin, we first create the interaction term for the difference-in-differences (DiD) estimator. This interaction captures the effect of monitoring intensity on pollution by combining the post-treatment period indicator with the number of monitors in a city. Unlike the earlier approach for our parallel trends assumption, where we classified cities into treatment and control groups based on whether their number of monitors was above or below the median, this method directly quantifies the treatment intensity by using the actual number of monitors. However, both approaches rely on the same post-treatment indicator to capture changes after the intervention.



**Task:** Just press `check` to create the DiD interaction term `Monitor_Effect`, which captures the effect of monitoring intensity `(number)` after the policy intervention `(post1)`.

```{r "4.2.10"}

#< task
# Create the interaction term for the DiD estimator
dat <- read_rds("city_pm.rds") %>%
  mutate(Monitor_Effect = post1 * number)
#>

```

Now, we estimate the fully specified difference-in-differences (DiD) model, incorporating fixed effects (FE) and control variables (CV), following the approach used by the authors. This approach includes `city`, `year`, and `month` fixed effects, while adjusting for economic and environmental factors to enhance the precision of our estimation.

**Task:** Fill in the blanks with `Monitor_Effect` (interaction term), `city_id` (fixed effects), and `pred` (control variable) to complete the DiD regression for PM2.5 levels. 

```{r "4.2.11"}

#< fill_in
# Run the DiD regression
did_model_1 <- feols(pm25 ~ ___________ + post1 * area + post1 * pop |
                       ________ + year + month + _______ + tem_meand + age_year + incentive2 * time, 
                     cluster = ~city_id, data = dat)

#>

did_model_1 <- feols(pm25 ~ Monitor_Effect + post1 * area + post1 * pop |
                       city_id + year + month + pred + tem_meand + age_year + incentive2 * time, 
                     cluster = ~city_id, data = dat)



```

Hmm… something feels off here. Do you see it? 
**Note:** it’s not about how we defined the Monitor Effect.

#< quiz "Model Specification Issue"
question: What is the key difference between the authors' DiD model and ours in terms of fixed effects (FE) and control variables (CV)?
sc:
- Our model includes more fixed effects than the authors' model.
- The authors' model estimates the impact of precipitation, temperature, and incentives separately, while our model absorbs them into fixed effects.
- There is no difference in how fixed effects and control variables are handled.
- The authors' model absorbs precipitation, temperature, age, and incentives into Fixed Effects, while our model treats them as explicit control variables.*
success: Correct! 
failure: Incorrect. Try again! 
#>


It appears that the authors unintentionally absorbed `precipitation`, `temperature`, `age`, and `incentives` into the fixed effects (FE) rather than treating them as explicit control variables (CV). By doing so, these variables are no longer estimated separately, but rather implicitly controlled for, meaning their individual impact on PM2.5 levels cannot be observed. This approach removes variation that could help refine the estimation of the policy effect. Given the structure of their model, it is likely that the authors did not intend to specify it this way, and an oversight or mistake in their model formulation led to the unintended absorption of key control variables into the fixed effects.

We now assess whether this modeling decision meaningfully affects the estimated treatment effect. Specifically, we compare the three model specifications we previously estimated: one that includes only fixed effects, another that combines fixed effects with explicit control variables, and the authors’ model, where key control variables appear to have been absorbed into the fixed effects. This comparison allows us to evaluate how the treatment effect responds to different modeling choices and whether omitting control variables from the regression materially alters the results or if the estimated policy effect remains robust across specifications.

**Task:** Just press `check` to display the results of our three models.

```{r "4.2.12"}
#< task

# Estimate the three models
reg.fe <- feols(pm25 ~ Monitor_Effect | city_id + year + month,  
                cluster = ~city_id, data = dat)

reg.fe.cv <- feols(pm25 ~ Monitor_Effect + post1 * area + post1* pop + pred + tem_meand + age_year + incentive2*time |
                       city_id + year + month,  
                     cluster = ~city_id, data = dat)

did_model_1 <- feols(pm25 ~ Monitor_Effect+ post1 * area + post1 * pop |
                       city_id + year + month + pred + tem_meand + age_year + incentive2 * time, 
                     cluster = "city_id", data = dat)



# Create a list of models
models <- list("FE Only" = reg.fe, 
               "FE + CV" = reg.fe.cv, 
               "Authors' Model FE+CV" = did_model_1)

# Display only Monitor_Effect
modelsummary(models, 
             coef_map = c("Monitor_Effect" = "Monitor Effect"),
             stars = TRUE, 
             gof_omit = "IC|Log|Adj|F",
             output = "kableExtra") %>%
kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover"))
#>
```

#< quiz "comparison results"
question: How does the estimated Monitor Effect change across the three model specifications?
sc:
- It disappears in the FE + CV model.
- It becomes insignificant in the FE-only model.
- It becomes stronger in the authors' model compared to the others.*
- It fluctuates randomly with no pattern.
success: Correct! 
failure: Not quite. 
#>

The results consistently show a negative and significant Monitor Effect across all models, confirming that increased monitoring reduces PM2.5 levels. However, the estimated magnitude varies depending on model specification. The FE Only model estimates an effect of **−0.028**, while the FE + CV model slightly reduces this to **−0.024**, suggesting that omitted factors in the FE Only model may have led to a slight overestimation. The authors’ model shows the largest effect at **−0.031**, likely due to misallocating control variables into fixed effects (FE), which removes meaningful variation and potentially over-controls the estimation.

This estimated Monitor Effect of **−0.031** means that increased monitoring leads to an average reduction of **0.031** units in PM2.5 levels. While this confirms the effectiveness of monitoring in improving air quality, the slightly larger effect in the authors' model suggests that some variation was absorbed into the fixed effects, potentially inflating the estimate.

Although the authors’ model has the highest R² (0.719) and lowest RMSE (0.12), this comes at the cost of reduced transparency regarding how individual control variables influence pollution. The FE + CV model provides the most reliable estimate, avoiding both omitted variable bias (present in the FE Only model) and over-controlling (as seen in the authors’ model). This comparison underscores the impact of model specification on the estimated policy effect, highlighting the importance of properly distinguishing fixed effects and control variables.
Importantly, the differences in the estimated Monitor Effects across the three specifications are relatively small. 

Nonetheless, we will proceed with the authors' model as it forms the basis for the next chapter. This ensures that our findings remain aligned with their estimates and maintain a consistent methodological approach.


As the final step in this chapter, we extend our difference-in-differences (DiD) analysis to examine the effect of monitoring on enforcement activity. Using the same model structure as before, incorporating fixed effects (FE) and control variables (CV) in line with the authors’ specification, we assess whether increased monitoring intensity affected the log number of firms subjected to air pollution enforcement measures. This allows us to evaluate whether enhanced monitoring translated into stronger regulatory action.


**Task:** Click `check` to run the code and estimate the impact of monitoring on regulatory enforcement using the difference-in-differences (DiD) approach. This analysis follows the same methodology as before but now examines whether increased monitoring influenced the log of firms receiving air pollution enforcement actions.

```{r "4.2.13"}

#< task
# Load dataset
dat <- read_rds("city_enf.rds") %>%
  mutate(Monitor_Effect = post1 * number)  # Create interaction term

# Run DiD regression with correct time variable
did_model_2 <- feols(log_any_air ~ Monitor_Effect + post1 * area + post1 * pop |
                     city_id + year + quarter + pred + tem_meand + age_year + incentive2 * time, 
                   cluster = "city_id", data = dat)

# Display results using modelsummary
modelsummary(
  list("Authors' Model FE+CV ENF" = did_model_2),   # <<–– Name hier ändern
  coef_map = c("Monitor_Effect" = "Monitor Effect"),
  stars = TRUE, 
  gof_omit = "IC|Log|Adj|F",
  output = "kableExtra"
) %>%
  kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover"))

#>


```

The result of the difference-in-differences (DiD) analysis for enforcement outcomes shows a Monitor Effect estimate of **0.151** with a standard error of **0.045**.

#< quiz "Expected_Impact_of_Monitors_on_Enforcement"
question: Does the positive Monitor Effect coefficient **(0.151)** align with expectations regarding the impact of monitors on enforcement actions?
sc:
- Yes, because installing monitors is expected to increase regulatory enforcement actions by providing better data on pollution violations.*
- No, because installing monitors should not influence enforcement actions directly.
- No, because we expected a decrease in enforcement actions after installing monitors.
- Yes, because monitors are expected to reduce enforcement actions by improving compliance.
success: Correct! 
failure: Not quite. 
#>

The positive Monitor Effect coefficient of **0.151** suggests that installing air pollution monitors increases regulatory enforcement actions by providing better data on pollution violations. This aligns with expectations, as more precise monitoring allows authorities to identify and target firms that exceed pollution limits more effectively. The statistically significant effect indicates that improved monitoring plays a key role in strengthening enforcement efforts and ensuring compliance with environmental regulations. 

**Summary**

This chapter used difference-in-differences models to estimate the impact of monitoring on pollution. After confirming the parallel trends assumption, we compared models with and without control variables. All specifications showed a significant negative effect of monitoring on pollution, confirming its effectiveness. While effect sizes varied slightly, results were robust across models. The specification with both fixed effects and control variables offered the most reliable estimate. Additionally, enforcement analysis showed that increased monitoring led to stronger regulatory actions.

To address potential endogeneity concerns such as reverse causality or omitted variable bias in the relationship between monitoring and pollution, the next chapter introduces an instrumental variables (IV) approach. This method helps identify the causal effect of monitoring intensity more reliably by using an exogenous source of variation in monitor assignment.



## Exercise 4.3 -- Instrumental Variables Approach

One potential concern in estimating the causal impact of pollution monitoring on enforcement and pollution reduction is the endogeneity of monitor placement. If cities with stricter environmental enforcement policies or higher pollution levels were more likely to receive more monitors, our estimates may be biased. To address this concern, we implement an instrumental variable (IV) strategy, leveraging the exogenous assignment of monitors by the central government.

The instrumental variable (IV) approach is a method used in econometrics to address the issue of endogeneity, which occurs when an explanatory variable is correlated with the error term in a regression. Endogeneity can arise due to omitted variable bias, reverse causality, or measurement error, leading to biased estimates. The IV approach helps to obtain consistent estimators by using instruments that are correlated with the endogenous explanatory variables but uncorrelated with the error term (Angrist & Pischke, 2008, pp. 83–85).

In the context of this study, one concern is that cities may have strategically chosen the number of monitors based on unobserved factors, such as expected future pollution levels or enforcement capacity. If this is the case, a simple difference-in-differences (DiD) approach may yield biased results because the treatment variable (number of monitors) is not randomly assigned.



#< quiz "Identification Strategy"
question: Recall *Chapter 4.1*. The central government implemented a systematic method for distributing pollution monitors across cities. What key factors influenced this allocation?
sc:
- Population size and geographical area.*
- Historical pollution levels.
- The number of previous enforcement actions.
- Random assignment across all cities.
success: Great, your answer is correct!
failure: Try again.
#>


The central government allocated pollution monitors to cities based on predefined criteria, particularly population size and geographical area. These rules determined the number of monitors a city received, independent of its prior enforcement levels or pollution reduction efforts. We exploit this exogenous variation in monitor assignment as an instrument to isolate the causal effect of monitoring on enforcement and pollution.


In empirical research, estimating causal effects can be challenging when an explanatory variable is endogenous. Consider the following regression model:

$$
Y = \beta_0 + \beta_1 X + \gamma W + \epsilon
$$

In this equation, $Y$ denotes the dependent variable, $X$ is the endogenous explanatory variable, $W$ is a vector of control variables, and $\epsilon$ is the error term.



The endogeneity problem arises when *X* is correlated with $\epsilon$ :

$$
\text{Cov}(X, \epsilon) \neq 0
$$

This correlation leads to biased and inconsistent estimates when using Ordinary Least Squares (OLS).
To resolve endogeneity, we use an instrumental variable (IV), denoted as *Z*, that satisfies two key conditions:

**1. Relevance Condition**  

- The instrument *Z* must be correlated with *X*:

$$
\text{Cov}(Z, X) \neq 0
$$

This ensures that *Z* influences *X* sufficiently to explain variation.

**2. Exogeneity Condition**  

- The instrument *Z* must be uncorrelated with the error term $\epsilon$:

$$
\text{Cov}(Z, \epsilon) = 0
$$

This guarantees that *Z* affects *Y* only through *X* and is not directly related to other unobserved factors. These conditions are essential for the validity of the IV approach and are thoroughly discussed in econometric literature. For a comprehensive treatment, see Wooldridge (2010, pp. 83ff.).

To further clarify the identification strategy proposed by the authors, I constructed a **Directed Acyclic Graph (DAG)**. The graph summarizes the key causal relationships in the model. The number of installed monitors, *X*, may be endogenous due to unobserved city-specific factors, *U*, that also influence enforcement or pollution outcomes, *Y*. To address this issue, the centrally assigned number of monitors, *Z*, is used as an instrument. Since *Z* is based on predetermined allocation rules and is unrelated to local conditions, it provides exogenous variation in *X*. The DAG illustrates that *Z* affects YY only through *X*, supporting the validity of the IV approach discussed by the authors.


**Task:** Click `check` to generate the instrumental variable (IV) DAG. 

```{r "4.3.1",  echo=TRUE, fig.width=12, fig.height=7}
#< task
# Load package
library(igraph)

# Define node IDs
nodes <- c("Z", "X", "Y", "U")

# Define edges (directed)
edges <- matrix(c(
  "Z", "X",  # Instrument → Treatment
  "X", "Y",  # Treatment → Outcome
  "U", "X",  # Confounder → Treatment
  "U", "Y"   # Confounder → Outcome
), byrow = TRUE, ncol = 2)

# Create graph
g <- graph_from_edgelist(edges, directed = TRUE)

# Set readable labels
V(g)$label <- c(
  "Z: Assigned Monitors",
  "X: Installed Monitors",
  "Y: Pollution / Enforcement",
  "U: Unobserved Factors"
)

# Set node color to yellow
V(g)$color <- "gold"

# Custom layout for clean positioning
layout <- matrix(c(
  0, 1,   # Z
  1, 0.5, # X
  2, 0,   # Y
  1, 1.5  # U
), byrow = TRUE, ncol = 2)

# Plot the DAG
plot(g,
     layout = layout,
     vertex.size = 45,
     vertex.label.cex = 1.1,
     vertex.label.color = "black",
     edge.arrow.size = 0.5,
     main = "IV DAG with Unobserved Factors")
#>

```

#< quiz "Understanding the IV DAG"
question: Why is the instrument Assigned Monitors *Z* considered valid in the context of this model?
sc:
- Because Assigned Monitors Z influence the number of Installed Monitors X, and X affects the Outcome Y, while Z is assumed to be independent of unobserved confounders U.*
- Because Assigned Monitors Z directly determine the Outcome Y, such as pollution levels or enforcement intensity.
- Because the number of Installed Monitors X is randomly assigned across cities.
- Because unobserved factors U influence only the Outcome Y, but not the number of Installed Monitors X.
success: Correct!
failure: Try again.
#>


The directed acyclic graph (DAG) illustrates the logic behind the instrumental variable (IV) approach. The number of assigned monitors, denoted by *Z*, affects the number of installed monitors, *X*. In turn, *X* influences the outcome variable *Y*, such as air pollution levels or the intensity of enforcement. The validity of this identification strategy rests on two key assumptions. First, *Z* must be relevant, meaning it must be correlated with *X*. Second, *Z* must be exogenous, meaning it affects *Y* only through its impact on *X* and is uncorrelated with unobserved determinants of *Y*, denoted by *U*.

Monitor assignment was determined centrally using predetermined criteria like population size and geographic area, reducing the likelihood of strategic placement. Moreover, *Z* has no direct channel through which it could affect enforcement or pollution outcomes, satisfying the exclusion restriction.

We proceed under the assumption that *Z* is a valid instrument, providing exogenous variation in monitoring intensity. In the next step, we formally evaluate these conditions and estimate the causal effect of installed monitors on outcomes using the Two-Stage Least Squares (2SLS) method.


## Two-Stage Least Squares (2SLS) 

Two-Stage Least Squares (2SLS)  is an econometric method used to address endogeneity by replacing endogenous variables with predicted values from instrumental variables. This approach enhances the reliability of regression estimates and is widely applied in causal analysis and policy evaluation. By implementing 2SLS, endogeneity bias is mitigated, ensuring consistent parameter estimates. The method consists of two stages:

**First Stage: Predicting the Endogenous Variable**

We regress X on Z:

$$
X = \pi_0 + \pi_1 Z + \gamma W + v
$$

This provides the predicted values $\hat{X}$, which contain only exogenous variation.

**Second Stage: Estimating the Causal Effect**

We replace X with $\hat{X}$ in the main equation:

$$
Y = \beta_0 + \beta_1 \hat{X} + \gamma W + u
$$

This ensures that only the exogenous component of X is used in estimation. A detailed treatment of the Two-Stage Least Squares (2SLS) method, including its assumptions and implementation, can be found in Angrist & Imbens (1995, pp. 431–442) and Angrist & Pischke (2008, pp. 90–91).

We first begin by analyzing the instrument relevance condition, as this is the first requirement for a valid instrumental variable. The relevance condition states that the instrument must have a non-zero impact on the endogenous variable.


#< quiz "instrument_relevance_1"
question: Does the number of government-assigned pollution monitors significantly influence the number of installed monitors?
sc:
- Yes, assigned monitors strongly predict installed monitors.*
- No, there is no significant relationship.
success: Correct! 
failure: Try again.
#>

Since the central government assigned pollution monitors based on predetermined criteria such as population size and geographical area, it is reasonable to assume that these assignments directly influence the actual number of installed monitors.

Thus, we have strong theoretical justification that the instrument is relevant. To empirically assess this, we estimate the first-stage regression separately by regressing the endogenous variable (Installed Monitors) on the instrument (Assigned Monitors) to examine how well the instrument explains the endogenous variable.

**Task:** Just `check` the chunk to load the dataset.

```{r "4.3.2"}

#< task
#read city_pm.rds with read_rds() and save it in dat

dat <- read_rds("city_pm.rds")
#>
```

First, we generate an interaction term that captures how the effect of monitoring enforcement varies depending on the number of installed monitors.
This is important because the impact of the policy (post-policy period) may not be uniform but instead depend on the intensity of monitoring in each city. By incorporating this interaction, we ensure that our IV estimation correctly reflects variations in enforcement responses and air quality across different monitoring levels.

**Task:** Just `check` the chunk to  create the interaction term `Monitor_Effect_IV`.

```{r "4.3.3"}

#< task
#create the interaction term Monitor_Effect_IV

dat$Monitor_Effect_IV <- dat$post1 * dat$number

#>
```

Next, we estimate the first-stage regression to examine how well the instrument (Assigned Monitors) predicts the endogenous variable (Installed Monitors). This step is crucial for assessing the validity of our instrumental variable (IV) approach, ensuring that the instrument is strongly correlated with the endogenous variable.

By including relevant controls and fixed effects, we account for potential confounding factors and isolate the exogenous variation in monitoring intensity.

**Task:** Fill in the blanks to estimate the first-stage regression, which examines whether the instrument `(number_iv)` significantly predicts the potentially endogenous treatment variable `(Monitor_Effect_IV)` in the post-treatment period.

```{r "4.3.4"}

#< fill_in
# Run the first-stage regression to test whether the instrument (number_iv) explains variation in Monitor_Effect_IV
first_stage <- feols(________________ ~ post1 * number_iv + post1 * area + post1 * pop + 
                       pred + tem_meand + age_year + incentive2:time | city_id + year:quarter, 
                     data = dat, cluster = "city_id")

summary(first_stage)

#>
   
first_stage <- feols(Monitor_Effect_IV ~ post1 * number_iv + post1 * area + post1 * pop + 
                       pred + tem_meand + age_year + incentive2:time | city_id + year:quarter, 
                     data = dat, cluster = "city_id")

summary(first_stage) 
```

The table presents the results of the first-stage regression, where the endogenous variable `Monitor_Effect_IV` is regressed on the instrument `(post1:number_iv)` along with various control variables. The key coefficient of interest, `post1:number_iv`, is estimated at **0.722** with a standard error of **0.111**, resulting in a t-value of 6.53 and a highly significant p-value (<0.001). This indicates a strong correlation between the instrument and the endogenous variable. Additionally, the Adjusted R² of 0.949 suggests that the model explains a substantial portion of the variation in the dependent variable.

To ensure that the instrument is strong enough for a valid IV estimation, we calculate the **First-Stage F-Statistic**, which is given by:

$$
F = \left(\frac{\beta}{SE_{\beta}}\right)^2
$$

This formula assesses the strength of the instrument by evaluating the significance of its coefficient in the first-stage regression. As discussed by Staiger and Stock (1997, p.557), a first-stage F-statistic above 10 is widely regarded as a benchmark for identifying strong instruments.


#< quiz "first_stage_f_stat_value"
question: What is the calculated First-Stage F-Statistic for post1:number_iv?
sc:
- 6.53
- 42.71*
- 10.11
- 0.722

success: Correct! 
failure: Try again.
#>

The calculated **First-Stage F-Statistic** for `post1:number_iv` is **42.71**, derived from:

$$
F = \left(\frac{0.722}{0.111}\right)^2 = 42.71
$$

Since this value is well above the critical threshold of 10, the instrument is considered  strong. This ensures that `number_iv` provides sufficient variation in `Monitor_Effect_IV`, reducing the risk of weak instrument bias. As a result, the second-stage IV estimation can be expected to produce consistent and reliable estimates of the causal effect.


So far, we have confirmed that the instrument satisfies the relevance condition, as the number of assigned monitors `(number_iv)` significantly predicts the number of installed monitors `(number)` in the first-stage regression. However, to establish the validity of `number_iv` as an instrument, we must also assess whether it meets the exogeneity condition.

The validity of our IV strategy relies on the exogeneity of `number_iv`, meaning it must affect outcomes only through its influence on installed monitors `(number)`, not via other unobserved factors.

This assumption is supported by the following points:

* *Centralized Assignment:* Axbard and Deng (2024) state that the number of monitors assigned to each city was determined by the central government based on predetermined criteria, namely population size and the geographical size of the built-up area. This rule-based allocation mitigates concerns about endogenous placement of monitors.

* *No Evidence of Strategic Placement:* The authors examine the determinants of monitor location and find that placement is unrelated to prior enforcement activity and that there are no differential pre-trends in enforcement for firms at varying distances from the monitor. This supports the validity of the identification strategy.

* *No Differential Pre-Trends:* Using a difference-in-differences design, Axbard and Deng (2024) show that firms located near and far from monitors followed similar enforcement and pollution trends before the monitors were installed, suggesting that monitor assignment was not correlated with unobserved determinants of the outcomes.

Together, these factors provide strong support for the exogeneity of `number_iv` and the credibility of our IV estimates. Since we rely on a single instrument, formal tests of exogeneity are not feasible. While we substantiate the assumption with institutional context and empirical evidence, it cannot be directly verified and is therefore not examined further empirically.

Now that we have established both the relevance and the exogeneity of the instrument, we proceed with the second-stage regression to estimate the causal effect of monitoring on enforcement. This step is essential because it uses the exogenous variation in monitoring intensity provided by the instrument to identify the impact on enforcement outcomes. By addressing potential endogeneity in monitor placement, this approach produces more credible and consistent estimates of the policy’s effect.

Click on `Info` to learn more about how to implement instrumental variables in a difference-in-differences framework using the `feols()` function.


#< info "Using instrumental variables in a difference-in-differences model with feols()."

Instrumental variable (IV) estimation in `feols()` uses a three-part formula to handle potential endogeneity. This is useful when a treatment variable (e.g., monitoring intensity) may be correlated with unobserved factors.

```{r eval=FALSE}
feols(outcome ~ controls | fixed_effects | endogenous_var ~ instrument, data = your_data, cluster = ~cluster_var)
```

- endogenous_var ~ instrument: specifies the instrument for the endogenous variable
- Fixed effects and clustering are specified as usual
- The instrument must be relevant and exogenous to identify the causal effect

This setup combines difference-in-differences with IV estimation, improving causal inference when simple comparisons may be biased.

The details are specified in the [fixest walkthrough on instrumental variables](https://cran.r-project.org/web/packages/fixest/vignettes/fixest_walkthrough.html#3_Instrumental_variables).

#>


**Task:** Fill in the blanks to specify the dependent variable `(pm25)` and the instrument `(number_iv)` in the DiD + IV estimation, ensuring a correct instrumental variable regression for estimating the effect of monitoring on air pollution levels.

```{r "4.3.5"}

#< fill_in
# Estimate DiD + IV regression: instrument Monitor_Effect_IV to assess its effect on pm25
did_iv_model_3 <- feols(_____ ~ post1 * area + post1 * pop | 
                          city_id + year:month + pred + tem_meand + age_year + incentive2:time |
                          Monitor_Effect_IV ~ post1 * __________,  # Correct Instrument Equation
                        data = dat, cluster = "city_id")


#>
   
did_iv_model_3 <- feols(pm25 ~ post1 * area + post1 * pop | 
                          city_id + year:month + pred + tem_meand + age_year + incentive2:time |
                          Monitor_Effect_IV ~ post1 * number_iv,  # Correct Instrument Equation
                        data = dat, cluster = "city_id")

```


Before we look at the results for PM2.5, we first apply the same DiD + IV estimation for enforcement outcomes.
This ensures consistency in our empirical strategy and allows us to directly compare whether increased monitoring affected both regulatory enforcement and air quality in a meaningful way.


**Task:** Just `check` the chunk to  run the DiD + IV regression for enforcement.

```{r "4.3.6"}
#< task
#Load the dataset
dat <- readRDS("city_enf.rds")

# Create the interaction variable for IV regression
dat$Monitor_Effect_IV <- dat$post1 * dat$number


#DiD + IV Estimation for enforcement outcomes
did_iv_model_4 <- feols(log_any_air ~ post1 * area + post1 * pop | 
                          city_id + year:quarter + pred + tem_meand + age_year + incentive2:time |
                          Monitor_Effect_IV ~ post1 * number_iv,  # Correct Instrument Equation
                        data = dat, cluster = "city_id")

#>

```

## Comparison OLS and 2SLS

Now that we have estimated both models using the difference-in-differences approach with an instrumental variable, one for enforcement and one for air pollution, we can compare these results to the baseline DiD models from the previous chapter.

This comparison is important for evaluating whether potential endogeneity in monitor placement may have biased the initial estimates. If the IV results differ substantially from the baseline findings, it suggests that unobserved factors may have influenced the earlier analysis. In that case, the instrumental variable strategy offers a more credible and consistent estimate of the causal effect of monitoring on enforcement and pollution outcomes.


**Task:** Just press `check` to display a formatted summary table comparing the estimated effects of monitoring on pollution and enforcement across DiD and DiD + IV models.


```{r "4.3.7"}

#< task_notest
#  Load DiD model for PM2.5 
dat_pm <- read_rds("city_pm.rds")

dat_pm$Monitor_Effect <- dat_pm$post1 * dat_pm$number

did_model_1 <- feols(pm25 ~ Monitor_Effect + post1 * area + post1 * pop |
                     city_id + year + month + pred + tem_meand + age_year + incentive2 * time,
                     data = dat_pm, cluster = "city_id")

# Load DiD model for enforcement 
dat_enf <- read_rds("city_enf.rds")

dat_enf$Monitor_Effect <- dat_enf$post1 * dat_enf$number

did_model_2 <- feols(log_any_air ~ Monitor_Effect + post1 * area + post1 * pop |
                     city_id + year + quarter + pred + tem_meand + age_year + incentive2 * time,
                     data = dat_enf, cluster = "city_id")
model_list <- list(
  "DiD: PM2.5" = did_model_1,
  "DiD: Enforcement" = did_model_2,
  "DiD + IV: PM2.5" = did_iv_model_3,
  "DiD + IV: Enforcement" = did_iv_model_4
)

# Extract tidy model results
tidy_list <- model_list %>%
  lapply(broom::tidy, conf.int = FALSE)

# Filter for Monitoring Effect only
monitor_table <- bind_rows(tidy_list, .id = "Model") %>%
  filter(term %in% c("Monitor_Effect", "fit_Monitor_Effect_IV")) %>%
  mutate(
    # Assign panel labels
    term_label = case_when(
      Model %in% c("DiD: PM2.5", "DiD + IV: PM2.5") ~ "PM2.5",
      Model %in% c("DiD: Enforcement", "DiD + IV: Enforcement") ~ "Enforcement"
    ),
    # Strategy for columns
    strategy = case_when(
      grepl("DiD \\+ IV", Model) ~ "DiD + IV",
      TRUE ~ "DiD"
    ),
    # Format estimate with stars and standard error
    estimate_formatted = paste0(
      sprintf("%.3f", estimate),
      ifelse(p.value < 0.001, "***",
      ifelse(p.value < 0.01, "**",
      ifelse(p.value < 0.05, "*",
      ifelse(p.value < 0.1, "+", "")))),
      " (", sprintf("%.3f", std.error), ")"
    )
  ) %>%
  select(term_label, strategy, estimate_formatted)

# Pivot to wide format: rows = outcomes, columns = DiD / DiD + IV
monitor_table_wide <- monitor_table %>%
  pivot_wider(names_from = strategy, values_from = estimate_formatted)

# Rename first column for clarity
colnames(monitor_table_wide)[1] <- "Outcome"

# Generate final table
monitor_table_wide %>%
  kable(format = "html", escape = FALSE,
        caption = "Impact of Monitoring on Pollution and Enforcement") %>%
  kable_styling(full_width = FALSE, position = "center",
                bootstrap_options = c("striped", "hover"))
#>

```

#< quiz "IV_vs_DiD_results"
question:  What is the main difference between the IV and the regular DiD estimates of the monitoring effect?
sc:
- The IV estimates show a weaker effect than the DiD estimates, suggesting that monitoring has less impact than initially thought.
- The IV and DiD estimates are identical, meaning endogeneity does not influence the results.
- The IV estimates show a stronger effect of monitoring on both pollution reduction and enforcement compared to the DiD estimates.*
- The IV estimates show no significant relationship between monitoring and either pollution or enforcement.

success: Correct! 
failure: Try again! 
#>

Our results closely match those of the authors, confirming the validity of our replication.
The IV estimates reveal a stronger causal impact of monitoring on both enforcement intensity and pollution reduction compared to the DiD estimates, suggesting that the original DiD specification may have underestimated the true effect due to endogeneity.

For pollution reduction, our IV estimate **(−0.046)** is larger in magnitude than the DiD estimate **(−0.031)**, consistent with the paper’s finding that an additional monitor leads to a **3.1–4.6%** reduction in PM2.5 concentrations, reinforcing the credibility of the IV estimates. This suggests that the DiD approach underestimates the true effect of monitoring due to omitted variable bias or non-random monitor placement.

For enforcement actions, our IV estimate **(0.202)** is also larger than the DiD estimate **(0.151)**, in line with the authors’ conclusion that an additional monitor increases enforcement by **15–19%**. The IV approach reveals that monitoring intensity directly influences enforcement,whereas DiD alone might underestimate enforcement effects due to unobserved factors like political pressure or local pollution trends that are not adequately controlled for.


As we established in the previous chapter, the authors made a minor error in assigning fixed effects (FE) and control variables (CV). We now re-estimate the models using the correct specifications to assess whether the main results remain consistent.

**Task:** Just press `check` to display the results for all four models with the correct FE and CV assignments and compare them to the original estimates.  

```{r "4.3.8"}

#< task_notest
#  Create interaction term for IV models
dat_pm$Monitor_Effect_IV <- dat_pm$post1 * dat_pm$number
dat_enf$Monitor_Effect_IV <- dat_enf$post1 * dat_enf$number


# Corrected DiD Model for PM2.5
did_model_1 <- feols(pm25 ~ Monitor_Effect + post1 * area + post1 * pop + pred + tem_meand + age_year + incentive2:time |
                     city_id + year:month,
                     data = dat_pm, cluster = "city_id")

# Corrected DiD Model for Enforcement
did_model_2 <- feols(log_any_air ~ Monitor_Effect + post1 * area + post1 * pop + pred + tem_meand + age_year + incentive2:time |
                     city_id + year:quarter,
                     data = dat_enf, cluster = "city_id")

# Corrected DiD + IV Model for PM2.5
did_iv_model_3 <- feols(pm25 ~ post1 * area + post1 * pop + pred + tem_meand + age_year + incentive2:time | 
                        city_id + year:month |
                        Monitor_Effect_IV ~ post1 * number_iv,
                        data = dat_pm, cluster = "city_id")

# Corrected DiD + IV Model for Enforcement
did_iv_model_4 <- feols(log_any_air ~ post1 * area + post1 * pop + pred + tem_meand + age_year + incentive2:time | 
                        city_id + year:quarter |
                        Monitor_Effect_IV ~ post1 * number_iv,
                        data = dat_enf, cluster = "city_id")
# Compare all models
model_list <- list(
  "DiD: PM2.5" = did_model_1,
  "DiD: Enforcement" = did_model_2,
  "DiD + IV: PM2.5" = did_iv_model_3,
  "DiD + IV: Enforcement" = did_iv_model_4
)

# Extract and tidy coefficients
tidy_models <- model_list %>%
  lapply(tidy, conf.int = FALSE) %>%
  bind_rows(.id = "Model")

# Filter only Monitoring Effects
monitor_table <- tidy_models %>%
  filter(term %in% c("Monitor_Effect", "fit_Monitor_Effect_IV")) %>%
  mutate(
    Outcome = case_when(
      grepl("PM2.5", Model) ~ "PM2.5",
      grepl("Enforcement", Model) ~ "Enforcement"
    ),
    Strategy = case_when(
      grepl("DiD \\+ IV", Model) ~ "DiD + IV",
      TRUE ~ "DiD"
    ),
    # Format estimates and SEs with stars
    Result = paste0(
      sprintf("%.3f", estimate),
      case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        p.value < 0.1   ~ "+",
        TRUE            ~ ""
      ),
      " (", sprintf("%.3f", std.error), ")"
    )
  ) %>%
  select(Outcome, Strategy, Result) %>%
  pivot_wider(names_from = Strategy, values_from = Result)

# Format and print table
monitor_table %>%
  kable(format = "html", escape = FALSE,
        caption = "Impact of Monitoring on Pollution and Enforcement (correct FE and CV assignments)") %>%
  kable_styling(full_width = FALSE, position = "center",
                bootstrap_options = c("striped", "hover"))
#>

```


With the corrected fixed effects (FE) and control variables (CV), the estimated effects of monitoring on pollution and enforcement change slightly but remain consistent in direction and significance.
For PM2.5, both the DiD and IV estimates are slightly smaller than in the original model (DiD: **−0.024** vs. **−0.031**, IV: **−0.036** vs. **−0.046**), suggesting that the true pollution reduction effect is weaker than initially estimated.
For enforcement, the corrected model shows a stronger effect (DiD: **0.185** vs. **0.151**, IV: **0.232** vs. **0.202**), indicating that monitoring influences enforcement more than previously thought.
While the IV estimates remain larger than DiD, the correction suggests the bias in the original DiD model was smaller than initially assumed, leading to a more precise causal estimate.

#< award "Urban Causality Specialist" >
You saw through the smog, literally and econometrically. From firm data to full city dynamics, your toolkit of fixed effects, control variables, and a clean IV turned uncertainty into urban insight.
#>

**Summary**

In this chapter, we addressed potential endogeneity in monitor placement by applying an instrumental variables (IV) strategy using the government-assigned number of monitors as an instrument. After confirming instrument relevance and plausibly satisfying the exogeneity condition, we estimated Two-Stage Least Squares (2SLS) models for both pollution and enforcement outcomes.
Using DiD + IV, we found stronger effects on pollution reduction and enforcement compared to DiD alone, indicating that the DiD estimates were downward biased due to endogeneity. The IV approach corrects this bias, confirming that exogenous variation in monitoring enhances both enforcement intensity and pollution control.

Next, we examine the mechanisms behind the observed effects, focusing on top-down accountability and information provision to understand how monitoring improves governance and enforcement.

## Exercise 5 -- Mechanisms of Improved Governance and Enforcement

In this section, we examine two critical mechanisms that underpin the effectiveness of the air pollution monitoring program.

First, we focus on top-down accountability through performance incentives. This analysis explores how the introduction of air quality monitors enhances the ability of the central government to hold local officials accountable for achieving pollution reduction targets. Specifically, we investigate the role of promotion incentives tied to environmental performance and how they drive stricter enforcement of regulations at the local level. The question is whether these incentives create stronger alignment between local actions and central policy goals.

Second, we analyze changing information provision by examining the impact of transferring control of pollution monitors from local governments to independent third parties. This shift is crucial in addressing data manipulation concerns and improving the reliability of pollution data. We explore whether this change in information governance increases the effectiveness of enforcement actions and how improved data quality contributes to better environmental outcomes.

Together, these mechanisms provide insights into how institutional structures and information systems can enhance governance and enforcement in environmental policy.


### Structure

5.1 Top-Down Accountability

5.2 Changing Information Provision



## Exercise 5.1 -- Top-Down Accountability

In China's centralized political system, the central government sets environmental policies, while local governments are responsible for implementation. This arrangement can lead to principal-agent challenges, as local officials may prioritize economic growth over environmental mandates. To address this, the central government has linked officials' career advancements to environmental performance, introducing accountability measures to ensure compliance with national goals (Wu & Cao, 2021, S. 1–2).

#< info "Effective Top-Down Accountability: Evidence from Uganda."

An illustrative example of effective top-down accountability comes from a field experiment in Uganda (Buntaine & Daniels, 2020). In this study, local citizen monitors submitted standardized reports about service delivery issues to higher-level authorities. These reports were certified and regularly forwarded to the central government.

The key insight: Corruption decreased significantly when citizen reports were combined with central oversight, while bottom-up monitoring alone had little effect.

Why was the combination so effective?

- Higher-level officials had the authority to respond;
- Local actors could no longer ignore problems;
- Reports created a credible threat of consequences.

This evidence supports the broader idea that information improves outcomes only when embedded in an institutional structure that enables enforcement from above. In China, air pollution monitors enabled the central government to hold mayors accountable. Similarly, in Uganda, oversight from higher levels of authority increased local responsiveness, provided that reliable information was available to those in charge.

If you want to read more about the Uganda case, click [here](https://journals.sagepub.com/doi/full/10.1177/2053168020934350).
 
#>

In China, the career advancement of local officials is closely tied to their economic performance, as promotions and terminations are significantly influenced by provincial growth outcomes (Li & Zhou, 2005, pp. 1746-1747). However, these political incentives are not uniform. They vary depending on the official’s career stage, age-related promotion constraints, and the national political calendar. Promotion opportunities are strongly influenced by the five-year cycle of the *National People’s Congress* (NPC), with the highest probability of promotion occurring in the final year of each cycle. In addition, mayors of prefecture-level cities in China face two institutional constraints: a mandatory retirement age of 60 and a requirement to serve at least three years in office to qualify for promotion. As a result, officials aged 57 or older at the time of the National People's Congress face a discontinuously lower probability of promotion and thus weaker performance incentives (Xi, Yao, & Zhang, 2018, p. 1048).

This age-based constraint affects policy enforcement behavior, as officials close to the promotion threshold have stronger incentives to align with central mandates, while those beyond this window may exhibit weaker compliance efforts. The introduction of real-time environmental monitoring enhances the central government’s ability to track policy enforcement, but its effectiveness depends on whether officials have career incentives to respond.


To better understand the role of age in shaping political incentives, we first examine the age distribution of local officials in our dataset. This allows us to assess the variation in career stages and how it aligns with promotion incentives.


**Task:** Just `check` the chunk to read the data set from *city_pm.rds* and save it in `dat`.

```{r "5.1.1"}

#< task
dat <- readRDS("city_pm.rds")  
#>

```

Next, we calculate key summary statistics, including the `minimum`, `maximum`, `median`, `mean`, and `quartiles` of the `age` variable. This provides an overview of the demographic structure of officials before proceeding with further analysis.


**Task:** Just `check` the chunk that summarizes the `age` variable.

```{r "5.1.2"}

#< task
#Summarize the age variable
summary(dat$age)
#>

```

#< quiz "age_distribution"
question: Based on the summary statistics of the `age` variable, what can we conclude about the typical career stage of officials in the dataset?
sc:
- The majority of officials are in their mid-50s.*
- Most officials are in their early 40s.
- Officials are evenly distributed across all ages.
- The dataset primarily consists of younger officials under 40.

success: Great, your answer is correct! 
failure: Try again. 
#>

To better visualize the age distribution of local officials, I created a histogram that provides a clear overview of the underlying demographic structure.

**Task:** Click on `check` to generate a histogram of the age distribution.

```{r "5.1.3"}

#< task_notest
#  Histogram of age distribution
ggplot(dat, aes(x = age)) +
  geom_histogram(binwidth = 2, fill = "blue", alpha = 0.6, color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution of Mayors",
       x = "Age",
       y = "Frequency")
#>

```

The histogram illustrates the age distribution of mayors, showing that the majority fall between 50 and 60 years old, with a peak in the mid-to-late 50s. This aligns with typical political career progressions, where officials reach leadership positions later in their careers. There are very few mayors under 40, suggesting that younger officials rarely attain such roles, while the decline after 60 reflects institutional retirement policies. The distribution is slightly skewed toward older ages, indicating that political promotions favor experienced officials. This pattern is crucial for understanding how career incentives shape policy enforcement, as younger mayors, still eligible for promotion, may be more responsive to accountability mechanisms than those nearing retirement.

Next, we examine the effect of mayoral age on pollution reduction to assess whether environmental monitoring leads to stronger enforcement at different career stages. By estimating the impact of monitoring on pollution levels across age groups, we aim to determine if younger mayors implement stricter pollution control measures compared to older counterparts.

To illustrate this relationship, we will visualize the estimated effect of monitoring on pollution by age, with the goal of identifying potential variations in enforcement intensity.

We begin by converting the `age` variable into a factor and set age 58 as the reference category. This allows us to estimate the interaction between `post1 × number` and mayor age relative to age 58. By doing so, we mirror the baseline group selection used in the original data specification, enabling a clear interpretation of the age-specific coefficients as deviations from the effect observed at age 58.

**Task:** Fill in the blank to convert `age` into a factor variable and set age 58 as the reference category for categorical comparisons in the regression.

```{r "5.1.4"}

#< fill_in
#  Define age as a categorical variable with 58 as the reference group
dat$age <- relevel(factor(dat$age), ref = "__")


#>

dat$age <- relevel(factor(dat$age), ref = "58")


```

#< quiz "Understanding the Reference Value"
question: Why is age 58 chosen as the reference value in this analysis? 
sc:
- It is an arbitrary midpoint in the analyzed age range.
- It is the oldest age in the analyzed range.
- It is the youngest age in the analyzed range.
- It marks a pivotal point where promotion incentives begin to weaken.*
success: Correct!
failure: Try again.
#>

A reference value serves as a baseline to interpret the effects of other categories or variables relative to it. In this analysis, age 58 is chosen as the reference because, according to the authors, it holds significant interpretive and policy relevance within the context of promotion incentives for mayors. Situated near the center of the analyzed age range (50 to 60), it provides a balanced baseline for comparison. At this age, mayors are still eligible for promotion but are nearing retirement, making it a pivotal point for understanding the dynamics of career incentives. Setting age 58 as the reference allows the effects of other ages to be interpreted relative to this group, offering insights into how monitoring and governance outcomes change as mayors deviate from this age. Additionally, age 58 marks a transition point where promotion incentives begin to weaken, emphasizing its importance in analyzing how career motivations influence governance and pollution control efforts.

Next, we want to examine whether the effect of pollution monitoring depends on the mayor’s age. We estimate a fixed effects model where `pm25` is regressed on monitoring intensity `(post1 * number)` interacted with mayor age. Using the `i() function`, we estimate age-specific effects relative to age 58. This allows us to test whether mayors close to promotion age respond more strongly to monitoring, consistent with a top-down accountability mechanism.


**Task:** Just `check` the chunk to estimate a fixed effects model with an age-specific interaction relative to age 58.

```{r "5.1.5"}

#< task
# Estimate fixed effects model with age-specific interaction effects (relative to age 58)
model <- feols(
  pm25 ~ post1 * number + post1 * area + post1 * pop +
    i(age, post1 * number, ref = "58") |
    city_id + interaction(year, month) + pred + tem_meand + year^incentive2,
  data = dat,
  cluster = "city_id"
)
#>
```


Finally, we extract the relevant interaction coefficients and plot them to visualize how the effect of monitoring intensity on pollution varies by mayor age. This helps us assess whether the treatment effect is driven by mayors with promotion incentives, as suggested by the proposed mechanism.

**Task:** Just `check` the following code chunk to load and prepare the interaction coefficients, and finally plot the graph showing the age-specific effects of monitoring on pollution.

```{r "5.1.6"}

#< task_notest
#  Extract coefficients and confidence intervals
coefs <- tidy(model, conf.int = TRUE)

# Filter interaction terms and keep only ages 50–60
coefs_age <- subset(coefs, grepl("age::\\d+:post1 \\* number", term) &
                              as.numeric(gsub("age::(\\d+):post1 \\* number", "\\1", term)) >= 50 &
                              as.numeric(gsub("age::(\\d+):post1 \\* number", "\\1", term)) <= 60)

# Extract numeric age
coefs_age$age <- as.numeric(gsub("age::(\\d+):post1 \\* number", "\\1", coefs_age$term))

# Create the plot
plot_age <- ggplot(coefs_age, aes(x = age, y = estimate)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  scale_x_continuous(breaks = 50:60) +
  scale_y_continuous(limits = c(-0.04, 0.02), breaks = seq(-0.04, 0.02, 0.02)) +
  labs(x = "Age of mayor", y = "", title = "Effect on pollution, by age") +
  theme_minimal()

# Display the plot
print(plot_age)

#>
```

#< quiz "Interpreting the Results from the Graph" 
question: What do the results from the graph suggest about the relationship between monitoring `(post1)` and air pollution `(pm25)` across mayoral ages? 
sc:
- Monitoring has a uniform effect on air pollution across all mayoral ages.
- Monitoring is most effective for mayors aged 59 and above.
- Monitoring has the strongest effect on reducing air pollution for mayors aged 57 and below.*
- Monitoring has no significant impact on air pollution regardless of mayoral age. 
success: Correct!  
failure: Not quite.Try again. 
#>

The results reveal that the effect of monitoring on air pollution reduction varies systematically with mayoral age. The graph shows that the impact is strongest for mayors aged 57 and younger. This supports the idea that younger officials, especially those close to potential promotion, are more responsive to central monitoring due to stronger career incentives (Xi, Yao, & Zhang, 2018, p. 1048).

For mayors aged 58 and above, the estimated effects become smaller and less statistically distinguishable from zero. This suggests that enforcement motivation declines as officials approach retirement and become ineligible for promotion. The noticeable shift around age 57 highlights the importance of political incentives in driving policy responsiveness.

These findings show that the effectiveness of monitoring depends on the local institutional context. In particular, the alignment between central oversight and individual career concerns plays a key role. Considering such incentive structures can help improve the design and impact of environmental policies.



**Summary**

In this chapter, we analyzed whether the impact of pollution monitoring varies with the age of local mayors. The results show that monitoring is more effective in reducing pollution when mayors are younger and closer to potential promotion. In contrast, older mayors nearing retirement respond less to monitoring. This supports the idea that top-down accountability works best when political incentives are aligned with individual career goals.

Having shown that promotion incentives and accountability mechanisms play a key role in improving environmental outcomes, we now turn to the role of information provision. The next chapter explores how changes such as real-time data availability can strengthen enforcement behavior and increase the effectiveness of environmental policy. In particular, we examine how monitoring systems enhance transparency and enable more responsive governance.




## Exercise 5.2 -- Changing Information Provision

This final chapter examines whether separating the responsibilities for information provision and enforcement can improve data quality, limit opportunities for manipulation, and ultimately lead to better environmental governance outcomes.

Although performance-based incentives are widely used to mitigate principal–agent problems, they can unintentionally encourage strategic behavior, including the manipulation of performance metrics to meet targets. This risk is particularly acute when the same actors are responsible both for enforcing regulations and for generating the data used to evaluate their own performance. Evidence suggests that institutional separation between information providers and enforcers can increase data integrity and reinforce accountability mechanisms (Banerjee, Duflo, & Glennerster, 2008, pp. 487–489; Sandefur & Glassman, 2015, pp. 116–118, 124).

To assess this question, we analyze the effects of transferring the responsibility for data provision from local governments to independent third parties. Building on the institutional shift in information flow outlined in *Chapter 1.1*, this chapter takes a closer look at how the reassignment influences the extent of data manipulation and the overall reliability of pollution measurements.

Since all monitors in our sample were reassigned simultaneously in 2016, we cannot exploit cross-sectional variation to estimate a causal effect of the information provider. Instead, we conduct a descriptive analysis to examine potential implications.

Specifically, we study how the Aerosol Optical Depth (AOD) elasticity of PM2.5 changes when the responsibility for data provision shifts from local governments to third parties. This approach allows us to investigate whether the shift improves the alignment between satellite-based AOD data and ground-level PM2.5 measurements, thereby enhancing the reliability of the data. By examining these changes, we aim to understand the broader impact of the reassignment on the quality of air pollution data and its implications for enforcement and accountability.

To estimate this relationship, we use the following regression model:

$$
log(PM2.5)mt=δ_m+γ_t+β_{1}⋅AOD_{mt}+β_2⋅AOD_{mt}×I_{t}+ε_{mt}
$$

Here, the dependent variable is the logarithm of $PM2.5$ concentration measured at monitoring station $m$ in month $t$. The key regressor $AOD_{mt}$ is a satellite-derived proxy for air pollution, while $I_t$ is a post-policy dummy indicating whether the period follows the reassignment of monitoring stations to third-party control. The model includes station fixed effects $\delta_m$ and time fixed effects $\gamma_t$ to account for time-invariant location characteristics and common temporal shocks, respectively. The interaction term identifies the differential relationship between satellite and ground-level pollution before and after the institutional reform, and $\varepsilon_{mt}$ captures unobserved factors.

#< quiz "interpretation_of_beta2"
question: What does a positive value of $\beta_2$ indicate in the regression model?
sc:
- The reassignment of monitors has no impact on data accuracy
- The alignment between AOD and PM2.5 improves after the reassignment of monitors*
- The relationship between AOD and PM2.5 weakens after the reassignment
- Pollution levels decrease significantly due to reassignment

success: Correct! 
failure: Not quite. 
#>

A positive $\beta_2$ indicates that the reassignment of pollution monitors to third parties led to improved data accuracy. This suggests that AOD (satellite-based pollution measures) and PM2.5 (ground-level pollution data) became more aligned after the change, reducing potential manipulation by local governments.


Let's now take a closer look at the results to see how the reassignment impacted the alignment between AOD and PM2.5.

**Task:** Click on `check` to load the dataset *monitor_api.rds* and display its first few rows to get an overview of the data.

```{r "5.2.1"}

#< task
monitor_api <- read_rds("monitor_api.rds")
head(monitor_api, 3)
#>
```


We first conduct regression analyses to assess whether AOD reliably predicts ground-level pollution, measured by the natural logarithm of PM2.5 concentrations `(log_pm25api)`. To prepare the data, we construct `log_pm25api` by applying the natural logarithm to `pm25api`, which normalizes its distribution and allows for interpretation of coefficients in percentage terms. We also define `reassign` as a binary variable equal to 1 for observations from November 2016 onward, capturing the policy shift in monitoring responsibilities. This setup enables us to examine both the predictive validity of `AOD` as a proxy for pollution and the impact of the institutional reform on data reliability.

**Task:** Complete the `mutate()` function to correctly define the `reassign` variable. Set `reassign` to 1 for all observations where the `year` is 2017, or where the `year` is 2016 and the `month` is greater than or equal to 11. In all other cases, set `reassign` to 0. This coding step ensures that the `reassign` variable accurately captures the start of the policy change, which was implemented beginning in November 2016.

```{r "5.2.2"}

#< fill_in
# Prepare the data
monitor_api <- monitor_api %>%
  mutate(
    log_pm25api = log(pm25api),  # Create the log of pm25api
    reassign = ifelse(year == ____ | (year == _____ & month >= _____), _____, ______),  # Define reassigned variable
    AOD = pm25
    )# Rename pm25 as AOD
#>
# Prepare the data
monitor_api <- monitor_api %>%
  mutate(
    log_pm25api = log(pm25api),  # Create the log of pm25api
    reassign = ifelse(year == 2017 | (year == 2016 & month >= 11), 1, 0),  # Define reassigned variable
    AOD = pm25
    )# Rename pm25 as AOD
```

The years 2016 and 2017 were selected because the reassignment of monitoring responsibilities to external parties began in November 2016. From this point onward, pollution data reflects third-party monitoring. Therefore, the `reassign` variable is set to 1 for observations from November 2016 onward, capturing the period after the policy change.

We now perform a regression analysis to examine the relationship between satellite-based pollution measurements (`AOD`) and the natural logarithm of ground-level PM2.5 concentrations `(log_pm25api)`. To account for unobserved heterogeneity, we include fixed effects for monitor ID `(monitor_id)` and time (`year` and `month`), which control for monitor-specific and temporal factors that may influence pollution levels. As in the previous chapters, we cluster standard errors at the city level `(~city_id)` to obtain robust statistical inference by accounting for potential within-city correlations.

**Task:** Fill in the ______ in the regression formula to specify the relationship between `AOD` and `log_pm25api`, including fixed effects for monitors `(monitor_id)` and time `(year:month)`. Ensure the model clusters standard errors by city `(~city_id)`.

```{r "5.2.3"}

#< fill_in
# Run Regression
model_1 <- feols(log_pm25api ~ ____ | ______ + ______^month, 
                   cluster = ~city_id, 
                   data = monitor_api)
#>

model_1 <- feols(log_pm25api ~ AOD | monitor_id + year^month, 
                   cluster = ~city_id, 
                   data = monitor_api)


```

Now, we aim to extract and display the coefficient and standard error for the `AOD` variable to summarize its relationship with `log_pm25api` as estimated in the regression model.

**Task:** Press `check` to display the coefficient for the `AOD` variable. 

```{r "5.2.4", eval=FALSE}

#< task
# Extract the coefficient and standard error for the variable AOD
aod_coefficient <- coef(model_1)["AOD"]
aod_std_error <- sqrt(vcov(model_1)["AOD", "AOD"])

# Create a data frame to store the results
coef_and_se <- data.frame(
  Coefficient = aod_coefficient,
  Std_Error = aod_std_error
)

# Add row names for better display without repeating variable name
row.names(coef_and_se) <- "AOD"

# Print the coefficient and standard error
print(round(coef_and_se, 3))


#>
```

#< quiz "Interpreting the Coefficient of AOD" 
question: What does the coefficient **0.315** for `AOD` suggest about its relationship with `log_pm25api`?
sc:
- A 1-unit increase in AOD is associated with a 0.315% increase in log_pm25api.
- AOD has no statistically significant relationship with log_pm25api.
- A 1-unit increase in AOD is associated with a 31.5% increase in log_pm25api.*
- A 1-unit increase in AOD decreases log_pm25api by 31.5%.
success: Correct!
failure: Not quite. Try again!

#>

After extracting the coefficient and standard error for `AOD`, we can interpret the coefficient value of **0.315** as follows: a 1-unit increase in `AOD` is associated with a **31.5%** increase in `log_pm25api`, indicating a positive and statistically significant relationship between satellite-based pollution measurements (`AOD`) and ground-level PM2.5 data (`log_pm25api`).

We now aim to expand our analysis by estimating a more comprehensive regression model. Specifically, we include an interaction term between `AOD` and the `reassign` variable to examine how the reassignment of monitoring responsibilities influences the relationship between satellite-based pollution measurements (`AOD`) and ground-level PM2.5 concentrations (`log_pm25api`). In addition, we control for variables such as `pre` (precipitation) and `tem_mean` (average temperature) to account for other factors that may affect pollution levels. This extended specification allows us to gain deeper insights into whether and how the policy reform alters the alignment between satellite-based and ground-based pollution data.

**Task:** Fill in the _____ in the regression formula to include the interaction term `AOD:reassign`, which captures how the reassignment of monitoring responsibilities affects the relationship between `AOD` and `log_pm25api`. Additionally, include the control variables `pre` and `tem_mean` to account for precipitation and average temperature. 

```{r "5.2.5"}

#< fill_in
# Regression (including interaction term)
model_2 <- feols(log_pm25api ~ AOD + ________ + _______ + _________ | monitor_id + year^month, 
                 cluster = ~city_id, 
                 data = monitor_api)

#>
# Regression (including interaction term)
model_2 <- feols(log_pm25api ~ AOD + AOD:reassign + pre + tem_mean | monitor_id + year^month, 
                 cluster = ~city_id, 
                 data = monitor_api)


```

After completing the regression formula and running the model, we will analyze the results to determine how the reassignment policy influences the relationship between satellite-based pollution measurements `(AOD)` and ground-level PM2.5 data `(log_pm25api)`. By incorporating the interaction term and additional control variables, we aim to capture any changes in the alignment of these measures that may have resulted from the reassignment of monitoring responsibilities.


**Task:** Press `check` to display the coefficient for the `AOD` variable. 

```{r "5.2.6", message=FALSE}
#< task
# Extract the coefficient for AOD
coef_and_se <- data.frame(
  Coefficient = coef(model_2)["AOD"],
  Std_Error = sqrt(vcov(model_2)["AOD", "AOD"])
)
row.names(coef_and_se) <- "AOD"
print(round(coef_and_se, 3))

#>
```

The coefficient for `AOD` in this regression is **0.273**, indicating the elasticity between satellite-based pollution measurements (`AOD`) and ground-level PM2.5 data (`log_pm25api`). This value tells us that a 1% increase in `AOD` is associated with a **27.3%** increase in ground-level PM2.5 measurements.

Next, we will directly estimate a regression model that includes the interaction term `AOD × reassign` to analyze how the reassignment of monitoring responsibilities influences the relationship between satellite-based pollution measurements (`AOD`) and ground-level PM2.5 data (`log_pm25api`). Specifically, we will extract the coefficient for this interaction term to quantify the extent to which the reassignment policy affects the alignment between these two measures.

This step is crucial because it helps us isolate the effect of the policy change on the reliability of monitoring data, providing deeper insights into whether separating monitoring responsibilities from enforcement leads to improved data quality and reduced manipulation. By focusing on this interaction term, we can assess the direct impact of the reassignment policy on the consistency of satellite-based and ground-based pollution measurements.

**Task:** Just `check` the following code to estimate the regression model with the interaction term `AOD × Reassigned` and display its value.

```{r "5.2.7"}

#< task
# Regression for Column 2 (with interaction term AOD × Reassigned)
model_2_1 <- feols(log_pm25api ~ AOD + AOD:reassign | monitor_id + year^month, 
                 cluster = ~city_id, 
                 data = monitor_api)

# Extract coefficient and standard error for AOD:reassign
interaction_coef <- coef(model_2_1)["AOD:reassign"]
interaction_se <- sqrt(vcov(model_2_1)["AOD:reassign", "AOD:reassign"])

# Create and display results
interaction_result <- data.frame(
  Coefficient = interaction_coef,
  Std_Error = interaction_se
)
row.names(interaction_result) <- "AOD:reassign"
print(round(interaction_result, 3))
#>
```

The coefficient for `AOD:reassign` **0.101** provides valuable insight into the relationship between the reassignment of monitoring responsibilities and the alignment of satellite-based pollution measurements (`AOD`) with ground-level PM2.5 data (`log_pm25api`). Specifically, the positive coefficient indicates that after the reassignment, the elasticity between AOD and PM2.5 increased, suggesting improved alignment between these measures.

Based on the formula provided, answer the following question:

$$
Percentage Increase=(Coefficient for AOD × Reassigned)/ (Base Elasticity))×100
$$

#< quiz "Elasticity Increase after Reassignment" 
question: What does the **0.10** increase in elasticity after the reassignment indicate?
sc:
- The reassignment improved the alignment between AOD and PM2.5 measurements by approximately 37%.*
- The reassignment led to a 37% decrease in the quality of monitoring information.
- The reassignment caused a significant increase in data manipulation by local governments.
- The reassignment had no impact on the alignment between satellite and ground-level pollution data.
success: Correct!
failure: Not quite.
#>

After monitoring responsibilities were reassigned to independent third parties, the elasticity between satellite-based AOD and ground-level PM2.5 measurements increased by 0.10. This corresponds to a 37% improvement compared to the baseline elasticity under local government control. In other words, the reform enhanced the consistency between satellite and ground-based pollution data, suggesting that the monitoring system became more reliable and less prone to manipulation. The proportional increase is calculated as follows:



$$
\text{Percentage Increase} = \left( \frac{0.10}{0.27} \right) \times 100 
$$

This evidence supports the conclusion that transferring monitoring responsibilities to third parties reduces data manipulation and enhances the quality of information. An alternative explanation for the observed changes could be that the AOD data better capture pollution variations after the reassignment, potentially due to changes in pollution composition or advancements in satellite instrumentation. However, the consistency of these findings with the increased elasticity further underscores the impact of reducing manipulation through the reassignment of responsibilities.

To ensure that the observed changes are due to improved monitor data rather than satellite data, we conduct a placebo analysis using background monitors. These monitors are not used by the central government to evaluate local officials, meaning there are fewer incentives for manipulation. By comparing how the relationship between AOD and PM2.5 behaves for these background monitors, we can assess whether the observed changes result from enhanced monitor data rather than external factors. We explore this in the following analysis.

**Task:** Click on `check` to filter the dataset for background monitors and run the regression model to estimate the effect of AOD on log-transformed PM2.5 levels. 

```{r "5.2.8"}

#< task
# Filter for Background Monitors ('compare' identifies background monitors)
monitor_api_background <- monitor_api %>%
  filter(compare == 1)

# Run regression for Column 5 (Background Monitors)
model_3 <- feols(log_pm25api ~ AOD + AOD:reassign + pre + tem_mean | monitor_id + year^month, 
                 cluster = ~city_id, 
                 data = monitor_api %>% filter(compare == 1))

# Extract coefficients and standard errors
results <- broom::tidy(model_3) %>%
  filter(term %in% c("AOD", "AOD:reassign")) %>%
  select(term, estimate, std.error) %>%
  mutate(
    estimate = round(estimate, 3),
    std.error = round(std.error, 3)
  )

# Display results
print(results)


#>

```

#< quiz "elasticity_difference"
question: What does the stronger elasticity between satellite-based AOD and PM2.5 readings at background monitors suggest?
sc:
- Background monitors were more affected by reassignment than the main sample.
- The increase in elasticity suggests that satellite data became less reliable.
- Background monitors were more manipulated than the main sample before reassignment.
- Background monitors were likely less manipulated from the start, leading to a stronger natural correlation with satellite data.*

success: Correct! 
failure: Not quite! 
#>

These results suggest that background monitors were subject to less manipulation from the outset, which explains their already strong alignment with satellite-based pollution data. Since these monitors were not used to evaluate local government performance, local officials had fewer incentives to tamper with the recorded PM2.5 levels. This is reflected in the higher baseline elasticity **(0.37)** between AOD and PM2.5 observed for background monitors, compared to incentivized monitors **(0.27)**. Moreover, while the elasticity increased slightly after the reassignment in both groups, the change was more pronounced for incentivized monitors **(Δ = 0.10 vs 0.05)**, consistent with a reduction in manipulation where incentives to misreport were initially stronger.

Because the elasticity remained stable even after reassignment, we can infer that the observed changes in the main sample were driven by a reduction in data manipulation rather than improvements in satellite measurements. This supports the idea that local officials previously altered pollution data when they had control over monitoring, and that transferring responsibility to third parties helped restore accuracy in pollution reporting.

This analysis highlights the importance of independent monitoring in ensuring reliable environmental data, which in turn strengthens the effectiveness of pollution regulations and enforcement mechanisms.

To conclude our analysis, we investigate whether local governments responded to the reassignment of monitoring responsibilities by intensifying their efforts to reduce air pollution. This will allow us to evaluate whether the policy change effectively reduced manipulation and led to a genuine improvement in pollution control.
To investigate whether local governments exert more effort to reduce pollution levels after the reassignment of monitoring responsibilities, we will estimate a regression model that incorporates interaction terms between the number of monitors and the reassignment policy. Specifically, we aim to assess how the relationship between monitoring (`post1:number`) and pollution (`pm25`) changes after the reassignment by including the interaction term `post1:number:reassign`. This model will also control for factors such as city area, population, and additional fixed effects. By doing so, we can isolate the impact of reassignment on pollution control efforts.

**Task:**  Just `check` the following code to display the results.

```{r "5.2.9"}

#< task
# Load and prepare the data in a single step
city_pm <- read_rds("city_pm.rds") %>%
  mutate(
    reassign = ifelse(year == 2017 | (year == 2016 & month >= 10), 1, 0)  # Define reassigned variable
  )

# Regression for Column 6 (Monitors × Reassigned)
model_4 <- feols(pm25 ~ post1:number + post1:number:reassign + post1:area + post1:pop | 
                   city_id + year^month + pred + tem_meand + age_year + incentive2:time, 
                 cluster = ~city_id, 
                 data = city_pm)

# Extract the coefficient and standard error for Monitors × Reassigned
# Extract the numeric values first
coef_value <- coef(model_4)[["post1:number:reassign"]]
se_value <- sqrt(vcov(model_4)[["post1:number:reassign", "post1:number:reassign"]])

# Build the data frame manually
coef_and_se <- data.frame(
  Variable = "Monitors × Reassigned",
  Coefficient = round(coef_value, 4),
  Std_Error = round(se_value, 4)
)

# Print the result
print(coef_and_se)

#>

```

#< quiz "Impact of Monitors × Reassigned on Pollution Reduction"
question: What does the coefficient for `Monitors × Reassigned` **(−0.0146)** suggest about local governments' efforts after the reassignment of monitoring responsibilities?
sc:
- Local governments exerted more effort to enforce environmental regulations, leading to reduced pollution.*
- Local governments decreased their effort to reduce pollution after the reassignment.
- The reassignment had no significant impact on pollution reduction efforts.
- Pollution levels increased after the reassignment due to reduced enforcement.
success: Correct! 
failure: Not quite. 
#>

The coefficient for `Monitors × Reassigned` highlights a significant shift in local government behavior following the reassignment of monitoring responsibilities to third parties. Specifically, the negative coefficient of **−0.0146** corresponds to a **1.46** percentage point greater reduction in pollution per monitor. This finding supports the hypothesis that local governments, unable to manipulate data as easily post-reassignment, redirected their focus toward actual enforcement of environmental regulations.


**Summary**

In this chapter, we examined whether separating the responsibility for pollution monitoring from local enforcement can improve data quality and policy effectiveness. Using regression analysis, we explored the relationship between satellite-based pollution measures and ground-level data before and after the reassignment of monitoring responsibilities to independent third parties. The results show that this institutional change improved the alignment between the two data sources, suggesting reduced manipulation and more reliable information. Additional analysis indicates that, in response to the reassignment, local governments shifted from manipulating data to actively reducing pollution.

However, these findings should be interpreted with caution, as temporal variation may not fully isolate the effects of reassignment from other influencing factors. Despite this, the evidence underscores the importance of independent monitoring in strengthening accountability and enhancing the effectiveness of environmental governance.

#< award "Governance Architect" >
You cracked the code on how rules and information shape real change. This is more than just policy talk. You are learning what it takes to build smarter systems and cleaner cities. Nice work, policy pro!
#>



## Exercise 6 -- Conclusion

**Recapitulation**

Building on the preceding chapters, this section summarizes the methodological steps, analytical components, and key empirical results of the investigation.

In the first exercise, we introduced the institutional and spatial context of the analysis, covering the evolution of China’s environmental accountability system, the 2015 rollout of real-time pollution monitoring, and the transfer of monitor operations to independent third parties. We also visualized regional patterns of PM2.5 pollution and the spatial rollout of monitors across Chinese cities.

In the second exercise, we presented and prepared the core datasets used throughout the thesis. This involved loading and cleaning firm-level enforcement data, exploring key variables such as pollution exposure, regulatory actions, and firm characteristics, and preparing the data for regression analysis using tidyverse tools in R.

In the third exercise, we performed a firm-level difference-in-differences (DiD) analysis to estimate the causal effect of monitor proximity on enforcement activity. We first estimated a baseline model, then included cluster-robust standard errors to address within-city correlation, and finally added firm and time fixed effects to account for unobserved heterogeneity.

In the fourth exercise, we scaled the analysis to the city level. We re-estimated the DiD model with city and time fixed effects and included control variables to account for observable heterogeneity. To address potential endogeneity in monitor placement, we applied an instrumental variables (IV) approach using centrally assigned monitor quotas as an instrument, estimated via two-stage least squares (2SLS). This allowed us to isolate the causal effect of monitoring on city-level pollution outcomes.

In the fifth exercise, we examined underlying mechanisms. We analyzed top-down accountability by linking enforcement to central oversight, and investigated changes in data credibility using interaction models between satellite-based AOD and ground-level PM2.5. A placebo test using background monitors was included to validate the empirical strategy.

Taken together, the exercises form a comprehensive empirical strategy to study the effect of institutional design on enforcement behavior and data reliability. Each step builds on the previous one and contributes to a deeper understanding of how independent monitoring can improve governance in the context of environmental policy.

**Results**

The paper, which forms the basis of the underlying problem set, examines how real-time pollution monitoring influences environmental enforcement and governance in China. The study focuses on the nationwide introduction of air quality monitors and explores how improved transparency affects the behavior of local governments. By combining georeferenced data on enforcement actions, satellite-based pollution indicators (AOD), and ground-level measurements (PM2.5), the authors analyze whether continuous monitoring leads to stronger environmental regulation and improved data accuracy.

At the firm level, the study finds that the installation of monitors leads to a sharp increase in enforcement activity for firms located near the monitors. Using a difference in differences approach, the authors show that firms within 10 kilometers of a monitor are significantly more likely to face enforcement actions after the monitors are introduced. This increase is not uniform but concentrated on firms identified as major polluters. Enforcement becomes both more frequent and more stringent, with regulatory authorities applying stronger sanctions to visible sources of pollution.

At the city level, the analysis exploits variation in the number of monitors installed across cities, which was based on exogenous criteria such as population size and the extent of the urban area. Cities with a higher number of monitors exhibited stronger enforcement responses and larger reductions in air pollution, as captured by satellite-based AOD measurements. The authors apply several empirical strategies, including difference in differences and instrumental variables estimation. Across these approaches, the results consistently indicate that greater monitoring coverage leads to more intensive regulatory enforcement and lower pollution levels.

A key institutional reform occurred in 2016 when the control of monitors was reassigned from local governments to independent third-party contractors. This reform broke the link between local political incentives and data reporting. Following the reassignment, the correlation between satellite-based and ground-level pollution data increased substantially, indicating that local manipulation of monitor readings declined. The improvement in data quality was not observed for background monitors, which were not tied to official performance targets, strengthening the interpretation that the change was driven by institutional reform rather than external factors.

Overall, the study provides robust evidence that real-time pollution monitoring improves both the enforcement and the credibility of environmental regulation. The effects are especially pronounced when monitoring is independent from enforcement. These findings underscore the importance of institutional design in aligning local government behavior with national policy objectives and suggest that separating information provision from enforcement responsibilities enhances accountability and regulatory effectiveness.

**Remark on Further Robustness Checks**

This thesis has presented and replicated core findings from Axbard and Deng (2024), focusing on the effect of real-time air pollution monitoring on regulatory enforcement and pollution outcomes. While the main results have been discussed and interpreted in detail, readers interested in a broader set of robustness checks such as alternative enforcement outcomes, sample restrictions, or city-level estimation strategies are encouraged to consult the [Online Appendix](https://assets.aeaweb.org/asset-server/files/19887.pdf) of the original paper.

**Related Literature**

A similar research focus can be found in Greenstone and Hanna (2014), who document that environmental regulations can successfully reduce pollution in developing countries, although enforcement often remains a major challenge. Shimshack (2014) further emphasizes that without strong mechanisms for accountability, regulatory efforts frequently fall short. In line with this, Axbard and Deng (2024) investigate how real-time pollution monitoring, by enhancing top-down accountability, can close existing enforcement gaps.
The role of information in improving governance has also been highlighted by Besley and Burgess (2002) and Olken (2007), showing that greater transparency can positively influence government actions. Axbard and Deng build on these findings by demonstrating that automatic monitoring technologies can lead to better enforcement without eliminating local discretion.
Finally, concerns about strategic behavior under strong incentives are addressed by Acemoglu et al. (2020) and Ghanem and Zhang (2014). Their work shows that high-powered incentives can induce data manipulation. Axbard and Deng complement this literature by showing that transferring control over pollution monitoring infrastructure to external firms reduces opportunities for data falsification and improves both information quality and environmental outcomes.

## Exercise 7 -- References

### Bibliography

Acemoglu, D., Fergusson, L., Robinson, J., Romero, D., & Vargas, J. (2020). The perils of high-powered incentives: Evidence from Colombia’s false positives. American Economic Journal: Economic Policy, 12(3), 1–43.

Angrist, J. D., & Imbens, G. W. (1995). Two-stage least squares estimation of average causal effects in models with variable treatment intensity. Journal of the American Statistical Association, 90(430), 431–442.

Angrist, J. D., & Pischke, J. S. (2008). Mostly harmless econometrics: An empiricist’s companion. Princeton University Press.

Axbard, S., & Deng, Z. (2024). Informed enforcement: Lessons from pollution monitoring in China. American Economic Journal: Applied Economics, 16(1), 213–252. 

Banerjee, A. V., Duflo, E., & Glennerster, R. (2008). Putting a band-aid on a corpse: Incentives for nurses in the Indian public health care system. Journal of the European Economic Association, 6(2–3), 487–500.

Bertrand, M., Duflo, E., & Mullainathan, S. (2004). How much should we trust differences-in-differences estimates? Quarterly Journal of Economics, 119(1), 249–275.

Besley, T., & Burgess, R. (2002). The political economy of government responsiveness: Theory and evidence from India. Quarterly Journal of Economics, 117(4), 1415–1451.

Buntaine, M. T., & Daniels, B. (2020). Combining bottom-up monitoring and top-down accountability: A field experiment on managing corruption in Uganda. Research & Politics, 7(3), 1–8.

Cameron, A. C., & Miller, D. L. (2015). A practitioner’s guide to cluster-robust inference. Journal of Human Resources, 50(2), 317–372.

Chang, T., Graff Zivin, J., Gross, T., & Neidell, M. (2016). The effect of pollution on worker productivity: Evidence from call-center workers in China (NBER Working Paper No. 22328). National Bureau of Economic Research.

Ghanem, D., & Zhang, J. (2014). ‘Effortless perfection’: Do Chinese cities manipulate air pollution data? Journal of Environmental Economics and Management, 68(2), 203–225.

Greene, W. H. (2012). Econometric analysis (7th ed.). Pearson Education.

Greenstone, M., & Hanna, R. (2014). Environmental regulations, air and water pollution, and infant mortality in India. American Economic Review, 104(10), 3038–3072.

Haddad, M. A. (2015). Increasing environmental performance in a context of low governmental enforcement: Evidence from China. Journal of Environment & Development, 24(1), 3–25.

Kostka, G., & Mol, A. P. J. (2013). Implementation and participation in China’s local environmental politics: Challenges and innovations. Journal of Environmental Policy and Planning, 15(1), 3–16.

Kranz, S. (2021). Parallel trends plot in R. Retrieved April 10, 2025, from https://skranz.github.io/r/2021/10/20/ParallelTrendsPlot.html#plot

Li, H., & Zhou, L.-A. (2005). Political turnover and economic performance: The incentive role of personnel control in China. Journal of Public Economics, 89(9–10), 1743–1762.

Mailman School of Public Health. (2024). Difference-in-differences estimation. Columbia University. Retrieved April 10, 2025, from https://www.publichealth.columbia.edu/research/population-health-methods/difference-difference-estimation

Ministry of Environmental Protection. (2013). Technical regulation for selection of ambient air quality monitoring stations. Retrieved April 10, 2025, from http://www.mee.gov.cn/ywgz/fgbz/bz/bzwb/jcffbz/201309/t20130925_260810.htm

Nakagaki, M., Kompanek, A., & Tominic, M. (2012). Improving public governance: Closing the implementation gap between law and practice. Center for International Private Enterprise.

Olken, B. A. (2007). Monitoring corruption: Evidence from a field experiment in Indonesia. Journal of Political Economy, 115(2), 200–249.

Sandefur, J., & Glassman, A. (2015). The political economy of bad data: Evidence from African survey and administrative statistics. The Journal of Development Studies, 51(2), 116–132.

Shimshack, J. P. (2014). The economics of environmental monitoring and enforcement. Annual Review of Resource Economics, 6(1), 339–360.

Staiger, D., & Stock, J. H. (1997). Instrumental variables regression with weak instruments. Econometrica, 65(3), 557–586.

White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica, 48(4), 817–838.

World Health Organization. (2016). Ambient air pollution: A global assessment of exposure and burden of disease. World Health Organization.

World Health Organization (WHO). (2021). WHO global air quality guidelines: Particulate matter (PM₂.₅ and PM₁₀), ozone, nitrogen dioxide, sulfur dioxide and carbon monoxide. Retrieved April 10, 2025, from https://www.who.int/publications/i/item/9789240034228

World Bank. (2017). World development report 2017: Governance and the law. World Bank.

Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data (2nd ed.). MIT Press.

Wu, M., & Cao, X. (2021). Greening the career incentive structure for local officials in China: Does less pollution increase the chances of promotion for Chinese local leaders? Journal of Environmental Economics and Management, 107, 102440.

Xi, T., Yao, Y., & Zhang, M. (2018). Capability and opportunism: Evidence from city officials in China. Journal of Comparative Economics, 46(4), 1046–1061.

Zheng, S., & Shi, X. (2017). Environmental regulations and industrial performance: Evidence from China’s Eleventh Five-Year Plan. Journal of Environmental Economics and Management, 86, 188–209.


### R Packages


Arel-Bundock, V. (2022). modelsummary: Data and Model Summaries in R. Journal of Statistical Software, 103(1), 1–23. https://doi.org/10.18637/jss.v103.i01

Bergé, L. (2019). fixest: Fast Fixed-Effects Estimations (Version 0.12.1) [Computer software]. CRAN. https://cran.r-project.org/package=fixest

Csárdi, G., & Nepusz, T. (2006). The igraph software package for complex network research. InterJournal, Complex Systems, 1695. https://igraph.org

Kranz, S. (2020). RTutor: Creating Interactive R Problem Sets with Automatic Feedback [Computer software]. GitHub. https://github.com/skranz/RTutor

Massicotte, P., & South, A. (2025). rnaturalearth: World Map Data from Natural Earth (Version 1.1.0) [Computer software]. rOpenSci. https://docs.ropensci.org/rnaturalearth/

Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org

Wickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). dplyr: A Grammar of Data Manipulation (Version 1.1.4) [Computer software]. CRAN. https://cran.r-project.org/package=dplyr

Wickham, H., Hester, J., & Bryan, J. (2024). readr: Read Rectangular Text Data (Version 2.1.5) [Computer software]. Tidyverse. https://readr.tidyverse.org

Wickham, H., Miller, E., & Smith, D. (2023). haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files (Version 2.5.4) [Computer software]. CRAN. https://cran.r-project.org/package=haven

Wickham, H., Vaughan, D., & Girlich, M. (2024). tidyr: Tidy Messy Data (Version 1.3.1) [Computer software]. CRAN. https://cran.r-project.org/package=tidyr

Zhu, H. (2024). kableExtra: Construct complex tables with 'kable' and pipe syntax (Version 1.4.0) [Computer software]. CRAN. https://cran.r-project.org/package=kableExtra





